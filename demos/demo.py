#!/usr/bin/env python
"""
FusionRAG å¿«é€Ÿæ€§èƒ½æµ‹è¯•
ä½¿ç”¨è¾ƒå°æ•°æ®è§„æ¨¡è¿›è¡Œå¿«é€ŸéªŒè¯
"""

import sys
import time
import json

sys.path.append('.')

from modules.utils.interfaces import Document, Query
from modules.retriever.bm25_retriever import BM25Retriever
from modules.retriever.dense_retriever import DenseRetriever
from modules.retriever.graph_retriever import GraphRetriever
from modules.fusion.fusion import MultiFusion
from modules.evaluator.evaluator import IRMetricsEvaluator

def load_sample_data(doc_limit: int = 100, query_limit: int = 10):
    """åŠ è½½å°è§„æ¨¡æµ‹è¯•æ•°æ®"""
    print(f"ğŸ“ åŠ è½½æµ‹è¯•æ•°æ® (æ–‡æ¡£:{doc_limit}, æŸ¥è¯¢:{query_limit})")
    
    # åŠ è½½æ–‡æ¡£
    documents = []
    with open('data/processed/nfcorpus_corpus.jsonl', 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= doc_limit:
                break
            data = json.loads(line)
            doc = Document(
                doc_id=data['doc_id'],
                title=data.get('title', ''),
                text=data['text'],
                metadata=data.get('metadata', {})
            )
            documents.append(doc)
    
    # åŠ è½½æŸ¥è¯¢
    queries = []
    with open('data/processed/nfcorpus_queries.jsonl', 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= query_limit:
                break
            data = json.loads(line)
            query = Query(
                query_id=data['query_id'],
                text=data['text'],
                metadata=data.get('metadata', {})
            )
            queries.append(query)
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(documents)}æ–‡æ¡£, {len(queries)}æŸ¥è¯¢")
    return documents, queries

def quick_performance_test():
    """å¿«é€Ÿæ€§èƒ½æµ‹è¯•"""
    print("âš¡ FusionRAG å¿«é€Ÿæ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # åŠ è½½å°è§„æ¨¡æ•°æ®
    documents, queries = load_sample_data(doc_limit=100, query_limit=10)
    
    # ä¼˜åŒ–é…ç½®
    configs = {
        'bm25': {'k1': 1.5, 'b': 0.75, 'top_k': 50},
        'dense': {'model_name': 'sentence-transformers/all-MiniLM-L6-v2', 'top_k': 50},
        'graph': {'entity_threshold': 2, 'max_walk_length': 2, 'top_k': 30}
    }
    
    # åˆå§‹åŒ–æ£€ç´¢å™¨
    print("\nğŸ”§ åˆå§‹åŒ–æ£€ç´¢å™¨...")
    retrievers = {}
    
    # BM25æ£€ç´¢å™¨
    retrievers['bm25'] = BM25Retriever(name="bm25", config=configs['bm25'])
    print("âœ… BM25æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # Denseæ£€ç´¢å™¨
    retrievers['dense'] = DenseRetriever(name="dense", config=configs['dense'])
    print("âœ… Denseæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # Graphæ£€ç´¢å™¨
    retrievers['graph'] = GraphRetriever(name="graph", config=configs['graph'])
    print("âœ… Graphæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # æ„å»ºç´¢å¼•
    print("\nğŸ”¨ æ„å»ºç´¢å¼•...")
    build_start = time.time()
    
    for name, retriever in retrievers.items():
        index_start = time.time()
        if name == 'graph':
            retriever.build_index(documents, dataset_name="quick_test")
        else:
            retriever.build_index(documents)
        index_time = time.time() - index_start
        print(f"  âœ… {name}: {index_time:.2f}s")
    
    total_build_time = time.time() - build_start
    print(f"ğŸ“Š æ€»æ„å»ºæ—¶é—´: {total_build_time:.2f}s")
    
    # èåˆå™¨
    fusion_config = {
        'method': 'weighted',
        'weights': {'bm25': 0.5, 'dense': 0.35, 'graph': 0.15},
        'top_k': 20
    }
    fusion = MultiFusion(config=fusion_config)
    
    # æ‰§è¡Œæ£€ç´¢æµ‹è¯•
    print("\nğŸ¯ æ‰§è¡Œæ£€ç´¢æµ‹è¯•...")
    
    all_results = []
    retrieval_times = []
    retriever_stats = {'bm25': 0, 'dense': 0, 'graph': 0}
    
    for i, query in enumerate(queries):
        print(f"\nğŸ” æŸ¥è¯¢ {i+1}: {query.text[:40]}...")
        
        query_start = time.time()
        
        # å„æ£€ç´¢å™¨æ£€ç´¢
        retriever_results = {}
        for name, retriever in retrievers.items():
            try:
                results = retriever.retrieve(query, top_k=configs[name]['top_k'])
                retriever_results[name] = results
                retriever_stats[name] += len(results)
                print(f"  {name}: {len(results)} ç»“æœ")
            except Exception as e:
                print(f"  âš ï¸ {name} æ£€ç´¢å¤±è´¥: {e}")
                retriever_results[name] = []
        
        # èåˆç»“æœ
        if retriever_results:
            fused_results = fusion.fuse(retriever_results, query)
            all_results.append((query, fused_results))
            print(f"  èåˆ: {len(fused_results)} ç»“æœ")
        
        query_time = time.time() - query_start
        retrieval_times.append(query_time)
        print(f"  â±ï¸ æ—¶é—´: {query_time:.3f}s")
    
    # æ€§èƒ½ç»Ÿè®¡
    print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡")
    print("=" * 40)
    
    avg_query_time = sum(retrieval_times) / len(retrieval_times)
    print(f"å¹³å‡æŸ¥è¯¢æ—¶é—´: {avg_query_time:.3f}s")
    print(f"æŸ¥è¯¢ååé‡: {1/avg_query_time:.1f} queries/sec")
    
    print(f"\næ£€ç´¢å™¨ç»“æœç»Ÿè®¡:")
    for name, count in retriever_stats.items():
        avg_per_query = count / len(queries)
        print(f"  {name}: å¹³å‡ {avg_per_query:.1f} ç»“æœ/æŸ¥è¯¢")
    
    # èåˆç»“æœåˆ†æ
    print(f"\nèåˆç»“æœåˆ†æ:")
    fusion_contributions = {'bm25': 0, 'dense': 0, 'graph': 0}
    total_fused = 0
    
    for _, fused_results in all_results:
        total_fused += len(fused_results)
        for result in fused_results[:10]:  # çœ‹å‰10ä¸ªç»“æœ
            # FusionResultåŒ…å«individual_scoresï¼Œæ˜¾ç¤ºå„æ£€ç´¢å™¨çš„è´¡çŒ®
            if hasattr(result, 'individual_scores') and result.individual_scores:
                # æ‰¾åˆ°è´¡çŒ®æœ€å¤§çš„æ£€ç´¢å™¨
                max_score_retriever = max(result.individual_scores.items(), key=lambda x: x[1])[0]
                if max_score_retriever in fusion_contributions:
                    fusion_contributions[max_score_retriever] += 1
    
    print(f"å‰10èåˆç»“æœä¸­ä¸»è¦è´¡çŒ®æ£€ç´¢å™¨:")
    total_contributions = sum(fusion_contributions.values())
    for name, count in fusion_contributions.items():
        percentage = count / total_contributions * 100 if total_contributions > 0 else 0
        print(f"  {name}: {count} ({percentage:.1f}%)")
    
    # å›¾æ£€ç´¢å™¨è´¨é‡åˆ†æ
    print(f"\nğŸ”— å›¾æ£€ç´¢å™¨åˆ†æ:")
    graph_retriever = retrievers['graph']
    stats = graph_retriever.get_statistics()
    
    if stats:
        print(f"  èŠ‚ç‚¹æ•°: {stats.get('nodes', 'N/A')}")
        print(f"  è¾¹æ•°: {stats.get('edges', 'N/A')}")
        print(f"  å¹³å‡åº¦: {stats.get('avg_degree', 'N/A'):.1f}")
        print(f"  æ•°æ®é›†: {stats.get('dataset', 'N/A')}")
    
    # ç¤ºä¾‹ç»“æœå±•ç¤º
    print(f"\nğŸ¯ ç¤ºä¾‹æ£€ç´¢ç»“æœ:")
    if all_results:
        query, results = all_results[0]
        print(f"æŸ¥è¯¢: {query.text}")
        print(f"å‰5ä¸ªç»“æœ:")
        for i, result in enumerate(results[:5], 1):
            print(f"  {i}. [{result.final_score:.3f}] {result.document.title[:50]}...")
            # æ˜¾ç¤ºå„æ£€ç´¢å™¨çš„è´¡çŒ®åˆ†æ•°
            if hasattr(result, 'individual_scores') and result.individual_scores:
                scores_str = ", ".join([f"{k}:{v:.3f}" for k, v in result.individual_scores.items()])
                print(f"     åˆ†æ•°åˆ†å¸ƒ: {scores_str}")
    
    print(f"\nğŸ‰ å¿«é€Ÿæµ‹è¯•å®Œæˆ!")
    print(f"æµ‹è¯•è§„æ¨¡: {len(documents)}æ–‡æ¡£, {len(queries)}æŸ¥è¯¢")
    print(f"æ€»è€—æ—¶: {time.time() - build_start + total_build_time:.2f}s")

if __name__ == "__main__":
    quick_performance_test()