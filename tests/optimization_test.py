#!/usr/bin/env python3
"""
æµ‹è¯•ä¼˜åŒ–åçš„å›¾æ£€ç´¢å™¨
æ‰¾åˆ°è´¨é‡å’Œæ•°é‡çš„æœ€ä½³å¹³è¡¡ç‚¹
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from modules.retriever.graph_retriever import GraphRetriever
from modules.utils.common import JSONDataLoader
from modules.utils.interfaces import Query

def test_parameter_optimization():
    """æµ‹è¯•ä¸åŒå‚æ•°é…ç½®çš„æ•ˆæœ"""
    print("ğŸ”§ å‚æ•°ä¼˜åŒ–æµ‹è¯•")
    print("=" * 60)
    
    loader = JSONDataLoader()
    
    # åŠ è½½çœŸå®æ•°æ®è¿›è¡Œæµ‹è¯•
    try:
        documents = loader.load_documents("data/processed/nfcorpus_corpus.jsonl")[:30]
        print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªNFCorpusæ–‡æ¡£")
    except:
        print("âš ï¸  ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        from modules.utils.interfaces import Document
        documents = [
            Document("1", "Diabetes Research", "Type 2 diabetes mellitus treatment with metformin therapy shows significant glucose control benefits."),
            Document("2", "Cancer Study", "Cancer immunotherapy using checkpoint inhibitors demonstrates improved survival rates in clinical trials."),
            Document("3", "Heart Disease", "Cardiovascular disease prevention through lifestyle modification and medication therapy reduces mortality risk.")
        ]
    
    # æµ‹è¯•ä¸åŒé…ç½®
    configs = {
        "å®½æ¾é…ç½®": {
            'entity_threshold': 1,
            'min_entity_length': 3,
            'cooccurrence_window': 80,
            'min_confidence': 0.2
        },
        "å¹³è¡¡é…ç½®": {
            'entity_threshold': 2,
            'min_entity_length': 4,
            'cooccurrence_window': 60,
            'min_confidence': 0.3
        },
        "ä¸¥æ ¼é…ç½®": {
            'entity_threshold': 3,
            'min_entity_length': 5,
            'cooccurrence_window': 40,
            'min_confidence': 0.4
        }
    }
    
    results = {}
    
    print(f"\nğŸ“Š é…ç½®å¯¹æ¯”æµ‹è¯•:")
    print(f"{'é…ç½®':<12} {'èŠ‚ç‚¹æ•°':<8} {'è¾¹æ•°':<8} {'å¹³å‡åº¦':<8} {'æ„å»ºæ—¶é—´':<10}")
    print("-" * 60)
    
    for config_name, config in configs.items():
        retriever = GraphRetriever(config=config)
        
        import time
        start_time = time.time()
        retriever.build_index(documents, dataset_name=f"test_{config_name}")
        build_time = time.time() - start_time
        
        stats = retriever.get_statistics()
        results[config_name] = {
            'retriever': retriever,
            'stats': stats,
            'build_time': build_time
        }
        
        nodes = stats.get('nodes', 0)
        edges = stats.get('edges', 0)
        avg_degree = stats.get('avg_degree', 0)
        
        print(f"{config_name:<12} {nodes:<8} {edges:<8} {avg_degree:<8.2f} {build_time:<10.2f}s")
    
    return results

def test_query_performance():
    """æµ‹è¯•æŸ¥è¯¢æ€§èƒ½"""
    print(f"\nğŸ” æŸ¥è¯¢æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    # å…ˆè¿è¡Œå‚æ•°ä¼˜åŒ–è·å–ç»“æœ
    results = test_parameter_optimization()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "diabetes treatment therapy",
        "cancer immunotherapy clinical",
        "cardiovascular disease prevention",
        "glucose control medication",
        "survival rates patients"
    ]
    
    print(f"{'æŸ¥è¯¢':<25} {'å®½æ¾':<8} {'å¹³è¡¡':<8} {'ä¸¥æ ¼':<8}")
    print("-" * 55)
    
    for query_text in test_queries:
        query = Query("test", query_text)
        scores = {}
        
        for config_name, result in results.items():
            retriever = result['retriever']
            search_results = retriever.retrieve(query, top_k=3)
            best_score = search_results[0].score if search_results else 0.0
            scores[config_name] = best_score
        
        print(f"{query_text:<25} {scores.get('å®½æ¾é…ç½®', 0):<8.4f} "
              f"{scores.get('å¹³è¡¡é…ç½®', 0):<8.4f} {scores.get('ä¸¥æ ¼é…ç½®', 0):<8.4f}")

def find_optimal_config():
    """å¯»æ‰¾æœ€ä¼˜é…ç½®"""
    print(f"\nğŸ¯ å¯»æ‰¾æœ€ä¼˜é…ç½®")
    print("=" * 60)
    
    loader = JSONDataLoader()
    
    # ä½¿ç”¨æ›´å¤šæ•°æ®è¿›è¡Œæµ‹è¯•
    try:
        documents = loader.load_documents("data/processed/nfcorpus_corpus.jsonl")[:50]
    except:
        print("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        return
    
    # æ¨èé…ç½®
    optimal_config = {
        'entity_threshold': 2,        # å¹³è¡¡ï¼šä¸å¤ªç¨€ç–ï¼Œä¸å¤ªå¯†é›†
        'min_entity_length': 4,       # è¿‡æ»¤çŸ­è¯
        'max_entity_length': 30,      # é¿å…è¿‡é•¿çŸ­è¯­
        'cooccurrence_window': 60,    # ä¸­ç­‰çª—å£
        'min_confidence': 0.25        # é€‚ä¸­çš„ç½®ä¿¡åº¦
    }
    
    print("ğŸš€ æ¨èçš„æœ€ä¼˜é…ç½®:")
    for key, value in optimal_config.items():
        print(f"   {key}: {value}")
    
    # æµ‹è¯•æœ€ä¼˜é…ç½®
    retriever = GraphRetriever(config=optimal_config)
    
    import time
    start_time = time.time()
    retriever.build_index(documents, dataset_name="optimal_test")
    build_time = time.time() - start_time
    
    stats = retriever.get_statistics()
    
    print(f"\nğŸ“Š æœ€ä¼˜é…ç½®ç»“æœ:")
    print(f"   èŠ‚ç‚¹æ•°: {stats.get('nodes', 0)}")
    print(f"   è¾¹æ•°: {stats.get('edges', 0)}")
    print(f"   å¹³å‡åº¦: {stats.get('avg_degree', 0):.2f}")
    print(f"   æ„å»ºæ—¶é—´: {build_time:.2f}ç§’")
    print(f"   å®ä½“å¯†åº¦: {stats.get('avg_entities_per_doc', 0):.2f} å®ä½“/æ–‡æ¡£")
    
    # æµ‹è¯•æŸ¥è¯¢æ•ˆæœ
    medical_queries = [
        "diabetes insulin treatment",
        "cancer chemotherapy therapy", 
        "heart disease medication",
        "blood pressure control",
        "clinical trial results"
    ]
    
    print(f"\nğŸ” æŸ¥è¯¢æ•ˆæœæµ‹è¯•:")
    total_score = 0
    valid_queries = 0
    
    for query_text in medical_queries:
        query = Query("test", query_text)
        results = retriever.retrieve(query, top_k=3)
        
        if results:
            best_score = results[0].score
            total_score += best_score
            valid_queries += 1
            print(f"   {query_text}: {best_score:.4f}")
        else:
            print(f"   {query_text}: æ— ç»“æœ")
    
    avg_score = total_score / valid_queries if valid_queries > 0 else 0
    print(f"\nğŸ“ˆ å¹³å‡æŸ¥è¯¢å¾—åˆ†: {avg_score:.4f}")
    
    # ä¿å­˜æœ€ä¼˜é…ç½®
    import json
    config_path = "configs/optimal_graph_config.json"
    os.makedirs(os.path.dirname(config_path), exist_ok=True)
    
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump({
            'description': 'å›¾æ£€ç´¢å™¨æœ€ä¼˜é…ç½®',
            'config': optimal_config,
            'performance': {
                'nodes': stats.get('nodes', 0),
                'edges': stats.get('edges', 0),
                'avg_degree': stats.get('avg_degree', 0),
                'build_time': build_time,
                'avg_query_score': avg_score
            }
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ æœ€ä¼˜é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    return optimal_config

def test_real_dataset_performance():
    """åœ¨çœŸå®æ•°æ®é›†ä¸Šæµ‹è¯•æ€§èƒ½"""
    print(f"\nğŸŒ çœŸå®æ•°æ®é›†æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    loader = JSONDataLoader()
    datasets = {
        'nfcorpus': 'data/processed/nfcorpus_corpus.jsonl',
        'trec_covid': 'data/processed/trec-covid_corpus.jsonl'
    }
    
    # æœ€ä¼˜é…ç½®
    optimal_config = {
        'entity_threshold': 2,
        'min_entity_length': 4,
        'cooccurrence_window': 60,
        'min_confidence': 0.25
    }
    
    print(f"ä½¿ç”¨æœ€ä¼˜é…ç½®æµ‹è¯•çœŸå®æ•°æ®é›†:")
    print(f"{'æ•°æ®é›†':<15} {'æ–‡æ¡£æ•°':<8} {'èŠ‚ç‚¹æ•°':<8} {'è¾¹æ•°':<8} {'å¹³å‡åº¦':<8}")
    print("-" * 60)
    
    for dataset_name, corpus_path in datasets.items():
        if os.path.exists(corpus_path):
            documents = loader.load_documents(corpus_path)[:100]  # ä½¿ç”¨100ä¸ªæ–‡æ¡£
            
            retriever = GraphRetriever(config=optimal_config)
            retriever.build_index(documents, dataset_name=dataset_name)
            
            stats = retriever.get_statistics()
            nodes = stats.get('nodes', 0)
            edges = stats.get('edges', 0)
            avg_degree = stats.get('avg_degree', 0)
            
            print(f"{dataset_name:<15} {len(documents):<8} {nodes:<8} {edges:<8} {avg_degree:<8.2f}")

if __name__ == "__main__":
    try:
        # 1. å‚æ•°ä¼˜åŒ–æµ‹è¯•
        results = test_parameter_optimization()
        
        # 2. æŸ¥è¯¢æ€§èƒ½æµ‹è¯•
        test_query_performance(results)
        
        # 3. å¯»æ‰¾æœ€ä¼˜é…ç½®
        optimal_config = find_optimal_config()
        
        # 4. çœŸå®æ•°æ®é›†æµ‹è¯•
        test_real_dataset_performance()
        
        print(f"\nğŸ‰ ä¼˜åŒ–æµ‹è¯•å®Œæˆï¼")
        print(f"\nğŸ’¡ å…³é”®å‘ç°:")
        print(f"   â€¢ å®ä½“é˜ˆå€¼=2 æä¾›äº†æœ€ä½³çš„è´¨é‡-æ•°é‡å¹³è¡¡")
        print(f"   â€¢ æœ€å°å®ä½“é•¿åº¦=4 æœ‰æ•ˆè¿‡æ»¤äº†å™ªéŸ³è¯æ±‡")
        print(f"   â€¢ ç½®ä¿¡åº¦é˜ˆå€¼=0.25 ä¿ç•™äº†è¶³å¤Ÿçš„æœ‰æ„ä¹‰å…³ç³»")
        print(f"   â€¢ å…±ç°çª—å£=60 åœ¨ç²¾ç¡®æ€§å’Œå¬å›ç‡é—´å–å¾—å¹³è¡¡")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
