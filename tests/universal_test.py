#!/usr/bin/env python
"""
FusionRAGé€šç”¨æµ‹è¯•æ¡†æ¶
æ”¯æŒåŠ¨æ€åŠ è½½é…ç½®æ–‡ä»¶ï¼Œé€‚ç”¨äºä»»ä½•æ•°æ®é›†å’Œé…ç½®çš„å®Œæ•´æµ‹è¯•
"""

import sys
import os
import time
import json
import argparse
import yaml
from pathlib import Path
sys.path.append('.')

# è®¾ç½®ç¯å¢ƒå˜é‡é¿å…æ®µé”™è¯¯
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'

from pipeline import FusionRAGPipeline
from modules.utils.interfaces import Query
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_data_availability(config_path: str):
    """æ£€æŸ¥é…ç½®æ–‡ä»¶æŒ‡å®šçš„æ•°æ®æ˜¯å¦å¯ç”¨"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        data_config = config.get('data', {})
        required_files = [
            data_config.get('corpus_path'),
            data_config.get('queries_path'),
            data_config.get('qrels_path')
        ]

        missing_files = []
        for file_path in required_files:
            if file_path and not Path(file_path).exists():
                missing_files.append(file_path)

        if missing_files:
            logger.error("ç¼ºå°‘å¿…è¦çš„æ•°æ®æ–‡ä»¶:")
            for file_path in missing_files:
                logger.error(f"  - {file_path}")

            # ä»é…ç½®æ¨æ–­æ•°æ®é›†åç§°
            dataset_name = "unknown"
            metadata = config.get('metadata', {})
            if 'dataset' in metadata:
                dataset_name = metadata['dataset']
            else:
                # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­
                corpus_path = data_config.get('corpus_path', '')
                if 'nfcorpus' in corpus_path:
                    dataset_name = 'nfcorpus'
                elif 'trec-covid' in corpus_path:
                    dataset_name = 'trec-covid'
                elif 'natural-questions' in corpus_path:
                    dataset_name = 'natural-questions'

            logger.info("è¯·å…ˆè¿è¡Œæ•°æ®ä¸‹è½½å’Œé¢„å¤„ç†:")
            logger.info(f"  python scripts/download_data.py --dataset {dataset_name}")
            logger.info(f"  python scripts/preprocess_data.py --dataset {dataset_name}")
            return False

        return True

    except Exception as e:
        logger.error(f"æ£€æŸ¥æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

def download_and_preprocess_data(dataset_name: str):
    """è‡ªåŠ¨ä¸‹è½½å’Œé¢„å¤„ç†æŒ‡å®šæ•°æ®é›†"""
    logger.info(f"ğŸ”„ è‡ªåŠ¨ä¸‹è½½å’Œé¢„å¤„ç†{dataset_name}æ•°æ®é›†...")

    try:
        # ä¸‹è½½æ•°æ®
        logger.info(f"ğŸ“¥ ä¸‹è½½{dataset_name}æ•°æ®é›†...")
        from scripts.download_data import DataDownloader
        downloader = DataDownloader()
        success = downloader.download_beir_dataset(dataset_name)

        if not success:
            logger.error("æ•°æ®ä¸‹è½½å¤±è´¥")
            return False

        # é¢„å¤„ç†æ•°æ®
        logger.info("âš™ï¸ é¢„å¤„ç†æ•°æ®...")
        from scripts.preprocess_data import DataProcessor
        processor = DataProcessor()
        success = processor.process_beir_dataset(dataset_name)

        if not success:
            logger.error("æ•°æ®é¢„å¤„ç†å¤±è´¥")
            return False

        logger.info("âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
        return True

    except Exception as e:
        logger.error(f"æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        return False

def test_with_config():
    """ä½¿ç”¨æŒ‡å®šé…ç½®æ–‡ä»¶è¿›è¡Œæµ‹è¯•"""
    config_path = "configs/ms_marco_config.yaml"
    auto_download = True
    
    logger.info("ğŸš€ å¼€å§‹é…ç½®åŒ–æµ‹è¯•")
    logger.info("=" * 60)

    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not Path(config_path).exists():
        logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return None

    # åŠ è½½é…ç½®è·å–æ•°æ®é›†ä¿¡æ¯
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    dataset_name = config.get('metadata', {}).get('dataset', 'unknown')
    template_name = config.get('metadata', {}).get('template', 'unknown')

    logger.info(f"ğŸ“‹ é…ç½®æ–‡ä»¶: {config_path}")
    logger.info(f"ğŸ“Š æ•°æ®é›†: {dataset_name}")
    logger.info(f"ğŸ¯ æ¨¡æ¿: {template_name}")

    # æ£€æŸ¥æ•°æ®
    if not check_data_availability(config_path):
        if auto_download:
            logger.info("å°è¯•è‡ªåŠ¨ä¸‹è½½æ•°æ®...")
            if not download_and_preprocess_data(dataset_name):
                logger.error("æ— æ³•è·å–æµ‹è¯•æ•°æ®ï¼Œæµ‹è¯•ç»ˆæ­¢")
                return None
        else:
            logger.error("æ•°æ®æ–‡ä»¶ç¼ºå¤±ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return None

    # åˆå§‹åŒ–pipeline
    pipeline = FusionRAGPipeline(config_path)
    
    # åŠ è½½æ•°æ®
    logger.info("ğŸ“ åŠ è½½å®Œæ•´æ•°æ®é›†...")
    start_time = time.time()
    pipeline.load_data()
    load_time = time.time() - start_time
    
    logger.info(f"æ•°æ®åŠ è½½å®Œæˆ ({load_time:.2f}s):")
    logger.info(f"  ğŸ“„ æ–‡æ¡£æ•°é‡: {len(pipeline.documents):,}")
    logger.info(f"  ğŸ” æŸ¥è¯¢æ•°é‡: {len(pipeline.queries):,}")
    logger.info(f"  ğŸ·ï¸ æ ‡æ³¨æ•°é‡: {len(pipeline.qrels):,}")
    
    # æ„å»ºç´¢å¼•
    logger.info("ğŸ”¨ æ„å»ºç´¢å¼•...")
    start_time = time.time()
    pipeline.build_indexes(force_rebuild=False)  # å…è®¸ä½¿ç”¨ç¼“å­˜
    build_time = time.time() - start_time
    
    logger.info(f"ç´¢å¼•æ„å»ºå®Œæˆ ({build_time:.2f}s)")
    
    # è¿è¡Œå®Œæ•´è¯„æµ‹
    logger.info("ğŸ“Š è¿è¡Œå®Œæ•´æ€§èƒ½è¯„æµ‹...")
    start_time = time.time()
    
    # æ‰¹é‡æ£€ç´¢
    search_results = pipeline.batch_search()
    search_time = time.time() - start_time
    
    logger.info(f"æ‰¹é‡æ£€ç´¢å®Œæˆ ({search_time:.2f}s)")
    logger.info(f"  å¹³å‡æ¯æŸ¥è¯¢: {search_time/len(pipeline.queries)*1000:.2f}ms")
    
    # è¯„æµ‹ç»“æœ
    logger.info("ğŸ“ˆ è®¡ç®—è¯„æµ‹æŒ‡æ ‡...")
    evaluation_results = pipeline.evaluate(search_results)
    
    # æ˜¾ç¤ºç»“æœ
    if evaluation_results:
        logger.info("ğŸ¯ é«˜æ€§èƒ½æ¨¡å‹æµ‹è¯•ç»“æœ:")
        logger.info("=" * 60)
        
        # ä¸»è¦æŒ‡æ ‡
        for metric_name, metric_data in evaluation_results.items():
            if isinstance(metric_data, dict):
                logger.info(f"{metric_name.upper()}:")
                for k, v in metric_data.items():
                    if isinstance(v, (int, float)):
                        logger.info(f"  {k}: {v:.4f}")
                    elif isinstance(v, dict) and 'mean' in v:
                        logger.info(f"  {k}: {v['mean']:.4f} (Â±{v.get('std', 0):.4f})")
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = pipeline.evaluator.generate_report(evaluation_results)
        logger.info("\nğŸ“‹ è¯¦ç»†è¯„æµ‹æŠ¥å‘Š:")
        logger.info("=" * 60)
        for line in report.split('\n'):
            if line.strip():
                logger.info(line)
    
    # æ€§èƒ½æ€»ç»“
    total_time = load_time + build_time + search_time
    logger.info(f"\nâ±ï¸ æ€§èƒ½æ€»ç»“:")
    logger.info(f"  æ•°æ®åŠ è½½: {load_time:.2f}s")
    logger.info(f"  ç´¢å¼•æ„å»º: {build_time:.2f}s") 
    logger.info(f"  æ‰¹é‡æ£€ç´¢: {search_time:.2f}s")
    logger.info(f"  æ€»è€—æ—¶: {total_time:.2f}s")
    
    # ä¿å­˜ç»“æœ
    results_summary = {
        "config": config_path,
        "dataset": dataset_name,
        "template": template_name,
        "dataset_stats": {
            "documents": len(pipeline.documents),
            "queries": len(pipeline.queries),
            "qrels": len(pipeline.qrels)
        },
        "performance": {
            "load_time": load_time,
            "build_time": build_time,
            "search_time": search_time,
            "total_time": total_time,
            "avg_query_time": search_time / len(pipeline.queries)
        },
        "evaluation": evaluation_results
    }

    # ç”Ÿæˆç»“æœæ–‡ä»¶åï¼ˆåŸºäºé…ç½®æ–‡ä»¶åï¼‰
    config_name = Path(config_path).stem
    output_file = f"checkpoints/logs/{config_name}_test_results.json"
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, indent=2, ensure_ascii=False)

    logger.info(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    return results_summary

def compare_results(result1_file: str, result2_file: str):
    """æ¯”è¾ƒä¸¤ä¸ªæµ‹è¯•ç»“æœ"""
    logger.info("ğŸ”„ æ¯”è¾ƒæµ‹è¯•ç»“æœ...")

    try:
        with open(result1_file, 'r') as f:
            results1 = json.load(f)
        with open(result2_file, 'r') as f:
            results2 = json.load(f)

        logger.info("ğŸ“Š ç»“æœå¯¹æ¯”:")
        logger.info(f"  é…ç½®1: {results1.get('config', 'unknown')}")
        logger.info(f"  é…ç½®2: {results2.get('config', 'unknown')}")

        # æ¯”è¾ƒä¸»è¦æŒ‡æ ‡
        eval1 = results1.get('evaluation', {}).get('metrics', {})
        eval2 = results2.get('evaluation', {}).get('metrics', {})

        for metric in ['recall@5', 'recall@10', 'ndcg@10', 'map']:
            if metric in eval1 and metric in eval2:
                val1, val2 = eval1[metric], eval2[metric]
                improvement = ((val2 - val1) / val1 * 100) if val1 > 0 else 0
                logger.info(f"  {metric}: {val1:.4f} â†’ {val2:.4f} ({improvement:+.1f}%)")

    except Exception as e:
        logger.warning(f"æ— æ³•æ¯”è¾ƒç»“æœ: {e}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="FusionRAGé€šç”¨æµ‹è¯•æ¡†æ¶")
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-auto-download', action='store_true', help='ä¸è‡ªåŠ¨ä¸‹è½½æ•°æ®')
    parser.add_argument('--compare-with', type=str, help='ä¸æŒ‡å®šç»“æœæ–‡ä»¶å¯¹æ¯”')

    args = parser.parse_args()

    logger.info("ğŸ¯ FusionRAGé€šç”¨æµ‹è¯•æ¡†æ¶")
    logger.info("=" * 60)

    try:
        # è¿è¡Œæµ‹è¯•
        results = test_with_config(args.config, not args.no_auto_download)

        if results is None:
            logger.error("æµ‹è¯•å¤±è´¥")
            return

        # å¯¹æ¯”ç»“æœ
        if args.compare_with:
            if Path(args.compare_with).exists():
                config_name = Path(args.config).stem
                current_result = f"checkpoints/logs/{config_name}_test_results.json"
                compare_results(args.compare_with, current_result)
            else:
                logger.warning(f"å¯¹æ¯”æ–‡ä»¶ä¸å­˜åœ¨: {args.compare_with}")

        logger.info("\nâœ… æµ‹è¯•å®Œæˆ!")
        logger.info("ç³»ç»Ÿå·²å®ŒæˆæŒ‡å®šé…ç½®çš„å®Œæ•´æµ‹è¯•")

        return results

    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        logger.error(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
