#!/usr/bin/env python
"""
ä¼˜åŒ–çš„FusionRAGç³»ç»Ÿ
åŸºäºè¯Šæ–­ç»“æœè¿›è¡Œçš„æ€§èƒ½ä¼˜åŒ–
"""

import sys
import os
import time
sys.path.append('.')

def create_optimized_config():
    """åˆ›å»ºä¼˜åŒ–çš„é…ç½®"""
    optimized_config = """
# ä¼˜åŒ–çš„FusionRAGé…ç½®

data:
  corpus_path: "data/processed/nfcorpus_corpus.jsonl"
  queries_path: "data/processed/nfcorpus_queries.jsonl"
  qrels_path: "data/processed/nfcorpus_qrels.tsv"
  output_dir: "data/processed/"

retrievers:
  bm25:
    enabled: true
    index_path: "checkpoints/retriever/optimized_bm25_index.pkl"
    k1: 1.2
    b: 0.75
    top_k: 200  # å¢åŠ æ£€ç´¢æ•°é‡
    
  dense:
    enabled: true
    model_name: "sentence-transformers/all-mpnet-base-v2"  # æ›´å¼ºçš„æ¨¡å‹
    index_path: "checkpoints/retriever/optimized_dense_index.faiss"
    embedding_dim: 768  # æ›´é«˜ç»´åº¦
    top_k: 200  # å¢åŠ æ£€ç´¢æ•°é‡
    batch_size: 16  # å‡å°batch sizeæé«˜è´¨é‡

fusion:
  method: "rrf"  # ä½¿ç”¨RRFèåˆï¼Œå¯¹æ’åæ›´æ•æ„Ÿ
  rrf_k: 60
  top_k: 50  # å¢åŠ æœ€ç»ˆç»“æœæ•°é‡

reranker:
  enabled: false

evaluation:
  metrics: ["recall@5", "recall@10", "recall@20", "ndcg@10", "ndcg@20", "map"]
  output_path: "checkpoints/logs/optimized_eval_results.json"

system:
  device: "cpu"
  batch_size: 16
  num_threads: 4
  log_level: "INFO"
  log_path: "checkpoints/logs/optimized_system.log"
"""
    
    with open("configs/optimized_config.yaml", 'w', encoding='utf-8') as f:
        f.write(optimized_config)
    
    print("âœ… ä¼˜åŒ–é…ç½®å·²åˆ›å»º: configs/optimized_config.yaml")

def test_optimized_system():
    """æµ‹è¯•ä¼˜åŒ–åçš„ç³»ç»Ÿ"""
    print("ğŸš€ æµ‹è¯•ä¼˜åŒ–åçš„FusionRAGç³»ç»Ÿ")
    print("=" * 50)
    
    from pipeline import FusionRAGPipeline
    
    # ä½¿ç”¨ä¼˜åŒ–é…ç½®
    pipeline = FusionRAGPipeline("configs/optimized_config.yaml")
    
    # åŠ è½½æ•°æ®
    print("åŠ è½½æ•°æ®...")
    pipeline.load_data()
    
    print(f"æ•°æ®ç»Ÿè®¡:")
    print(f"  æ–‡æ¡£æ•°: {len(pipeline.documents):,}")
    print(f"  æŸ¥è¯¢æ•°: {len(pipeline.queries):,}")
    print(f"  æ ‡æ³¨æ•°: {len(pipeline.qrels):,}")
    
    # æ„å»ºä¼˜åŒ–ç´¢å¼•
    print("\næ„å»ºä¼˜åŒ–ç´¢å¼•...")
    start_time = time.time()
    pipeline.build_indexes(force_rebuild=True)
    index_time = time.time() - start_time
    print(f"ç´¢å¼•æ„å»ºæ—¶é—´: {index_time:.2f}s")
    
    # æµ‹è¯•æ€§èƒ½
    print("\næµ‹è¯•æ£€ç´¢æ€§èƒ½...")
    
    # é€‰æ‹©æœ‰æ ‡æ³¨çš„æŸ¥è¯¢è¿›è¡Œæµ‹è¯•
    test_queries = [q for q in pipeline.queries if q.query_id in pipeline.qrels][:20]
    
    all_results = {}
    retrieval_times = []
    
    for i, query in enumerate(test_queries):
        start = time.time()
        results = pipeline.search(query, top_k=50)  # è·å–æ›´å¤šç»“æœ
        retrieval_time = time.time() - start
        retrieval_times.append(retrieval_time)
        
        all_results[query.query_id] = results
        
        if i < 5:  # æ˜¾ç¤ºå‰5ä¸ªæŸ¥è¯¢çš„è¯¦ç»†ç»“æœ
            relevant_docs = set(pipeline.qrels[query.query_id])
            result_docs = [r.doc_id for r in results]
            
            recall_5 = len(set(result_docs[:5]) & relevant_docs) / len(relevant_docs)
            recall_10 = len(set(result_docs[:10]) & relevant_docs) / len(relevant_docs)
            recall_20 = len(set(result_docs[:20]) & relevant_docs) / len(relevant_docs)
            
            print(f"\n  æŸ¥è¯¢ {i+1}: {query.text[:50]}...")
            print(f"    ç›¸å…³æ–‡æ¡£æ•°: {len(relevant_docs)}")
            print(f"    Recall@5: {recall_5:.3f}")
            print(f"    Recall@10: {recall_10:.3f}")
            print(f"    Recall@20: {recall_20:.3f}")
            print(f"    æ£€ç´¢æ—¶é—´: {retrieval_time:.3f}s")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªç»“æœ
            print(f"    å‰5ä¸ªç»“æœ:")
            for j, result in enumerate(results[:5]):
                is_relevant = "âœ“" if result.doc_id in relevant_docs else "âœ—"
                print(f"      {j+1}. [{result.final_score:.3f}] {is_relevant} {result.document.title[:40]}...")
    
    # å…¨é¢è¯„æµ‹
    print(f"\nå…¨é¢è¯„æµ‹ ({len(test_queries)} ä¸ªæŸ¥è¯¢)...")
    
    query_predictions = {}
    query_ground_truth = {}
    
    for query_id, results in all_results.items():
        query_predictions[query_id] = [r.doc_id for r in results]
        query_ground_truth[query_id] = pipeline.qrels[query_id]
    
    metrics = pipeline.evaluator.evaluate_retrieval(query_predictions, query_ground_truth)
    
    print(f"\nğŸ“Š ä¼˜åŒ–åæ€§èƒ½:")
    print(f"  å¹³å‡æ£€ç´¢æ—¶é—´: {sum(retrieval_times)/len(retrieval_times):.3f}s")
    print(f"  è¯„æµ‹ç»“æœ:")
    for metric, score in metrics.items():
        if metric not in ['num_queries', 'timestamp']:
            print(f"    {metric}: {score:.4f}")
    
    return metrics

def compare_with_baseline():
    """ä¸åŸºçº¿ç³»ç»Ÿå¯¹æ¯”"""
    print(f"\nğŸ“ˆ ä¸åŸºçº¿ç³»ç»Ÿå¯¹æ¯”")
    print("=" * 50)
    
    # åŸºçº¿ç»“æœ (ä»ä¹‹å‰çš„æµ‹è¯•)
    baseline_metrics = {
        'recall@5': 0.0189,
        'recall@10': 0.0189, 
        'ndcg@10': 0.1158,
        'map': 0.0189
    }
    
    # ä¼˜åŒ–ç³»ç»Ÿç»“æœ
    optimized_metrics = test_optimized_system()
    
    print(f"\nğŸ† æ€§èƒ½å¯¹æ¯”:")
    print(f"{'æŒ‡æ ‡':<12} {'åŸºçº¿':<10} {'ä¼˜åŒ–å':<10} {'æå‡':<10}")
    print("-" * 45)
    
    for metric in ['recall@5', 'recall@10', 'ndcg@10', 'map']:
        if metric in optimized_metrics:
            baseline = baseline_metrics.get(metric, 0)
            optimized = optimized_metrics.get(metric, 0)
            improvement = ((optimized - baseline) / baseline * 100) if baseline > 0 else 0
            
            print(f"{metric:<12} {baseline:<10.4f} {optimized:<10.4f} {improvement:>+7.1f}%")

def test_query_expansion():
    """æµ‹è¯•æŸ¥è¯¢æ‰©å±•æŠ€æœ¯"""
    print(f"\nğŸ” æŸ¥è¯¢æ‰©å±•ä¼˜åŒ–")
    print("=" * 50)
    
    def expand_query(query_text):
        """ç®€å•çš„æŸ¥è¯¢æ‰©å±•"""
        # æ·»åŠ åŒä¹‰è¯å’Œç›¸å…³è¯
        expansions = {
            'cancer': ['cancer', 'tumor', 'carcinoma', 'malignancy'],
            'breast': ['breast', 'mammary'],
            'statin': ['statin', 'cholesterol drug', 'HMG-CoA reductase inhibitor'],
            'treatment': ['treatment', 'therapy', 'medication'],
            'diet': ['diet', 'nutrition', 'food'],
            'obesity': ['obesity', 'overweight', 'BMI'],
        }
        
        expanded_terms = []
        words = query_text.lower().split()
        
        for word in words:
            expanded_terms.append(word)
            for key, synonyms in expansions.items():
                if key in word:
                    expanded_terms.extend([s for s in synonyms if s != word])
        
        return ' '.join(expanded_terms)
    
    from pipeline import FusionRAGPipeline
    from modules.utils.interfaces import Query
    
    pipeline = FusionRAGPipeline("configs/optimized_config.yaml")
    pipeline.load_data()
    
    # ä½¿ç”¨å·²æ„å»ºçš„ç´¢å¼•
    test_query_text = "breast cancer statin treatment"
    original_query = Query("test", test_query_text)
    expanded_query = Query("test_expanded", expand_query(test_query_text))
    
    print(f"åŸå§‹æŸ¥è¯¢: {original_query.text}")
    print(f"æ‰©å±•æŸ¥è¯¢: {expanded_query.text}")
    
    # å¯¹æ¯”ç»“æœ
    original_results = pipeline.search(original_query, top_k=20)
    expanded_results = pipeline.search(expanded_query, top_k=20)
    
    print(f"\nç»“æœå¯¹æ¯”:")
    print(f"åŸå§‹æŸ¥è¯¢æ‰¾åˆ° {len(original_results)} ä¸ªç»“æœ")
    print(f"æ‰©å±•æŸ¥è¯¢æ‰¾åˆ° {len(expanded_results)} ä¸ªç»“æœ")
    
    print(f"\nåŸå§‹æŸ¥è¯¢å‰3ä¸ªç»“æœ:")
    for i, result in enumerate(original_results[:3]):
        print(f"  {i+1}. [{result.final_score:.3f}] {result.document.title[:50]}...")
    
    print(f"\næ‰©å±•æŸ¥è¯¢å‰3ä¸ªç»“æœ:")
    for i, result in enumerate(expanded_results[:3]):
        print(f"  {i+1}. [{result.final_score:.3f}] {result.document.title[:50]}...")

def main():
    """ä¸»å‡½æ•°"""
    print("âš¡ FusionRAGç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–")
    print("=" * 60)
    
    try:
        # 1. åˆ›å»ºä¼˜åŒ–é…ç½®
        create_optimized_config()
        
        # 2. å¯¹æ¯”æµ‹è¯•
        compare_with_baseline()
        
        # 3. æµ‹è¯•æŸ¥è¯¢æ‰©å±•
        test_query_expansion()
        
        print(f"\nâœ… ä¼˜åŒ–å®Œæˆ!")
        print(f"ä¸»è¦æ”¹è¿›:")
        print(f"  1. ä½¿ç”¨æ›´å¼ºçš„å‘é‡æ¨¡å‹ (all-mpnet-base-v2)")
        print(f"  2. å¢åŠ æ£€ç´¢æ•°é‡ (top_k=200)")
        print(f"  3. ä½¿ç”¨RRFèåˆç­–ç•¥")
        print(f"  4. æ‰©å±•è¯„æµ‹æŒ‡æ ‡")
        print(f"  5. æŸ¥è¯¢æ‰©å±•æŠ€æœ¯")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ä¼˜åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()