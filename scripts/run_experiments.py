#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
FusionRAGå¤šæ•°æ®é›†å®éªŒè¿è¡Œè„šæœ¬

è¯¥è„šæœ¬ç”¨äºåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿è¡ŒFusionRAGç³»ç»Ÿçš„å®éªŒï¼Œæ”¯æŒï¼š
1. è‡ªåŠ¨åŠ è½½å’Œå¤„ç†æ•°æ®é›†
2. æ„å»ºç´¢å¼•
3. è¿è¡Œæ£€ç´¢å®éªŒ
4. è¯„ä¼°æ€§èƒ½
5. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""

import os
import sys
import time
import json
import argparse
import yaml
from pathlib import Path
import logging
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# è®¾ç½®ç¯å¢ƒå˜é‡é¿å…æ®µé”™è¯¯
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'

# å¯¼å…¥FusionRAGç³»ç»Ÿ
from fusionrag import FusionRAGSystem
from modules.utils.interfaces import Document, Query

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def check_data_availability(dataset_name: str) -> bool:
    """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å¯ç”¨
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        
    Returns:
        æ•°æ®é›†æ˜¯å¦å¯ç”¨
    """
    # æ„å»ºæ•°æ®æ–‡ä»¶è·¯å¾„
    # é¦–å…ˆæ£€æŸ¥æ–°æ ¼å¼è·¯å¾„ï¼ˆç›´æ¥åœ¨processedç›®å½•ä¸‹ï¼‰
    corpus_path = f"data/processed/{dataset_name}_corpus.jsonl"
    queries_path = f"data/processed/{dataset_name}_queries.jsonl"
    qrels_path = f"data/processed/{dataset_name}_qrels.tsv"
    
    # å¦‚æœæ–°æ ¼å¼è·¯å¾„ä¸å­˜åœ¨ï¼Œåˆ™æ£€æŸ¥æ—§æ ¼å¼è·¯å¾„ï¼ˆåœ¨dataset_nameå­ç›®å½•ä¸‹ï¼‰
    if not (Path(corpus_path).exists() and Path(queries_path).exists() and Path(qrels_path).exists()):
        corpus_path = f"data/processed/{dataset_name}/corpus.jsonl"
        queries_path = f"data/processed/{dataset_name}/queries.jsonl"
        qrels_path = f"data/processed/{dataset_name}/qrels.tsv"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    missing_files = []
    for file_path in [corpus_path, queries_path, qrels_path]:
        if not Path(file_path).exists():
            missing_files.append(file_path)
    
    if missing_files:
        logger.error(f"æ•°æ®é›† {dataset_name} ç¼ºå°‘å¿…è¦çš„æ•°æ®æ–‡ä»¶:")
        for file_path in missing_files:
            logger.error(f"  - {file_path}")
        return False
    
    return True


def download_and_preprocess_data(dataset_name: str) -> bool:
    """è‡ªåŠ¨ä¸‹è½½å’Œé¢„å¤„ç†æŒ‡å®šæ•°æ®é›†
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    logger.info(f"ğŸ”„ è‡ªåŠ¨ä¸‹è½½å’Œé¢„å¤„ç†{dataset_name}æ•°æ®é›†...")
    
    try:
        # ä¸‹è½½æ•°æ®
        logger.info(f"ğŸ“¥ ä¸‹è½½{dataset_name}æ•°æ®é›†...")
        from scripts.download_data import DataDownloader
        downloader = DataDownloader()
        success = downloader.download_beir_dataset(dataset_name)
        
        if not success:
            logger.error("æ•°æ®ä¸‹è½½å¤±è´¥")
            return False
        
        # é¢„å¤„ç†æ•°æ®
        logger.info("âš™ï¸ é¢„å¤„ç†æ•°æ®...")
        from scripts.preprocess_data import DataProcessor
        processor = DataProcessor()
        success = processor.process_beir_dataset(dataset_name)
        
        if not success:
            logger.error("æ•°æ®é¢„å¤„ç†å¤±è´¥")
            return False
        
        logger.info("âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        return False


def create_dataset_config(dataset_name: str, base_config_path: str) -> str:
    """ä¸ºæ•°æ®é›†åˆ›å»ºé…ç½®æ–‡ä»¶
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        base_config_path: åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ–°é…ç½®æ–‡ä»¶è·¯å¾„
    """
    # åŠ è½½åŸºç¡€é…ç½®
    with open(base_config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
        
    # å¤„ç†è‡ªåŠ¨è®¾å¤‡é…ç½®
    if config.get("system", {}).get("device") == "auto":
        import torch
        config["system"]["device"] = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"è‡ªåŠ¨é€‰æ‹©è®¾å¤‡: {config['system']['device']}")
    
    # æ›´æ–°æ•°æ®è·¯å¾„
    # æ£€æŸ¥æ–°æ ¼å¼è·¯å¾„ï¼ˆç›´æ¥åœ¨processedç›®å½•ä¸‹ï¼‰
    corpus_path = f"data/processed/{dataset_name}_corpus.jsonl"
    queries_path = f"data/processed/{dataset_name}_queries.jsonl"
    qrels_path = f"data/processed/{dataset_name}_qrels.tsv"
    
    # å¦‚æœæ–°æ ¼å¼è·¯å¾„ä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨æ—§æ ¼å¼è·¯å¾„ï¼ˆåœ¨dataset_nameå­ç›®å½•ä¸‹ï¼‰
    if not (Path(corpus_path).exists() and Path(queries_path).exists() and Path(qrels_path).exists()):
        corpus_path = f"data/processed/{dataset_name}/corpus.jsonl"
        queries_path = f"data/processed/{dataset_name}/queries.jsonl"
        qrels_path = f"data/processed/{dataset_name}/qrels.tsv"
    
    config['data']['corpus_path'] = corpus_path
    config['data']['queries_path'] = queries_path
    config['data']['qrels_path'] = qrels_path
    
    # æ›´æ–°å…ƒæ•°æ®
    if 'metadata' not in config:
        config['metadata'] = {}
    config['metadata']['dataset'] = dataset_name
    
    # ä¿å­˜æ–°é…ç½®
    output_dir = Path("configs/datasets")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_path = output_dir / f"{dataset_name}_config.yaml"
    with open(output_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"å·²ä¸ºæ•°æ®é›† {dataset_name} åˆ›å»ºé…ç½®æ–‡ä»¶: {output_path}")
    return str(output_path)


def run_experiment(config_path: str, force_rebuild: bool = False, auto_download: bool = True) -> Dict[str, Any]:
    """ä½¿ç”¨æŒ‡å®šé…ç½®æ–‡ä»¶è¿è¡Œå®éªŒ
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
        auto_download: æ˜¯å¦è‡ªåŠ¨ä¸‹è½½æ•°æ®
        
    Returns:
        å®éªŒç»“æœ
    """
    logger.info("ğŸš€ å¼€å§‹é…ç½®åŒ–å®éªŒ")
    logger.info("=" * 60)
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not Path(config_path).exists():
        logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return {}
    
    # åŠ è½½é…ç½®è·å–æ•°æ®é›†ä¿¡æ¯
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
        
    # å¤„ç†è‡ªåŠ¨è®¾å¤‡é…ç½®
    if config.get("system", {}).get("device") == "auto":
        import torch
        config["system"]["device"] = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"è‡ªåŠ¨é€‰æ‹©è®¾å¤‡: {config['system']['device']}")
        
        # ä¿å­˜æ›´æ–°åçš„é…ç½®
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    dataset_name = config.get('metadata', {}).get('dataset', 'unknown')
    
    logger.info(f"ğŸ“‹ é…ç½®æ–‡ä»¶: {config_path}")
    logger.info(f"ğŸ“Š æ•°æ®é›†: {dataset_name}")
    
    # æ£€æŸ¥æ•°æ®
    if not check_data_availability(dataset_name):
        if auto_download:
            logger.info("å°è¯•è‡ªåŠ¨ä¸‹è½½æ•°æ®...")
            if not download_and_preprocess_data(dataset_name):
                logger.error("æ— æ³•è·å–æµ‹è¯•æ•°æ®ï¼Œæµ‹è¯•ç»ˆæ­¢")
                return {}
        else:
            logger.error("æ•°æ®æ–‡ä»¶ç¼ºå¤±ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return {}
    
    # åˆå§‹åŒ–FusionRAGç³»ç»Ÿ
    start_time = time.time()
    system = FusionRAGSystem(config_path=config_path)
    init_time = time.time() - start_time
    logger.info(f"ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ ({init_time:.2f}s)")
    
    # åŠ è½½æ–‡æ¡£
    logger.info("ğŸ“ åŠ è½½æ–‡æ¡£...")
    start_time = time.time()
    documents = system.load_documents()
    load_time = time.time() - start_time
    logger.info(f"æ–‡æ¡£åŠ è½½å®Œæˆ ({load_time:.2f}s), å…± {len(documents)} ä¸ªæ–‡æ¡£")
    
    # æ„å»ºç´¢å¼•
    logger.info("ğŸ”¨ æ„å»ºç´¢å¼•...")
    start_time = time.time()
    system.index_documents(documents, force_rebuild=force_rebuild)
    build_time = time.time() - start_time
    logger.info(f"ç´¢å¼•æ„å»ºå®Œæˆ ({build_time:.2f}s)")
    
    # è¿è¡Œè¯„ä¼°
    logger.info("ğŸ“Š è¿è¡Œæ€§èƒ½è¯„ä¼°...")
    start_time = time.time()
    evaluation_results = system.evaluate(dataset_name=dataset_name)
    eval_time = time.time() - start_time
    logger.info(f"è¯„ä¼°å®Œæˆ ({eval_time:.2f}s)")
    
    # æ˜¾ç¤ºç»“æœ
    if evaluation_results:
        logger.info("ğŸ¯ å®éªŒç»“æœ:")
        logger.info("=" * 60)
        
        # æ˜¾ç¤ºä¸»è¦æŒ‡æ ‡
        for dataset, results in evaluation_results.items():
            logger.info(f"æ•°æ®é›†: {dataset}")
            
            for retriever_name, metrics in results.items():
                logger.info(f"æ£€ç´¢å™¨: {retriever_name}")
                
                for metric_name, value in metrics.items():
                    if isinstance(value, (int, float)):
                        logger.info(f"  {metric_name}: {value:.4f}")
    
    # æ€§èƒ½æ€»ç»“
    total_time = init_time + load_time + build_time + eval_time
    logger.info(f"\nâ±ï¸ æ€§èƒ½æ€»ç»“:")
    logger.info(f"  ç³»ç»Ÿåˆå§‹åŒ–: {init_time:.2f}s")
    logger.info(f"  æ•°æ®åŠ è½½: {load_time:.2f}s")
    logger.info(f"  ç´¢å¼•æ„å»º: {build_time:.2f}s") 
    logger.info(f"  æ€§èƒ½è¯„ä¼°: {eval_time:.2f}s")
    logger.info(f"  æ€»è€—æ—¶: {total_time:.2f}s")
    
    # ä¿å­˜ç»“æœ
    results_summary = {
        "config": config_path,
        "dataset": dataset_name,
        "performance": {
            "init_time": init_time,
            "load_time": load_time,
            "build_time": build_time,
            "eval_time": eval_time,
            "total_time": total_time
        },
        "evaluation": evaluation_results
    }
    
    # ç”Ÿæˆç»“æœæ–‡ä»¶å
    config_name = Path(config_path).stem
    output_file = f"reports/{config_name}_results.json"
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return results_summary


def run_multi_dataset_experiments(datasets: List[str], base_config_path: str, force_rebuild: bool = False, auto_download: bool = True) -> Dict[str, Any]:
    """åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿è¡Œå®éªŒ
    
    Args:
        datasets: æ•°æ®é›†åç§°åˆ—è¡¨
        base_config_path: åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
        auto_download: æ˜¯å¦è‡ªåŠ¨ä¸‹è½½æ•°æ®
        
    Returns:
        æ‰€æœ‰å®éªŒç»“æœ
    """
    logger.info(f"ğŸš€ å¼€å§‹åœ¨ {len(datasets)} ä¸ªæ•°æ®é›†ä¸Šè¿è¡Œå®éªŒ")
    logger.info("=" * 60)
    
    all_results = {}
    
    for dataset_name in datasets:
        logger.info(f"\nğŸ“Š æ•°æ®é›†: {dataset_name}")
        logger.info("-" * 40)
        
        # ä¸ºæ•°æ®é›†åˆ›å»ºé…ç½®
        config_path = create_dataset_config(dataset_name, base_config_path)
        
        # è¿è¡Œå®éªŒ
        results = run_experiment(config_path, force_rebuild, auto_download)
        all_results[dataset_name] = results
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    output_file = f"reports/multi_dataset_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nğŸ“„ æ‰€æœ‰æ•°æ®é›†çš„æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return all_results


def compare_retrievers(results_file: str):
    """æ¯”è¾ƒä¸åŒæ£€ç´¢å™¨çš„æ€§èƒ½
    
    Args:
        results_file: ç»“æœæ–‡ä»¶è·¯å¾„
    """
    logger.info("ğŸ”„ æ¯”è¾ƒæ£€ç´¢å™¨æ€§èƒ½...")
    
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        if not results:
            logger.warning("ç»“æœæ–‡ä»¶ä¸ºç©º")
            return
        
        # æå–è¯„ä¼°ç»“æœ
        if isinstance(results, dict) and 'evaluation' in results:
            # å•æ•°æ®é›†ç»“æœ
            evaluation_results = results['evaluation']
        elif isinstance(results, dict) and all(isinstance(v, dict) for v in results.values()):
            # å¤šæ•°æ®é›†ç»“æœ
            evaluation_results = {}
            for dataset, dataset_results in results.items():
                if 'evaluation' in dataset_results:
                    evaluation_results[dataset] = dataset_results['evaluation']
        else:
            logger.warning("æ— æ³•è§£æç»“æœæ–‡ä»¶æ ¼å¼")
            return
        
        # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
        logger.info("\nğŸ“Š æ£€ç´¢å™¨æ€§èƒ½æ¯”è¾ƒ:")
        logger.info("=" * 60)
        
        for dataset, dataset_results in evaluation_results.items():
            logger.info(f"\næ•°æ®é›†: {dataset}")
            logger.info("-" * 40)
            
            # æå–æ‰€æœ‰æ£€ç´¢å™¨å’ŒæŒ‡æ ‡
            retrievers = set()
            metrics = set()
            
            for retriever_results in dataset_results.values():
                for retriever, retriever_metrics in retriever_results.items():
                    retrievers.add(retriever)
                    metrics.update(retriever_metrics.keys())
            
            # æŒ‰æŒ‡æ ‡æ¯”è¾ƒ
            for metric in sorted(metrics):
                logger.info(f"\næŒ‡æ ‡: {metric}")
                
                # æ”¶é›†æ‰€æœ‰æ£€ç´¢å™¨åœ¨æ­¤æŒ‡æ ‡ä¸Šçš„æ€§èƒ½
                retriever_scores = {}
                
                for retriever_results in dataset_results.values():
                    for retriever, retriever_metrics in retriever_results.items():
                        if metric in retriever_metrics:
                            retriever_scores[retriever] = retriever_metrics[metric]
                
                # æŒ‰æ€§èƒ½æ’åº
                sorted_retrievers = sorted(retriever_scores.items(), key=lambda x: x[1], reverse=True)
                
                # æ˜¾ç¤ºæ’åºç»“æœ
                for i, (retriever, score) in enumerate(sorted_retrievers):
                    logger.info(f"  {i+1}. {retriever}: {score:.4f}")
    
    except Exception as e:
        logger.error(f"æ¯”è¾ƒæ£€ç´¢å™¨æ€§èƒ½å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="FusionRAGå¤šæ•°æ®é›†å®éªŒè¿è¡Œè„šæœ¬")
    parser.add_argument("--config", type=str, default="configs/fusionrag_config.yaml", help="åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--datasets", type=str, nargs="+", help="è¦è¿è¡Œçš„æ•°æ®é›†åˆ—è¡¨")
    parser.add_argument("--all-datasets", action="store_true", help="è¿è¡Œæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†")
    parser.add_argument("--force-rebuild", action="store_true", help="å¼ºåˆ¶é‡å»ºç´¢å¼•")
    parser.add_argument("--no-auto-download", action="store_true", help="ä¸è‡ªåŠ¨ä¸‹è½½æ•°æ®")
    parser.add_argument("--compare", type=str, help="æ¯”è¾ƒæŒ‡å®šç»“æœæ–‡ä»¶ä¸­çš„æ£€ç´¢å™¨æ€§èƒ½")
    
    args = parser.parse_args()
    
    logger.info("ğŸ¯ FusionRAGå¤šæ•°æ®é›†å®éªŒè¿è¡Œè„šæœ¬")
    logger.info("=" * 60)
    
    if args.compare:
        # æ¯”è¾ƒæ£€ç´¢å™¨æ€§èƒ½
        compare_retrievers(args.compare)
        return
    
    # ç¡®å®šè¦è¿è¡Œçš„æ•°æ®é›†
    if args.all_datasets:
        # è·å–æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†
        datasets = [
            "nfcorpus",
            "scifact",
            "scidocs",
            "fiqa",
            "trec-covid",
            "arguana",
            "webis-touche2020"
        ]
        logger.info(f"å°†åœ¨æ‰€æœ‰ {len(datasets)} ä¸ªæ•°æ®é›†ä¸Šè¿è¡Œå®éªŒ")
    elif args.datasets:
        datasets = args.datasets
        logger.info(f"å°†åœ¨æŒ‡å®šçš„ {len(datasets)} ä¸ªæ•°æ®é›†ä¸Šè¿è¡Œå®éªŒ")
    else:
        # é»˜è®¤ä½¿ç”¨NFCorpusæ•°æ®é›†
        datasets = ["nfcorpus"]
        logger.info("æœªæŒ‡å®šæ•°æ®é›†ï¼Œå°†ä½¿ç”¨é»˜è®¤çš„NFCorpusæ•°æ®é›†")
    
    # è¿è¡Œå¤šæ•°æ®é›†å®éªŒ
    run_multi_dataset_experiments(
        datasets=datasets,
        base_config_path=args.config,
        force_rebuild=args.force_rebuild,
        auto_download=not args.no_auto_download
    )
    
    logger.info("\nâœ… æ‰€æœ‰å®éªŒå®Œæˆ!")


if __name__ == "__main__":
    main()