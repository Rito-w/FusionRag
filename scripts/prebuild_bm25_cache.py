#!/usr/bin/env python3
"""
é¢„æ„å»ºæ‰€æœ‰æ•°æ®é›†çš„BM25ç´¢å¼•ç¼“å­˜
ä¸ºäº†åŠ é€Ÿåç»­å®éªŒï¼Œæå‰æ„å»ºæ‰€æœ‰BEIRæ•°æ®é›†çš„BM25ç´¢å¼•
"""

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import json
import time
from pathlib import Path
from typing import List, Dict, Any

from modules.retriever.bm25_retriever import BM25Retriever
from modules.utils.interfaces import Document


def load_dataset_documents(dataset_name: str) -> List[Document]:
    """åŠ è½½æ•°æ®é›†æ–‡æ¡£"""
    print(f"åŠ è½½æ•°æ®é›†: {dataset_name}")
    
    documents = []
    corpus_path = f"data/processed/{dataset_name}_corpus.jsonl"
    
    if not os.path.exists(corpus_path):
        print(f"âš ï¸ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {corpus_path}")
        return documents
    
    print(f"åŠ è½½æ–‡æ¡£: {corpus_path}")
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            doc = Document(
                doc_id=data['doc_id'],
                title=data.get('title', ''),
                text=data.get('text', '')
            )
            documents.append(doc)
    
    print(f"æ•°æ®é›† {dataset_name} åŠ è½½å®Œæˆ: æ–‡æ¡£æ•°é‡={len(documents)}")
    return documents


def prebuild_bm25_index(dataset_name: str, config: Dict[str, Any] = None) -> bool:
    """ä¸ºæŒ‡å®šæ•°æ®é›†é¢„æ„å»ºBM25ç´¢å¼•"""
    print(f"\n=== é¢„æ„å»º {dataset_name} BM25ç´¢å¼• ===")
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å·²å­˜åœ¨
    cache_dir = Path("checkpoints/retriever_cache")
    cache_dir.mkdir(parents=True, exist_ok=True)
    cache_path = cache_dir / f"bm25_{dataset_name}_index.pkl"
    
    if cache_path.exists():
        print(f"âœ… {dataset_name} BM25ç´¢å¼•ç¼“å­˜å·²å­˜åœ¨: {cache_path}")
        return True
    
    # åŠ è½½æ•°æ®é›†
    documents = load_dataset_documents(dataset_name)
    if not documents:
        print(f"âŒ {dataset_name} æ•°æ®é›†åŠ è½½å¤±è´¥")
        return False
    
    # åˆ›å»ºBM25æ£€ç´¢å™¨
    bm25_config = config.get('bm25', {}) if config else {}
    retriever = BM25Retriever(bm25_config)
    
    # æ„å»ºç´¢å¼•
    print(f"ğŸ”„ å¼€å§‹æ„å»º {dataset_name} BM25ç´¢å¼•...")
    start_time = time.time()
    
    try:
        retriever.build_index(documents)
        
        # ä¿å­˜ç´¢å¼•åˆ°ç¼“å­˜
        retriever.save_index(str(cache_path))
        
        end_time = time.time()
        print(f"âœ… {dataset_name} BM25ç´¢å¼•æ„å»ºå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
        print(f"ğŸ’¾ ç´¢å¼•å·²ç¼“å­˜åˆ°: {cache_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ {dataset_name} BM25ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°ï¼šä¸ºæ‰€æœ‰BEIRæ•°æ®é›†é¢„æ„å»ºBM25ç´¢å¼•"""
    print("ğŸš€ å¼€å§‹é¢„æ„å»ºæ‰€æœ‰æ•°æ®é›†çš„BM25ç´¢å¼•ç¼“å­˜")
    print("=" * 60)
    
    # æ‰€æœ‰BEIRæ•°æ®é›†
    datasets = [
        "nfcorpus", 
        "scifact", 
        "fiqa", 
        "arguana", 
        "quora", 
        "scidocs", 
        "trec-covid"
    ]
    
    # åŠ è½½é…ç½®
    config_path = "configs/cloud_experiments.json"
    config = {}
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config = json.load(f)
        print(f"ğŸ“‹ å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    else:
        print("âš ï¸ ä½¿ç”¨é»˜è®¤BM25é…ç½®")
        config = {"bm25": {"k1": 1.2, "b": 0.75}}
    
    # ç»Ÿè®¡ç»“æœ
    success_count = 0
    failed_datasets = []
    total_start_time = time.time()
    
    # ä¸ºæ¯ä¸ªæ•°æ®é›†æ„å»ºç´¢å¼•
    for dataset in datasets:
        success = prebuild_bm25_index(dataset, config)
        if success:
            success_count += 1
        else:
            failed_datasets.append(dataset)
    
    # æ€»ç»“
    total_end_time = time.time()
    total_time = total_end_time - total_start_time
    
    print("\n" + "=" * 60)
    print("ğŸ“Š BM25ç´¢å¼•é¢„æ„å»ºæ€»ç»“")
    print(f"âœ… æˆåŠŸæ„å»º: {success_count}/{len(datasets)} ä¸ªæ•°æ®é›†")
    print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
    
    if failed_datasets:
        print(f"âŒ å¤±è´¥çš„æ•°æ®é›†: {', '.join(failed_datasets)}")
    else:
        print("ğŸ‰ æ‰€æœ‰æ•°æ®é›†çš„BM25ç´¢å¼•ç¼“å­˜æ„å»ºå®Œæˆï¼")
    
    # æ˜¾ç¤ºç¼“å­˜æ–‡ä»¶ä¿¡æ¯
    cache_dir = Path("checkpoints/retriever_cache")
    if cache_dir.exists():
        print(f"\nğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}")
        bm25_files = list(cache_dir.glob("bm25_*_index.pkl"))
        print(f"ğŸ“¦ BM25ç¼“å­˜æ–‡ä»¶æ•°é‡: {len(bm25_files)}")
        for cache_file in sorted(bm25_files):
            file_size = cache_file.stat().st_size / (1024 * 1024)  # MB
            print(f"   - {cache_file.name} ({file_size:.1f} MB)")


if __name__ == "__main__":
    main()