"""
å‘é‡æ£€ç´¢å™¨å®ç°
åŸºäºsentence-transformerså’ŒFAISSçš„å¯†é›†å‘é‡æ£€ç´¢
"""

import numpy as np
import faiss
from typing import List, Dict, Any, Optional
from pathlib import Path
from sentence_transformers import SentenceTransformer
import pickle
import hashlib
import json

from ..utils.interfaces import BaseRetriever, Document, Query, RetrievalResult
from ..utils.common import FileUtils, TextProcessor


class DenseRetriever(BaseRetriever):
    """å¯†é›†å‘é‡æ£€ç´¢å™¨"""
    
    def __init__(self, name: str = "dense", config: Dict[str, Any] = None):
        super().__init__(name, config or {})
        
        # æ¨¡å‹é…ç½®
        self.model_name = self.config.get('model_name', 'sentence-transformers/all-MiniLM-L6-v2')
        self.embedding_dim = self.config.get('embedding_dim', 384)
        self.batch_size = self.config.get('batch_size', 32)
        self.max_length = self.config.get('max_length', 512)
        self.normalize_embeddings = self.config.get('normalize_embeddings', True)  # æ˜¯å¦L2å½’ä¸€åŒ–ï¼Œå®˜æ–¹æ¨è
        
        # æ¨¡å‹å’Œç´¢å¼•
        self.model: Optional[SentenceTransformer] = None
        self.index: Optional[faiss.Index] = None
        self.documents: List[Document] = []
        self.document_embeddings: Optional[np.ndarray] = None

        # ç¼“å­˜ç›¸å…³
        self.documents_hash: Optional[str] = None
        self.embeddings_cache_path: Optional[str] = None

        # åˆå§‹åŒ–æ¨¡å‹
        self._load_model()
    
    def _load_model(self) -> None:
        """åŠ è½½sentence-transformersæ¨¡å‹"""
        try:
            print(f"åŠ è½½æ¨¡å‹: {self.model_name}")
            self.model = SentenceTransformer(self.model_name)
            
            # éªŒè¯åµŒå…¥ç»´åº¦
            test_embedding = self.model.encode(["æµ‹è¯•æ–‡æœ¬"], show_progress_bar=False)
            actual_dim = test_embedding.shape[1]
            
            if actual_dim != self.embedding_dim:
                print(f"è­¦å‘Š: é…ç½®çš„åµŒå…¥ç»´åº¦({self.embedding_dim})ä¸å®é™…ç»´åº¦({actual_dim})ä¸åŒ¹é…ï¼Œä½¿ç”¨å®é™…ç»´åº¦")
                self.embedding_dim = actual_dim
            
            print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼ŒåµŒå…¥ç»´åº¦: {self.embedding_dim}")
            
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
    
    def _prepare_text(self, document: Document) -> str:
        """å‡†å¤‡æ–‡æ¡£æ–‡æœ¬ç”¨äºç¼–ç """
        # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹
        text = f"{document.title} {document.text}".strip()
        
        # æ–‡æœ¬é¢„å¤„ç†
        text = TextProcessor.normalize_text(text)
        
        # æˆªæ–­é•¿æ–‡æœ¬
        text = TextProcessor.truncate_text(text, self.max_length)
        
        return text
    
    def _encode_texts(self, texts: List[str], show_progress: bool = True) -> np.ndarray:
        """æ‰¹é‡ç¼–ç æ–‡æœ¬ï¼ˆæ”¯æŒL2å½’ä¸€åŒ–ï¼Œå®˜æ–¹BEIRæ¨èï¼‰"""
        if not texts:
            return np.array([]).reshape(0, self.embedding_dim)
        try:
            embeddings = self.model.encode(
                texts,
                batch_size=self.batch_size,
                show_progress_bar=show_progress,
                convert_to_numpy=True,
                normalize_embeddings=self.normalize_embeddings  # ç”±é…ç½®æ§åˆ¶
            )
            return embeddings.astype(np.float32)
        except Exception as e:
            raise RuntimeError(f"æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
    
    def build_index(self, documents: List[Document], force_rebuild: bool = False, dataset_name: str = None) -> None:
        """æ„å»ºå‘é‡ç´¢å¼•"""
        print(f"å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")

        if not self.model:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")

        self.documents = documents

        # ä»æ–‡æ¡£è·¯å¾„æ¨æ–­æ•°æ®é›†åç§°
        if dataset_name is None:
            dataset_name = self._infer_dataset_name(documents)

        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜çš„å‘é‡
        print(f"ğŸ” æ£€æŸ¥å‘é‡ç¼“å­˜ (force_rebuild={force_rebuild}, dataset={dataset_name})")
        if not force_rebuild and self._can_use_cached_embeddings(documents, dataset_name):
            print("âœ… ä½¿ç”¨ç¼“å­˜çš„æ–‡æ¡£å‘é‡ï¼Œè·³è¿‡é‡æ–°ç¼–ç ")
            self._build_faiss_index()
        else:
            # éœ€è¦é‡æ–°ç¼–ç 
            print("ğŸ”„ éœ€è¦é‡æ–°ç¼–ç æ–‡æ¡£å‘é‡")
            self._encode_and_build_index(documents, dataset_name)

        self.is_built = True
        print(f"å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œç´¢å¼•å¤§å°: {self.index.ntotal}")

    def _infer_dataset_name(self, documents: List[Document]) -> str:
        """ä»æ–‡æ¡£æ¨æ–­æ•°æ®é›†åç§°"""
        if not documents:
            return "unknown"

        # é¦–å…ˆæ£€æŸ¥æ–‡æ¡£çš„metadataä¸­æ˜¯å¦æœ‰æ•°æ®é›†ä¿¡æ¯
        first_doc = documents[0]
        if hasattr(first_doc, 'metadata') and first_doc.metadata:
            if 'dataset' in first_doc.metadata:
                dataset_name = first_doc.metadata['dataset']
                print(f"ğŸ“‹ ä»metadataæ¨æ–­æ•°æ®é›†åç§°: {dataset_name}")
                return dataset_name

        # ä»æ–‡æ¡£IDæ¨æ–­
        first_doc_id = first_doc.doc_id
        print(f"ğŸ” ä»æ–‡æ¡£IDæ¨æ–­æ•°æ®é›†: {first_doc_id}")

        if "MED-" in first_doc_id:
            return "nfcorpus"
        elif "covid" in first_doc_id.lower():
            return "trec-covid"
        elif "nq" in first_doc_id.lower():
            return "natural_questions"
        elif first_doc_id.startswith("test-") and any(x in first_doc_id for x in ["pro", "con"]):
            return "arguana"
        elif first_doc_id.isdigit():
            # çº¯æ•°å­—IDï¼Œå¯èƒ½æ˜¯scifact, fiqa, quoraç­‰
            if len(documents) > 1000:  # æ ¹æ®æ•°æ®é›†å¤§å°æ¨æ–­
                if len(documents) > 100000:
                    return "quora"  # quoraæœ‰522kæ–‡æ¡£
                elif len(documents) > 50000:
                    return "fiqa"  # fiqaæœ‰57kæ–‡æ¡£
                elif len(documents) > 20000:
                    return "scidocs"  # scidocsæœ‰25kæ–‡æ¡£
                else:
                    return "scifact"  # scifactæœ‰5kæ–‡æ¡£
            else:
                return "scifact"
        elif len(first_doc_id) == 40 and all(c in '0123456789abcdef' for c in first_doc_id):
            # 40ä½åå…­è¿›åˆ¶å“ˆå¸Œï¼Œå¯èƒ½æ˜¯scidocs
            if len(documents) > 20000:
                return "scidocs"
            else:
                return "unknown"
        elif len(first_doc_id) == 8 and first_doc_id.isalnum():
            # 8ä½å­—æ¯æ•°å­—IDï¼Œå¯èƒ½æ˜¯trec-covid
            if len(documents) > 100000:
                return "trec-covid"
            else:
                return "unknown"
        elif len(documents) > 500000:
            return "quora"  # quoraæ˜¯æœ€å¤§çš„æ•°æ®é›†
        else:
            print(f"âš ï¸ æ— æ³•æ¨æ–­æ•°æ®é›†åç§°ï¼Œæ–‡æ¡£ID: {first_doc_id}, æ–‡æ¡£æ•°é‡: {len(documents)}")
            return "unknown"

    def _calculate_documents_hash(self, documents: List[Document]) -> str:
        """è®¡ç®—æ–‡æ¡£é›†åˆçš„å“ˆå¸Œå€¼"""
        # åˆ›å»ºæ–‡æ¡£å†…å®¹çš„å“ˆå¸Œ
        doc_contents = []
        for doc in documents:
            content = f"{doc.doc_id}|{doc.title}|{doc.text}"
            doc_contents.append(content)

        # è®¡ç®—æ•´ä½“å“ˆå¸Œ
        combined_content = "\n".join(sorted(doc_contents))
        return hashlib.md5(combined_content.encode('utf-8')).hexdigest()

    def _get_embeddings_cache_path(self, dataset_name: str = None) -> str:
        """è·å–å‘é‡ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_dir = Path("checkpoints/embeddings_cache")
        cache_dir.mkdir(parents=True, exist_ok=True)

        # ä»æ–‡æ¡£è·¯å¾„æ¨æ–­æ•°æ®é›†åç§°ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
        if dataset_name is None:
            dataset_name = "unknown"
            # å¯ä»¥ä»é…ç½®æˆ–å…¶ä»–åœ°æ–¹è·å–æ•°æ®é›†åç§°

        # æ¸…ç†æ¨¡å‹åç§°ç”¨äºæ–‡ä»¶å
        model_name_safe = self.model_name.replace("/", "_").replace(":", "_").replace("-", "_")

        # ç®€å•ç›´è§‚çš„å‘½åï¼šæ•°æ®é›†_æ¨¡å‹åç§°.pkl
        cache_filename = f"{dataset_name}_{model_name_safe}_embeddings.pkl"

        return str(cache_dir / cache_filename)

    def _can_use_cached_embeddings(self, documents: List[Document], dataset_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜çš„å‘é‡"""
        # è®¡ç®—å½“å‰æ–‡æ¡£é›†åˆçš„å“ˆå¸Œï¼ˆç”¨äºéªŒè¯ï¼‰
        current_hash = self._calculate_documents_hash(documents)

        # æ£€æŸ¥å†…å­˜ä¸­çš„ç¼“å­˜
        if (self.document_embeddings is not None and
            self.documents_hash == current_hash and
            len(self.documents) == len(documents)):
            print("âœ… ä½¿ç”¨å†…å­˜ä¸­çš„å‘é‡ç¼“å­˜")
            return True

        # æ£€æŸ¥ç£ç›˜ç¼“å­˜
        cache_path = self._get_embeddings_cache_path(dataset_name)
        if Path(cache_path).exists():
            try:
                print(f"ğŸ“ ä»ç£ç›˜åŠ è½½å‘é‡ç¼“å­˜: {cache_path}")
                cache_data = FileUtils.load_pickle(cache_path)

                # è¯¦ç»†çš„éªŒè¯ä¿¡æ¯
                cached_model = cache_data.get('model_name', 'unknown')
                cached_dataset = cache_data.get('dataset_name', 'unknown')
                cached_docs = cache_data.get('documents', [])

                print(f"ğŸ” ç¼“å­˜éªŒè¯:")
                print(f"   æ¨¡å‹åŒ¹é…: {cached_model} == {self.model_name} -> {cached_model == self.model_name}")
                print(f"   æ•°æ®é›†åŒ¹é…: {cached_dataset} == {dataset_name} -> {cached_dataset == dataset_name}")
                print(f"   æ–‡æ¡£æ•°é‡: {len(cached_docs)} vs {len(documents)}")

                # éªŒè¯ç¼“å­˜æ•°æ®
                if (cached_model == self.model_name and
                    cached_dataset == dataset_name and
                    'embeddings' in cache_data and
                    'documents' in cache_data):

                    # é¢å¤–éªŒè¯ï¼šæ£€æŸ¥æ–‡æ¡£æ•°é‡æ˜¯å¦åŒ¹é…æˆ–æ˜¯å­é›†å…³ç³»
                    if len(cached_docs) == len(documents):
                        # å®Œå…¨åŒ¹é…æƒ…å†µ
                        docs_match = True
                        check_count = min(5, len(documents))
                        for i in range(check_count):
                            if cached_docs[i].doc_id != documents[i].doc_id:
                                docs_match = False
                                print(f"   æ–‡æ¡£IDä¸åŒ¹é… [{i}]: {cached_docs[i].doc_id} vs {documents[i].doc_id}")
                                break

                        if docs_match:
                            self.document_embeddings = cache_data['embeddings']
                            self.documents = cached_docs
                            self.documents_hash = current_hash
                            self.embeddings_cache_path = cache_path

                            print(f"âœ… æˆåŠŸåŠ è½½ç¼“å­˜å‘é‡ï¼Œæ–‡æ¡£æ•°é‡: {len(self.documents)}")
                            return True
                        else:
                            print(f"âš ï¸ æ–‡æ¡£å†…å®¹ä¸åŒ¹é…ï¼Œéœ€è¦é‡æ–°ç¼–ç ")
                    
                    elif len(cached_docs) > len(documents):
                        # å­é›†åŒ¹é…æƒ…å†µï¼šå½“å‰æ–‡æ¡£æ˜¯ç¼“å­˜æ–‡æ¡£çš„å­é›†
                        print(f"ğŸ” æ£€æŸ¥å­é›†åŒ¹é…: ç¼“å­˜{len(cached_docs)}ä¸ªæ–‡æ¡£ vs å½“å‰{len(documents)}ä¸ªæ–‡æ¡£")
                        
                        # åˆ›å»ºç¼“å­˜æ–‡æ¡£IDåˆ°ç´¢å¼•çš„æ˜ å°„
                        cached_doc_map = {doc.doc_id: i for i, doc in enumerate(cached_docs)}
                        
                        # æ£€æŸ¥å½“å‰æ–‡æ¡£æ˜¯å¦éƒ½åœ¨ç¼“å­˜ä¸­
                        subset_indices = []
                        subset_docs = []
                        
                        for doc in documents:
                            if doc.doc_id in cached_doc_map:
                                idx = cached_doc_map[doc.doc_id]
                                subset_indices.append(idx)
                                subset_docs.append(cached_docs[idx])
                            else:
                                print(f"   æ–‡æ¡£ {doc.doc_id} ä¸åœ¨ç¼“å­˜ä¸­")
                                break
                        else:
                            # æ‰€æœ‰æ–‡æ¡£éƒ½åœ¨ç¼“å­˜ä¸­ï¼Œæå–å¯¹åº”çš„å‘é‡
                            import numpy as np
                            subset_embeddings = cache_data['embeddings'][subset_indices]
                            
                            self.document_embeddings = subset_embeddings
                            self.documents = subset_docs
                            self.documents_hash = current_hash
                            self.embeddings_cache_path = cache_path
                            
                            print(f"âœ… æˆåŠŸä»ç¼“å­˜æå–å­é›†å‘é‡ï¼Œæ–‡æ¡£æ•°é‡: {len(self.documents)}")
                            return True
                        
                        print(f"âš ï¸ å­é›†åŒ¹é…å¤±è´¥ï¼Œéœ€è¦é‡æ–°ç¼–ç ")
                    else:
                        print(f"âš ï¸ ç¼“å­˜æ–‡æ¡£æ•°é‡ä¸è¶³: {len(cached_docs)} < {len(documents)}")
                else:
                    print(f"âš ï¸ ç¼“å­˜éªŒè¯å¤±è´¥")

            except Exception as e:
                print(f"âš ï¸ åŠ è½½å‘é‡ç¼“å­˜å¤±è´¥: {e}")

        return False

    def _encode_and_build_index(self, documents: List[Document], dataset_name: str) -> None:
        """ç¼–ç æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•"""
        # å‡†å¤‡æ–‡æ¡£æ–‡æœ¬
        texts = []
        for doc in documents:
            text = self._prepare_text(doc)
            texts.append(text)

        print("æ­£åœ¨ç¼–ç æ–‡æ¡£...")
        # ç¼–ç æ–‡æ¡£
        self.document_embeddings = self._encode_texts(texts, show_progress=True)

        # ä¿å­˜å‘é‡åˆ°ç¼“å­˜
        self._save_embeddings_cache(documents, dataset_name)

        # æ„å»ºFAISSç´¢å¼•
        self._build_faiss_index()

    def _save_embeddings_cache(self, documents: List[Document], dataset_name: str) -> None:
        """ä¿å­˜å‘é‡åˆ°ç¼“å­˜"""
        try:
            # è®¡ç®—æ–‡æ¡£å“ˆå¸Œï¼ˆç”¨äºéªŒè¯ï¼‰
            documents_hash = self._calculate_documents_hash(documents)
            cache_path = self._get_embeddings_cache_path(dataset_name)

            # å‡†å¤‡ç¼“å­˜æ•°æ®
            cache_data = {
                'model_name': self.model_name,
                'embedding_dim': self.embedding_dim,
                'dataset_name': dataset_name,
                'documents_hash': documents_hash,
                'documents': documents,
                'embeddings': self.document_embeddings,
                'document_count': len(documents),
                'created_at': str(Path().cwd()),  # ç®€å•çš„æ—¶é—´æˆ³
            }

            # ä¿å­˜åˆ°ç£ç›˜
            FileUtils.save_pickle(cache_data, cache_path)

            # æ›´æ–°å®ä¾‹å˜é‡
            self.documents_hash = documents_hash
            self.embeddings_cache_path = cache_path

            print(f"ğŸ’¾ å‘é‡ç¼“å­˜å·²ä¿å­˜: {cache_path}")

        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å‘é‡ç¼“å­˜å¤±è´¥: {e}")
            # ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ

    def _build_faiss_index(self) -> None:
        """æ„å»ºFAISSç´¢å¼•"""
        print("æ„å»ºFAISSç´¢å¼•...")
        self.index = faiss.IndexFlatIP(self.embedding_dim)  # å†…ç§¯ç´¢å¼•(ä½™å¼¦ç›¸ä¼¼åº¦)

        # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        self.index.add(self.document_embeddings)
    
    def retrieve(self, query: Query, top_k: int = 10) -> List[RetrievalResult]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.is_built:
            raise RuntimeError("ç´¢å¼•å°šæœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_indexæ–¹æ³•")
        
        if not self.model or not self.index:
            raise RuntimeError("æ¨¡å‹æˆ–ç´¢å¼•æœªå‡†å¤‡å°±ç»ª")
        
        # ç¼–ç æŸ¥è¯¢
        query_text = TextProcessor.normalize_text(query.text)
        query_embedding = self._encode_texts([query_text], show_progress=False)
        
        # æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£
        scores, indices = self.index.search(query_embedding, top_k)
        
        # ç”Ÿæˆæ£€ç´¢ç»“æœ
        results = []
        for rank, (score, doc_idx) in enumerate(zip(scores[0], indices[0])):
            if doc_idx >= 0 and score > 0:  # æœ‰æ•ˆçš„ç´¢å¼•å’Œæ­£åˆ†æ•°
                result = RetrievalResult(
                    doc_id=self.documents[doc_idx].doc_id,
                    score=float(score),
                    document=self.documents[doc_idx],
                    retriever_name=self.name
                )
                results.append(result)
        
        return results
    
    def save_index(self, index_path: str) -> None:
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        if not self.is_built:
            raise RuntimeError("ç´¢å¼•å°šæœªæ„å»º")
        
        index_dir = Path(index_path).parent
        index_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜FAISSç´¢å¼•
        faiss_path = index_dir / "faiss_index.bin"
        faiss.write_index(self.index, str(faiss_path))
        
        # ä¿å­˜å…¶ä»–æ•°æ®
        metadata = {
            'documents': self.documents,
            'document_embeddings': self.document_embeddings,
            'config': self.config,
            'model_name': self.model_name,
            'embedding_dim': self.embedding_dim,
            'faiss_path': str(faiss_path)
        }
        
        FileUtils.save_pickle(metadata, index_path)
        print(f"å‘é‡ç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")
    
    def load_index(self, index_path: str) -> None:
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•"""
        try:
            # åŠ è½½å…ƒæ•°æ®
            metadata = FileUtils.load_pickle(index_path)
            
            self.documents = metadata['documents']
            self.document_embeddings = metadata['document_embeddings']
            
            # æ›´æ–°é…ç½®
            if 'model_name' in metadata:
                if metadata['model_name'] != self.model_name:
                    print(f"è­¦å‘Š: ç´¢å¼•ä½¿ç”¨çš„æ¨¡å‹({metadata['model_name']})ä¸å½“å‰é…ç½®({self.model_name})ä¸åŒ")
            
            if 'embedding_dim' in metadata:
                self.embedding_dim = metadata['embedding_dim']
            
            # åŠ è½½FAISSç´¢å¼•
            faiss_path = metadata.get('faiss_path')
            if faiss_path and Path(faiss_path).exists():
                self.index = faiss.read_index(faiss_path)
            else:
                # å¦‚æœFAISSæ–‡ä»¶ä¸å­˜åœ¨ï¼Œé‡æ–°æ„å»ºç´¢å¼•
                print("FAISSç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œé‡æ–°æ„å»º...")
                self.index = faiss.IndexFlatIP(self.embedding_dim)
                self.index.add(self.document_embeddings)
            
            self.is_built = True
            print(f"å‘é‡ç´¢å¼•å·²ä» {index_path} åŠ è½½å®Œæˆ")
            
        except Exception as e:
            raise RuntimeError(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
    
    def get_document_embedding(self, doc_id: str) -> Optional[np.ndarray]:
        """è·å–æŒ‡å®šæ–‡æ¡£çš„å‘é‡è¡¨ç¤º"""
        if not self.is_built:
            return None
        
        for i, doc in enumerate(self.documents):
            if doc.doc_id == doc_id:
                return self.document_embeddings[i]
        
        return None
    
    def find_similar_documents(self, doc_id: str, top_k: int = 10) -> List[RetrievalResult]:
        """æŸ¥æ‰¾ä¸æŒ‡å®šæ–‡æ¡£ç›¸ä¼¼çš„å…¶ä»–æ–‡æ¡£"""
        embedding = self.get_document_embedding(doc_id)
        if embedding is None:
            return []
        
        # æœç´¢ç›¸ä¼¼æ–‡æ¡£
        scores, indices = self.index.search(embedding.reshape(1, -1), top_k + 1)
        
        # ç”Ÿæˆç»“æœï¼ˆæ’é™¤è‡ªèº«ï¼‰
        results = []
        for score, doc_idx in zip(scores[0], indices[0]):
            if doc_idx >= 0 and self.documents[doc_idx].doc_id != doc_id:
                result = RetrievalResult(
                    doc_id=self.documents[doc_idx].doc_id,
                    score=float(score),
                    document=self.documents[doc_idx],
                    retriever_name=self.name
                )
                results.append(result)
                
                if len(results) >= top_k:
                    break
        
        return results
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        if not self.is_built:
            return {}
        
        return {
            'document_count': len(self.documents),
            'embedding_dim': self.embedding_dim,
            'model_name': self.model_name,
            'index_size': self.index.ntotal if self.index else 0,
            'memory_usage_mb': (
                self.document_embeddings.nbytes / (1024 * 1024) 
                if self.document_embeddings is not None else 0
            )
        }


def create_dense_retriever(config: Dict[str, Any]) -> DenseRetriever:
    """åˆ›å»ºå‘é‡æ£€ç´¢å™¨çš„å·¥å‚å‡½æ•°"""
    return DenseRetriever(config=config)