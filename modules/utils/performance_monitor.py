#!/usr/bin/env python
"""
FusionRAG æ€§èƒ½ç›‘æ§æ¡†æ¶
å®æ—¶ç›‘æ§å’Œè®°å½•ç³»ç»Ÿæ€§èƒ½ï¼Œä¸ºå‚æ•°ä¼˜åŒ–æä¾›åé¦ˆ
"""

import time
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import threading
import psutil
import os

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    timestamp: str
    dataset_name: str
    retriever_name: str
    query_id: str
    
    # æ—¶é—´æŒ‡æ ‡
    response_time: float  # å“åº”æ—¶é—´(ms)
    index_build_time: float  # ç´¢å¼•æ„å»ºæ—¶é—´(s)
    
    # è´¨é‡æŒ‡æ ‡
    recall_at_5: float
    recall_at_10: float
    ndcg_at_10: float
    map_score: float
    
    # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
    memory_usage: float  # MB
    cpu_usage: float     # %
    
    # æ£€ç´¢å™¨ç‰¹å®šæŒ‡æ ‡
    results_count: int
    avg_score: float
    score_variance: float
    
    # é…ç½®å‚æ•°
    parameters: Dict[str, Any]

@dataclass
class SystemStatus:
    """ç³»ç»ŸçŠ¶æ€"""
    timestamp: str
    total_queries_processed: int
    avg_response_time: float
    current_memory_usage: float
    current_cpu_usage: float
    active_retrievers: List[str]
    error_count: int
    warning_count: int

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, log_dir: str = "logs/performance", buffer_size: int = 1000):
        self.log_dir = log_dir
        self.buffer_size = buffer_size
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(log_dir, exist_ok=True)
        
        # æ€§èƒ½æ•°æ®ç¼“å†²åŒº
        self.metrics_buffer = deque(maxlen=buffer_size)
        self.system_status_buffer = deque(maxlen=100)
        
        # ç»Ÿè®¡æ•°æ®
        self.dataset_stats = defaultdict(list)
        self.retriever_stats = defaultdict(list)
        self.performance_history = defaultdict(list)
        
        # çº¿ç¨‹å®‰å…¨é”
        self.lock = threading.Lock()
        
        # æ—¥å¿—é…ç½®
        self._setup_logging()
        
        # ç³»ç»Ÿèµ„æºç›‘æ§
        self.start_time = time.time()
        self.query_count = 0
        self.error_count = 0
        self.warning_count = 0
        
        print("ğŸ“Š æ€§èƒ½ç›‘æ§æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_file = os.path.join(self.log_dir, f"performance_{datetime.now().strftime('%Y%m%d')}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
    
    def start_query_monitoring(self, query_id: str, dataset_name: str, retriever_name: str) -> str:
        """å¼€å§‹æŸ¥è¯¢ç›‘æ§"""
        monitor_id = f"{dataset_name}_{retriever_name}_{query_id}_{int(time.time())}"
        start_time = time.time()
        
        with self.lock:
            self.query_count += 1
        
        self.logger.info(f"ğŸ” å¼€å§‹ç›‘æ§æŸ¥è¯¢: {monitor_id}")
        return monitor_id
    
    def end_query_monitoring(self, monitor_id: str, results: List, parameters: Dict[str, Any] = None) -> PerformanceMetrics:
        """ç»“æŸæŸ¥è¯¢ç›‘æ§å¹¶è®°å½•æŒ‡æ ‡"""
        parts = monitor_id.split('_')
        
        # æœ€åä¸€ä¸ªéƒ¨åˆ†æ˜¯æ—¶é—´æˆ³
        start_timestamp = int(parts[-1])
        
        # å€’æ•°ç¬¬äºŒä¸ªéƒ¨åˆ†æ˜¯query_id
        query_id = parts[-2]
        
        # ç¬¬ä¸€ä¸ªéƒ¨åˆ†æ˜¯dataset_name
        dataset_name = parts[0]
        
        # ä¸­é—´éƒ¨åˆ†ç»„æˆretriever_name
        retriever_name = '_'.join(parts[1:-2])
        
        end_time = time.time()
        response_time = (end_time - start_timestamp) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # è®¡ç®—æ£€ç´¢è´¨é‡æŒ‡æ ‡
        results_count = len(results)
        avg_score = sum(r.score for r in results) / results_count if results else 0
        score_variance = sum((r.score - avg_score) ** 2 for r in results) / results_count if results else 0
        
        # è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
        memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        cpu_usage = psutil.Process().cpu_percent()
        
        # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡å¯¹è±¡
        metrics = PerformanceMetrics(
            timestamp=datetime.now().isoformat(),
            dataset_name=dataset_name,
            retriever_name=retriever_name,
            query_id=query_id,
            response_time=response_time,
            index_build_time=0.0,  # éœ€è¦å¤–éƒ¨è®¾ç½®
            recall_at_5=0.0,       # éœ€è¦å¤–éƒ¨è®¡ç®—
            recall_at_10=0.0,      # éœ€è¦å¤–éƒ¨è®¡ç®—
            ndcg_at_10=0.0,        # éœ€è¦å¤–éƒ¨è®¡ç®—
            map_score=0.0,         # éœ€è¦å¤–éƒ¨è®¡ç®—
            memory_usage=memory_usage,
            cpu_usage=cpu_usage,
            results_count=results_count,
            avg_score=avg_score,
            score_variance=score_variance,
            parameters=parameters or {}
        )
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        with self.lock:
            self.metrics_buffer.append(metrics)
            self.dataset_stats[dataset_name].append(metrics)
            self.retriever_stats[retriever_name].append(metrics)
        
        self.logger.info(f"âœ… æŸ¥è¯¢ç›‘æ§å®Œæˆ: {monitor_id}, å“åº”æ—¶é—´: {response_time:.2f}ms")
        return metrics
    
    def update_evaluation_metrics(self, monitor_id: str, recall_5: float, recall_10: float, 
                                  ndcg_10: float, map_score: float):
        """æ›´æ–°è¯„ä¼°æŒ‡æ ‡"""
        # æŸ¥æ‰¾å¯¹åº”çš„æ€§èƒ½æŒ‡æ ‡
        with self.lock:
            for metrics in reversed(self.metrics_buffer):
                expected_id = f"{metrics.dataset_name}_{metrics.retriever_name}_{metrics.query_id}"
                if monitor_id.startswith(expected_id):
                    metrics.recall_at_5 = recall_5
                    metrics.recall_at_10 = recall_10
                    metrics.ndcg_at_10 = ndcg_10
                    metrics.map_score = map_score
                    break
    
    def record_index_build_time(self, dataset_name: str, retriever_name: str, build_time: float):
        """è®°å½•ç´¢å¼•æ„å»ºæ—¶é—´"""
        self.logger.info(f"ğŸ”¨ ç´¢å¼•æ„å»ºå®Œæˆ: {retriever_name}@{dataset_name}, æ—¶é—´: {build_time:.2f}s")
        
        # æŸ¥æ‰¾æœ€è¿‘çš„ç›¸å…³æŒ‡æ ‡å¹¶æ›´æ–°
        with self.lock:
            for metrics in reversed(self.metrics_buffer):
                if (metrics.dataset_name == dataset_name and 
                    metrics.retriever_name == retriever_name and 
                    metrics.index_build_time == 0.0):
                    metrics.index_build_time = build_time
                    break
    
    def record_error(self, error_type: str, error_message: str, context: Dict[str, Any] = None):
        """è®°å½•é”™è¯¯"""
        with self.lock:
            self.error_count += 1
        
        self.logger.error(f"âŒ é”™è¯¯: {error_type} - {error_message}")
        if context:
            self.logger.error(f"   ä¸Šä¸‹æ–‡: {context}")
    
    def record_warning(self, warning_message: str, context: Dict[str, Any] = None):
        """è®°å½•è­¦å‘Š"""
        with self.lock:
            self.warning_count += 1
        
        self.logger.warning(f"âš ï¸ è­¦å‘Š: {warning_message}")
        if context:
            self.logger.warning(f"   ä¸Šä¸‹æ–‡: {context}")
    
    def get_current_system_status(self) -> SystemStatus:
        """è·å–å½“å‰ç³»ç»ŸçŠ¶æ€"""
        uptime = time.time() - self.start_time
        
        with self.lock:
            # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
            recent_metrics = list(self.metrics_buffer)[-50:]  # æœ€è¿‘50ä¸ªæŸ¥è¯¢
            avg_response_time = (sum(m.response_time for m in recent_metrics) / 
                               len(recent_metrics)) if recent_metrics else 0
            
            # è·å–æ´»è·ƒçš„æ£€ç´¢å™¨
            active_retrievers = list(set(m.retriever_name for m in recent_metrics))
        
        # å½“å‰ç³»ç»Ÿèµ„æº
        current_memory = psutil.Process().memory_info().rss / 1024 / 1024
        current_cpu = psutil.Process().cpu_percent()
        
        status = SystemStatus(
            timestamp=datetime.now().isoformat(),
            total_queries_processed=self.query_count,
            avg_response_time=avg_response_time,
            current_memory_usage=current_memory,
            current_cpu_usage=current_cpu,
            active_retrievers=active_retrievers,
            error_count=self.error_count,
            warning_count=self.warning_count
        )
        
        with self.lock:
            self.system_status_buffer.append(status)
        
        return status
    
    def get_performance_summary(self, dataset_name: str = None, retriever_name: str = None) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        with self.lock:
            metrics = list(self.metrics_buffer)
        
        # è¿‡æ»¤æ•°æ®
        if dataset_name:
            metrics = [m for m in metrics if m.dataset_name == dataset_name]
        if retriever_name:
            metrics = [m for m in metrics if m.retriever_name == retriever_name]
        
        if not metrics:
            return {}
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        response_times = [m.response_time for m in metrics]
        recall_5_scores = [m.recall_at_5 for m in metrics if m.recall_at_5 > 0]
        recall_10_scores = [m.recall_at_10 for m in metrics if m.recall_at_10 > 0]
        ndcg_scores = [m.ndcg_at_10 for m in metrics if m.ndcg_at_10 > 0]
        
        summary = {
            'total_queries': len(metrics),
            'avg_response_time': sum(response_times) / len(response_times),
            'min_response_time': min(response_times),
            'max_response_time': max(response_times),
            'avg_results_count': sum(m.results_count for m in metrics) / len(metrics),
        }
        
        if recall_5_scores:
            summary['avg_recall_at_5'] = sum(recall_5_scores) / len(recall_5_scores)
        if recall_10_scores:
            summary['avg_recall_at_10'] = sum(recall_10_scores) / len(recall_10_scores)
        if ndcg_scores:
            summary['avg_ndcg_at_10'] = sum(ndcg_scores) / len(ndcg_scores)
        
        return summary
    
    def export_metrics(self, filename: str = None) -> str:
        """å¯¼å‡ºæ€§èƒ½æŒ‡æ ‡åˆ°JSONæ–‡ä»¶"""
        if not filename:
            filename = f"{self.log_dir}/metrics_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with self.lock:
            export_data = {
                'export_timestamp': datetime.now().isoformat(),
                'total_queries': self.query_count,
                'system_uptime': time.time() - self.start_time,
                'metrics': [asdict(m) for m in self.metrics_buffer],
                'system_status_history': [asdict(s) for s in self.system_status_buffer],
                'dataset_summaries': {
                    dataset: self.get_performance_summary(dataset_name=dataset)
                    for dataset in self.dataset_stats.keys()
                },
                'retriever_summaries': {
                    retriever: self.get_performance_summary(retriever_name=retriever)
                    for retriever in self.retriever_stats.keys()
                }
            }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡å·²å¯¼å‡º: {filename}")
        return filename
    
    def generate_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        status = self.get_current_system_status()
        uptime_hours = (time.time() - self.start_time) / 3600
        
        report = f"""
# FusionRAG æ€§èƒ½ç›‘æ§æŠ¥å‘Š

## ğŸ“Š ç³»ç»Ÿæ¦‚è§ˆ
- **ç›‘æ§æ—¶é—´**: {status.timestamp}
- **ç³»ç»Ÿè¿è¡Œæ—¶é—´**: {uptime_hours:.2f} å°æ—¶
- **æ€»æŸ¥è¯¢æ•°**: {status.total_queries_processed}
- **å¹³å‡å“åº”æ—¶é—´**: {status.avg_response_time:.2f} ms
- **é”™è¯¯æ•°**: {status.error_count}
- **è­¦å‘Šæ•°**: {status.warning_count}

## ğŸ’» ç³»ç»Ÿèµ„æº
- **å½“å‰å†…å­˜ä½¿ç”¨**: {status.current_memory_usage:.1f} MB
- **å½“å‰CPUä½¿ç”¨**: {status.current_cpu_usage:.1f}%

## ğŸ” æ´»è·ƒæ£€ç´¢å™¨
{', '.join(status.active_retrievers)}

## ğŸ“ˆ æ•°æ®é›†æ€§èƒ½ç»Ÿè®¡
"""
        
        for dataset_name in self.dataset_stats.keys():
            summary = self.get_performance_summary(dataset_name=dataset_name)
            if summary:
                report += f"""
### {dataset_name}
- æŸ¥è¯¢æ•°: {summary['total_queries']}
- å¹³å‡å“åº”æ—¶é—´: {summary['avg_response_time']:.2f} ms
- å¹³å‡ç»“æœæ•°: {summary['avg_results_count']:.1f}
"""
                if 'avg_recall_at_10' in summary:
                    report += f"- å¹³å‡Recall@10: {summary['avg_recall_at_10']:.4f}\n"
        
        report += f"""
## ğŸ”§ æ£€ç´¢å™¨æ€§èƒ½å¯¹æ¯”
"""
        
        for retriever_name in self.retriever_stats.keys():
            summary = self.get_performance_summary(retriever_name=retriever_name)
            if summary:
                report += f"""
### {retriever_name}
- æŸ¥è¯¢æ•°: {summary['total_queries']}
- å¹³å‡å“åº”æ—¶é—´: {summary['avg_response_time']:.2f} ms
- å“åº”æ—¶é—´èŒƒå›´: {summary['min_response_time']:.2f} - {summary['max_response_time']:.2f} ms
"""
        
        return report
    
    def clear_metrics(self):
        """æ¸…ç©ºæ€§èƒ½æŒ‡æ ‡"""
        with self.lock:
            self.metrics_buffer.clear()
            self.system_status_buffer.clear()
            self.dataset_stats.clear()
            self.retriever_stats.clear()
            self.query_count = 0
            self.error_count = 0
            self.warning_count = 0
            self.start_time = time.time()
        
        self.logger.info("ğŸ§¹ æ€§èƒ½æŒ‡æ ‡å·²æ¸…ç©º")

# å…¨å±€ç›‘æ§å™¨å®ä¾‹
_global_monitor = None

def get_performance_monitor() -> PerformanceMonitor:
    """è·å–å…¨å±€æ€§èƒ½ç›‘æ§å™¨å®ä¾‹"""
    global _global_monitor
    if _global_monitor is None:
        _global_monitor = PerformanceMonitor()
    return _global_monitor

def test_performance_monitor():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§æ¡†æ¶"""
    print("ğŸ§ª æµ‹è¯•æ€§èƒ½ç›‘æ§æ¡†æ¶")
    print("=" * 50)
    
    # è·å–ç›‘æ§å™¨
    monitor = get_performance_monitor()
    
    # æ¨¡æ‹ŸæŸ¥è¯¢ç›‘æ§
    from modules.utils.interfaces import RetrievalResult, Document
    
    for i in range(5):
        # å¼€å§‹ç›‘æ§
        monitor_id = monitor.start_query_monitoring(f"test_query_{i}", "test_dataset", "test_retriever")
        
        # æ¨¡æ‹Ÿæ£€ç´¢è¿‡ç¨‹
        time.sleep(0.1)
        
        # æ¨¡æ‹Ÿç»“æœ
        results = [
            RetrievalResult(
                doc_id=f"doc_{j}",
                score=0.8 - j*0.1,
                document=Document(doc_id=f"doc_{j}", title="Test", text="Test"),
                retriever_name="test_retriever"
            )
            for j in range(3)
        ]
        
        # ç»“æŸç›‘æ§
        metrics = monitor.end_query_monitoring(monitor_id, results, {"param1": "value1"})
        
        # æ›´æ–°è¯„ä¼°æŒ‡æ ‡
        monitor.update_evaluation_metrics(monitor_id, 0.2, 0.3, 0.15, 0.1)
    
    # è®°å½•ç´¢å¼•æ„å»ºæ—¶é—´
    monitor.record_index_build_time("test_dataset", "test_retriever", 5.2)
    
    # è·å–ç³»ç»ŸçŠ¶æ€
    status = monitor.get_current_system_status()
    print(f"ç³»ç»ŸçŠ¶æ€: {status.total_queries_processed} æŸ¥è¯¢, {status.avg_response_time:.2f}ms å¹³å‡å“åº”")
    
    # è·å–æ€§èƒ½æ‘˜è¦
    summary = monitor.get_performance_summary()
    print(f"æ€§èƒ½æ‘˜è¦: {summary}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = monitor.generate_performance_report()
    print("æ€§èƒ½æŠ¥å‘Š:")
    print(report[:500] + "...")
    
    # å¯¼å‡ºæŒ‡æ ‡
    export_file = monitor.export_metrics()
    print(f"æŒ‡æ ‡å·²å¯¼å‡º: {export_file}")
    
    print("\nğŸ‰ æ€§èƒ½ç›‘æ§æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    test_performance_monitor()