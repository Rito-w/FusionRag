#!/usr/bin/env python
"""
FusionRAG è‡ªåŠ¨å‚æ•°è°ƒä¼˜æ¨¡å—
æ ¹æ®æ•°æ®é›†ç‰¹å¾è‡ªåŠ¨ä¼˜åŒ–æ£€ç´¢å™¨å‚æ•°ï¼Œæä¾›é€šç”¨çš„æ€§èƒ½æå‡
"""

import numpy as np
import json
from typing import Dict, List, Any, Tuple
from collections import Counter
import re
from dataclasses import dataclass

@dataclass
class DatasetProfile:
    """æ•°æ®é›†ç‰¹å¾æ¡£æ¡ˆ"""
    name: str
    doc_count: int
    query_count: int
    avg_doc_length: float
    avg_query_length: float
    doc_length_std: float
    query_length_std: float
    vocab_size: int
    domain: str
    language: str
    complexity_score: float

@dataclass
class OptimalParameters:
    """ä¼˜åŒ–å‚æ•°é›†åˆ"""
    bm25_k1: float
    bm25_b: float
    bm25_top_k: int
    dense_top_k: int
    dense_batch_size: int
    graph_entity_threshold: int
    graph_top_k: int
    fusion_weights: Dict[str, float]
    final_top_k: int

class AutoParameterTuner:
    """è‡ªåŠ¨å‚æ•°è°ƒä¼˜å™¨"""
    
    def __init__(self):
        # é¢„å®šä¹‰çš„å‚æ•°è§„åˆ™åº“
        self.parameter_rules = {
            'small_dataset': {  # <1000 docs
                'bm25_k1': (1.2, 1.5),
                'bm25_b': (0.75, 0.8),
                'bm25_top_k': (100, 200),
                'dense_top_k': (100, 200),
                'graph_entity_threshold': (2, 3)
            },
            'medium_dataset': {  # 1000-10000 docs
                'bm25_k1': (1.5, 1.8),
                'bm25_b': (0.75, 0.8),
                'bm25_top_k': (200, 500),
                'dense_top_k': (200, 500),
                'graph_entity_threshold': (3, 5)
            },
            'large_dataset': {  # >10000 docs
                'bm25_k1': (1.8, 2.0),
                'bm25_b': (0.8, 0.85),
                'bm25_top_k': (500, 1000),
                'dense_top_k': (500, 1000),
                'graph_entity_threshold': (5, 8)
            }
        }
        
        # æ–‡æ¡£é•¿åº¦é€‚åº”è§„åˆ™
        self.length_adaptation = {
            'short_docs': {'k1_multiplier': 0.8, 'b_multiplier': 0.9},  # <100 words
            'medium_docs': {'k1_multiplier': 1.0, 'b_multiplier': 1.0},  # 100-300 words
            'long_docs': {'k1_multiplier': 1.2, 'b_multiplier': 1.1}   # >300 words
        }
        
        # é¢†åŸŸç‰¹å®šè°ƒæ•´
        self.domain_adjustments = {
            'medical': {'bm25_weight': 0.4, 'dense_weight': 0.5, 'graph_weight': 0.1},
            'covid': {'bm25_weight': 0.35, 'dense_weight': 0.55, 'graph_weight': 0.1},
            'scientific': {'bm25_weight': 0.45, 'dense_weight': 0.45, 'graph_weight': 0.1},
            'qa': {'bm25_weight': 0.5, 'dense_weight': 0.4, 'graph_weight': 0.1},
            'general': {'bm25_weight': 0.4, 'dense_weight': 0.4, 'graph_weight': 0.2}
        }
        
        print("ğŸ”§ è‡ªåŠ¨å‚æ•°è°ƒä¼˜å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_dataset_profile(self, documents: List, queries: List, dataset_name: str = "unknown") -> DatasetProfile:
        """åˆ†ææ•°æ®é›†ç‰¹å¾"""
        print(f"ğŸ“Š åˆ†ææ•°æ®é›†ç‰¹å¾: {dataset_name}")
        
        # åŸºç¡€ç»Ÿè®¡
        doc_count = len(documents)
        query_count = len(queries)
        
        # æ–‡æ¡£é•¿åº¦åˆ†æ
        doc_lengths = [len(doc.text.split()) for doc in documents]
        avg_doc_length = np.mean(doc_lengths)
        doc_length_std = np.std(doc_lengths)
        
        # æŸ¥è¯¢é•¿åº¦åˆ†æ
        query_lengths = [len(q.text.split()) for q in queries]
        avg_query_length = np.mean(query_lengths)
        query_length_std = np.std(query_lengths)
        
        # è¯æ±‡è¡¨å¤§å°ä¼°ç®—
        all_text = ' '.join([doc.text for doc in documents[:1000]])  # é‡‡æ ·ä¼°ç®—
        vocab_size = len(set(all_text.lower().split()))
        
        # é¢†åŸŸæ¨æ–­
        domain = self._infer_domain(dataset_name, documents[:100])
        
        # è¯­è¨€æ£€æµ‹
        language = self._detect_language(documents[:10])
        
        # å¤æ‚åº¦è¯„åˆ†
        complexity_score = self._calculate_complexity(doc_lengths, query_lengths, vocab_size)
        
        profile = DatasetProfile(
            name=dataset_name,
            doc_count=doc_count,
            query_count=query_count,
            avg_doc_length=avg_doc_length,
            avg_query_length=avg_query_length,
            doc_length_std=doc_length_std,
            query_length_std=query_length_std,
            vocab_size=vocab_size,
            domain=domain,
            language=language,
            complexity_score=complexity_score
        )
        
        print(f"âœ… æ•°æ®é›†ç‰¹å¾åˆ†æå®Œæˆ:")
        print(f"   æ–‡æ¡£: {doc_count}, å¹³å‡é•¿åº¦: {avg_doc_length:.1f}")
        print(f"   æŸ¥è¯¢: {query_count}, å¹³å‡é•¿åº¦: {avg_query_length:.1f}")
        print(f"   é¢†åŸŸ: {domain}, è¯­è¨€: {language}")
        print(f"   å¤æ‚åº¦: {complexity_score:.2f}")
        
        return profile
    
    def _infer_domain(self, dataset_name: str, sample_docs: List) -> str:
        """æ¨æ–­æ•°æ®é›†é¢†åŸŸ"""
        dataset_name = dataset_name.lower()
        
        # åŸºäºæ•°æ®é›†åç§°
        if 'medical' in dataset_name or 'nfcorpus' in dataset_name:
            return 'medical'
        elif 'covid' in dataset_name or 'trec-covid' in dataset_name:
            return 'covid'
        elif 'question' in dataset_name or 'qa' in dataset_name:
            return 'qa'
        elif 'scientific' in dataset_name or 'arxiv' in dataset_name:
            return 'scientific'
        
        # åŸºäºå†…å®¹å…³é”®è¯
        all_text = ' '.join([doc.text[:200] for doc in sample_docs]).lower()
        
        medical_keywords = ['disease', 'patient', 'treatment', 'medical', 'clinical', 'health', 'drug']
        covid_keywords = ['covid', 'virus', 'pandemic', 'vaccine', 'infection', 'respiratory']
        scientific_keywords = ['research', 'study', 'analysis', 'method', 'experiment', 'data']
        
        medical_count = sum(1 for keyword in medical_keywords if keyword in all_text)
        covid_count = sum(1 for keyword in covid_keywords if keyword in all_text)
        scientific_count = sum(1 for keyword in scientific_keywords if keyword in all_text)
        
        if covid_count >= 2:
            return 'covid'
        elif medical_count >= 3:
            return 'medical'
        elif scientific_count >= 3:
            return 'scientific'
        else:
            return 'general'
    
    def _detect_language(self, sample_docs: List) -> str:
        """æ£€æµ‹ä¸»è¦è¯­è¨€"""
        sample_text = ' '.join([doc.text[:100] for doc in sample_docs])
        
        # ç®€å•çš„ä¸­è‹±æ–‡æ£€æµ‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', sample_text))
        english_chars = len(re.findall(r'[a-zA-Z]', sample_text))
        
        if chinese_chars > english_chars * 0.3:
            return 'mixed'
        elif english_chars > 0:
            return 'english'
        else:
            return 'unknown'
    
    def _calculate_complexity(self, doc_lengths: List[float], query_lengths: List[float], vocab_size: int) -> float:
        """è®¡ç®—æ•°æ®é›†å¤æ‚åº¦è¯„åˆ†"""
        # é•¿åº¦å˜å¼‚ç³»æ•°
        doc_cv = np.std(doc_lengths) / np.mean(doc_lengths) if np.mean(doc_lengths) > 0 else 0
        query_cv = np.std(query_lengths) / np.mean(query_lengths) if np.mean(query_lengths) > 0 else 0
        
        # è¯æ±‡ä¸°å¯Œåº¦
        vocab_richness = min(vocab_size / 10000, 1.0)  # å½’ä¸€åŒ–åˆ°0-1
        
        # ç»¼åˆå¤æ‚åº¦
        complexity = (doc_cv + query_cv + vocab_richness) / 3
        return min(complexity, 1.0)
    
    def optimize_parameters(self, profile: DatasetProfile) -> OptimalParameters:
        """åŸºäºæ•°æ®é›†ç‰¹å¾ä¼˜åŒ–å‚æ•°"""
        print(f"ğŸ¯ ä¸ºæ•°æ®é›† {profile.name} ä¼˜åŒ–å‚æ•°")
        
        # 1. ç¡®å®šæ•°æ®é›†è§„æ¨¡ç±»åˆ«
        if profile.doc_count < 1000:
            size_category = 'small_dataset'
        elif profile.doc_count < 10000:
            size_category = 'medium_dataset'
        else:
            size_category = 'large_dataset'
        
        # 2. ç¡®å®šæ–‡æ¡£é•¿åº¦ç±»åˆ«
        if profile.avg_doc_length < 100:
            length_category = 'short_docs'
        elif profile.avg_doc_length < 300:
            length_category = 'medium_docs'
        else:
            length_category = 'long_docs'
        
        # 3. è·å–åŸºç¡€å‚æ•°èŒƒå›´
        base_rules = self.parameter_rules[size_category]
        length_rules = self.length_adaptation[length_category]
        domain_rules = self.domain_adjustments.get(profile.domain, self.domain_adjustments['general'])
        
        # 4. è®¡ç®—ä¼˜åŒ–å‚æ•°
        # BM25å‚æ•°
        k1_range = base_rules['bm25_k1']
        k1 = np.mean(k1_range) * length_rules['k1_multiplier']
        k1 = np.clip(k1, 0.5, 3.0)
        
        b_range = base_rules['bm25_b']
        b = np.mean(b_range) * length_rules['b_multiplier']
        b = np.clip(b, 0.3, 1.0)
        
        # Top-kå‚æ•° (åŸºäºå¤æ‚åº¦è°ƒæ•´)
        complexity_factor = 1 + profile.complexity_score * 0.5
        
        bm25_top_k = int(np.mean(base_rules['bm25_top_k']) * complexity_factor)
        dense_top_k = int(np.mean(base_rules['dense_top_k']) * complexity_factor)
        
        # å›¾æ£€ç´¢å‚æ•°
        graph_threshold = int(np.mean(base_rules['graph_entity_threshold']))
        graph_top_k = min(dense_top_k // 2, 200)
        
        # èåˆæƒé‡
        fusion_weights = domain_rules.copy()
        
        # æœ€ç»ˆè¾“å‡ºæ•°é‡
        final_top_k = min(100, profile.doc_count // 10)
        final_top_k = max(final_top_k, 20)  # æœ€å°‘20ä¸ªç»“æœ
        
        # Denseæ‰¹å¤„ç†å¤§å°
        if profile.doc_count > 5000:
            batch_size = 64
        elif profile.doc_count > 1000:
            batch_size = 32
        else:
            batch_size = 16
        
        params = OptimalParameters(
            bm25_k1=k1,
            bm25_b=b,
            bm25_top_k=bm25_top_k,
            dense_top_k=dense_top_k,
            dense_batch_size=batch_size,
            graph_entity_threshold=graph_threshold,
            graph_top_k=graph_top_k,
            fusion_weights=fusion_weights,
            final_top_k=final_top_k
        )
        
        print(f"âœ… å‚æ•°ä¼˜åŒ–å®Œæˆ:")
        print(f"   BM25: k1={k1:.2f}, b={b:.2f}, top_k={bm25_top_k}")
        print(f"   Dense: top_k={dense_top_k}, batch_size={batch_size}")
        print(f"   Graph: threshold={graph_threshold}, top_k={graph_top_k}")
        print(f"   èåˆæƒé‡: {fusion_weights}")
        print(f"   æœ€ç»ˆç»“æœæ•°: {final_top_k}")
        
        return params
    
    def generate_config(self, params: OptimalParameters, profile: DatasetProfile) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–åçš„é…ç½®æ–‡ä»¶"""
        config = {
            'dataset_profile': {
                'name': profile.name,
                'doc_count': profile.doc_count,
                'avg_doc_length': profile.avg_doc_length,
                'domain': profile.domain,
                'complexity_score': profile.complexity_score
            },
            'retrievers': {
                'bm25': {
                    'enabled': True,
                    'k1': params.bm25_k1,
                    'b': params.bm25_b,
                    'top_k': params.bm25_top_k
                },
                'dense': {
                    'enabled': True,
                    'model_name': self._select_optimal_model(profile.domain),
                    'top_k': params.dense_top_k,
                    'batch_size': params.dense_batch_size,
                    'device': 'cpu'
                },
                'graph': {
                    'enabled': True,
                    'entity_threshold': params.graph_entity_threshold,
                    'max_walk_length': 2,
                    'top_k': params.graph_top_k
                }
            },
            'fusion': {
                'method': 'weighted',
                'weights': params.fusion_weights,
                'top_k': params.final_top_k
            },
            'evaluation': {
                'metrics': ['recall@5', 'recall@10', 'ndcg@10', 'map'],
                'eval_size': min(1000, profile.query_count)
            }
        }
        
        return config
    
    def _select_optimal_model(self, domain: str) -> str:
        """é€‰æ‹©æœ€ä½³Denseæ¨¡å‹"""
        model_mapping = {
            'medical': 'sentence-transformers/all-mpnet-base-v2',
            'covid': 'sentence-transformers/all-mpnet-base-v2',
            'scientific': 'sentence-transformers/allenai-specter',
            'qa': 'sentence-transformers/all-mpnet-base-v2',
            'general': 'sentence-transformers/all-mpnet-base-v2'
        }
        
        return model_mapping.get(domain, 'sentence-transformers/all-mpnet-base-v2')
    
    def save_optimized_config(self, config: Dict[str, Any], dataset_name: str) -> str:
        """ä¿å­˜ä¼˜åŒ–é…ç½®åˆ°æ–‡ä»¶"""
        filename = f"configs/auto_optimized_{dataset_name}.yaml"
        
        import yaml
        with open(filename, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"ğŸ’¾ ä¼˜åŒ–é…ç½®å·²ä¿å­˜: {filename}")
        return filename

def test_auto_tuner():
    """æµ‹è¯•è‡ªåŠ¨å‚æ•°è°ƒä¼˜å™¨"""
    print("ğŸ§ª æµ‹è¯•è‡ªåŠ¨å‚æ•°è°ƒä¼˜å™¨")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿæ•°æ®é›†ä¿¡æ¯
    from modules.utils.interfaces import Document, Query
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ–‡æ¡£
    documents = [
        Document(doc_id=f"doc_{i}", title=f"Title {i}", 
                text=f"Medical research about disease treatment and patient care. " * (10 + i % 20))
        for i in range(500)
    ]
    
    # åˆ›å»ºæ¨¡æ‹ŸæŸ¥è¯¢
    queries = [
        Query(query_id=f"q_{i}", text=f"treatment disease patient medical")
        for i in range(50)
    ]
    
    # åˆå§‹åŒ–è°ƒä¼˜å™¨
    tuner = AutoParameterTuner()
    
    # åˆ†ææ•°æ®é›†ç‰¹å¾
    profile = tuner.analyze_dataset_profile(documents, queries, "test_medical")
    
    # ä¼˜åŒ–å‚æ•°
    params = tuner.optimize_parameters(profile)
    
    # ç”Ÿæˆé…ç½®
    config = tuner.generate_config(params, profile)
    
    # ä¿å­˜é…ç½®
    tuner.save_optimized_config(config, "test_medical")
    
    print("\nğŸ‰ è‡ªåŠ¨å‚æ•°è°ƒä¼˜æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    test_auto_tuner()