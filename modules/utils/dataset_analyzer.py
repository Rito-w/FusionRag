#!/usr/bin/env python
"""
FusionRAG æ•°æ®é›†ç‰¹å¾åˆ†æå™¨
æ·±åº¦åˆ†ææ•°æ®é›†ç‰¹å¾ï¼Œä¸ºå‚æ•°ä¼˜åŒ–æä¾›ç§‘å­¦ä¾æ®
"""

import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Tuple
from collections import Counter, defaultdict
import re
from dataclasses import dataclass, asdict
import pandas as pd

@dataclass
class AdvancedDatasetFeatures:
    """é«˜çº§æ•°æ®é›†ç‰¹å¾"""
    # åŸºç¡€ç‰¹å¾
    doc_count: int
    query_count: int
    avg_doc_length: float
    avg_query_length: float
    
    # åˆ†å¸ƒç‰¹å¾
    doc_length_quartiles: Tuple[float, float, float, float]  # Q1, Q2, Q3, Q4
    query_length_distribution: Dict[str, int]  # short, medium, long
    
    # è¯æ±‡ç‰¹å¾
    vocab_size: int
    avg_word_frequency: float
    top_frequent_words: List[Tuple[str, int]]
    
    # è¯­è¨€ç‰¹å¾
    language_distribution: Dict[str, float]
    avg_sentence_length: float
    
    # å¤æ‚åº¦ç‰¹å¾
    lexical_diversity: float  # è¯æ±‡å¤šæ ·æ€§
    semantic_complexity: float  # è¯­ä¹‰å¤æ‚åº¦
    query_document_overlap: float  # æŸ¥è¯¢æ–‡æ¡£é‡å åº¦
    
    # é¢†åŸŸç‰¹å¾
    domain_keywords: Dict[str, int]
    entity_density: float  # å®ä½“å¯†åº¦
    
    # æ£€ç´¢éš¾åº¦è¯„ä¼°
    retrieval_difficulty: str  # easy, medium, hard
    recommended_approach: List[str]  # æ¨èçš„æ£€ç´¢ç­–ç•¥

class DatasetFeatureAnalyzer:
    """æ•°æ®é›†ç‰¹å¾åˆ†æå™¨"""
    
    def __init__(self):
        # é¢†åŸŸå…³é”®è¯å­—å…¸
        self.domain_keywords = {
            'medical': [
                'disease', 'patient', 'treatment', 'therapy', 'clinical', 'medical', 'health',
                'drug', 'medicine', 'hospital', 'doctor', 'diagnosis', 'symptom', 'cure',
                'infection', 'virus', 'bacteria', 'cancer', 'tumor', 'surgery', 'medication'
            ],
            'covid': [
                'covid', 'coronavirus', 'pandemic', 'vaccine', 'vaccination', 'lockdown',
                'quarantine', 'mask', 'social distancing', 'outbreak', 'epidemic', 'ppe',
                'ventilator', 'icu', 'respiratory', 'pneumonia', 'antibody', 'immunity'
            ],
            'scientific': [
                'research', 'study', 'analysis', 'method', 'experiment', 'data', 'result',
                'conclusion', 'hypothesis', 'theory', 'model', 'algorithm', 'evaluation',
                'performance', 'accuracy', 'precision', 'recall', 'validation', 'test'
            ],
            'technology': [
                'computer', 'software', 'algorithm', 'programming', 'code', 'system',
                'network', 'internet', 'database', 'ai', 'machine learning', 'deep learning',
                'neural network', 'automation', 'digital', 'technology', 'innovation'
            ]
        }
        
        # åœç”¨è¯åˆ—è¡¨
        self.stop_words = set([
            'the', 'is', 'are', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'an', 'a', 'this', 'that', 'be',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
            'should', 'may', 'might', 'can', 'all', 'any', 'some', 'no', 'not'
        ])
        
        print("ğŸ“Š æ•°æ®é›†ç‰¹å¾åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_comprehensive_features(self, documents: List, queries: List, dataset_name: str = "unknown") -> AdvancedDatasetFeatures:
        """å…¨é¢åˆ†ææ•°æ®é›†ç‰¹å¾"""
        print(f"ğŸ” å¼€å§‹å…¨é¢ç‰¹å¾åˆ†æ: {dataset_name}")
        
        # 1. åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        basic_features = self._analyze_basic_features(documents, queries)
        
        # 2. åˆ†å¸ƒç‰¹å¾
        distribution_features = self._analyze_distribution_features(documents, queries)
        
        # 3. è¯æ±‡ç‰¹å¾
        vocabulary_features = self._analyze_vocabulary_features(documents)
        
        # 4. è¯­è¨€ç‰¹å¾
        language_features = self._analyze_language_features(documents)
        
        # 5. å¤æ‚åº¦ç‰¹å¾
        complexity_features = self._analyze_complexity_features(documents, queries)
        
        # 6. é¢†åŸŸç‰¹å¾
        domain_features = self._analyze_domain_features(documents)
        
        # 7. æ£€ç´¢éš¾åº¦è¯„ä¼°
        difficulty_assessment = self._assess_retrieval_difficulty(
            basic_features, complexity_features, domain_features
        )
        
        # æ•´åˆæ‰€æœ‰ç‰¹å¾
        features = AdvancedDatasetFeatures(
            **basic_features,
            **distribution_features,
            **vocabulary_features,
            **language_features,
            **complexity_features,
            **domain_features,
            **difficulty_assessment
        )
        
        print(f"âœ… ç‰¹å¾åˆ†æå®Œæˆï¼Œæ£€ç´¢éš¾åº¦: {features.retrieval_difficulty}")
        return features
    
    def _analyze_basic_features(self, documents: List, queries: List) -> Dict[str, Any]:
        """åˆ†æåŸºç¡€ç‰¹å¾"""
        doc_lengths = [len(doc.text.split()) for doc in documents]
        query_lengths = [len(q.text.split()) for q in queries]
        
        return {
            'doc_count': len(documents),
            'query_count': len(queries),
            'avg_doc_length': float(np.mean(doc_lengths)),
            'avg_query_length': float(np.mean(query_lengths))
        }
    
    def _analyze_distribution_features(self, documents: List, queries: List) -> Dict[str, Any]:
        """åˆ†æåˆ†å¸ƒç‰¹å¾"""
        doc_lengths = [len(doc.text.split()) for doc in documents]
        query_lengths = [len(q.text.split()) for q in queries]
        
        # æ–‡æ¡£é•¿åº¦å››åˆ†ä½æ•°
        quartiles = np.percentile(doc_lengths, [25, 50, 75, 100])
        
        # æŸ¥è¯¢é•¿åº¦åˆ†å¸ƒ
        query_dist = {'short': 0, 'medium': 0, 'long': 0}
        for length in query_lengths:
            if length <= 3:
                query_dist['short'] += 1
            elif length <= 8:
                query_dist['medium'] += 1
            else:
                query_dist['long'] += 1
        
        return {
            'doc_length_quartiles': tuple(quartiles),
            'query_length_distribution': query_dist
        }
    
    def _analyze_vocabulary_features(self, documents: List) -> Dict[str, Any]:
        """åˆ†æè¯æ±‡ç‰¹å¾"""
        # æ”¶é›†æ‰€æœ‰è¯æ±‡
        all_words = []
        for doc in documents[:1000]:  # é‡‡æ ·åˆ†æ
            words = [word.lower() for word in doc.text.split() 
                    if word.isalpha() and word.lower() not in self.stop_words]
            all_words.extend(words)
        
        # è¯é¢‘ç»Ÿè®¡
        word_freq = Counter(all_words)
        vocab_size = len(word_freq)
        avg_word_frequency = np.mean(list(word_freq.values()))
        top_words = word_freq.most_common(20)
        
        return {
            'vocab_size': vocab_size,
            'avg_word_frequency': float(avg_word_frequency),
            'top_frequent_words': top_words
        }
    
    def _analyze_language_features(self, documents: List) -> Dict[str, Any]:
        """åˆ†æè¯­è¨€ç‰¹å¾"""
        sample_docs = documents[:100]
        
        # è¯­è¨€åˆ†å¸ƒæ£€æµ‹
        chinese_count = 0
        english_count = 0
        
        for doc in sample_docs:
            text = doc.text[:500]  # é‡‡æ ·
            chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', text))
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            
            if chinese_chars > english_chars * 0.3:
                chinese_count += 1
            if english_chars > 0:
                english_count += 1
        
        total_docs = len(sample_docs)
        language_dist = {
            'english': english_count / total_docs,
            'chinese': chinese_count / total_docs,
            'mixed': max(0, (chinese_count + english_count - total_docs) / total_docs)
        }
        
        # å¹³å‡å¥å­é•¿åº¦
        sentence_lengths = []
        for doc in sample_docs[:20]:
            sentences = re.split(r'[.!?]', doc.text)
            for sentence in sentences:
                if sentence.strip():
                    sentence_lengths.append(len(sentence.split()))
        
        avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0
        
        return {
            'language_distribution': language_dist,
            'avg_sentence_length': float(avg_sentence_length)
        }
    
    def _analyze_complexity_features(self, documents: List, queries: List) -> Dict[str, Any]:
        """åˆ†æå¤æ‚åº¦ç‰¹å¾"""
        # è¯æ±‡å¤šæ ·æ€§ (Type-Token Ratio)
        sample_text = ' '.join([doc.text for doc in documents[:100]])
        words = [word.lower() for word in sample_text.split() if word.isalpha()]
        unique_words = set(words)
        lexical_diversity = len(unique_words) / len(words) if words else 0
        
        # è¯­ä¹‰å¤æ‚åº¦ (åŸºäºè¯æ±‡å’Œå¥å­ç»“æ„)
        avg_word_length = np.mean([len(word) for word in words]) if words else 0
        semantic_complexity = min(avg_word_length / 10 + lexical_diversity, 1.0)
        
        # æŸ¥è¯¢æ–‡æ¡£é‡å åº¦
        query_words = set()
        for query in queries[:50]:
            query_words.update([word.lower() for word in query.text.split() if word.isalpha()])
        
        doc_words = set()
        for doc in documents[:100]:
            doc_words.update([word.lower() for word in doc.text.split() if word.isalpha()])
        
        overlap = len(query_words & doc_words) / len(query_words | doc_words) if query_words | doc_words else 0
        
        return {
            'lexical_diversity': float(lexical_diversity),
            'semantic_complexity': float(semantic_complexity),
            'query_document_overlap': float(overlap)
        }
    
    def _analyze_domain_features(self, documents: List) -> Dict[str, Any]:
        """åˆ†æé¢†åŸŸç‰¹å¾"""
        # é¢†åŸŸå…³é”®è¯ç»Ÿè®¡
        domain_scores = defaultdict(int)
        entity_count = 0
        
        sample_text = ' '.join([doc.text for doc in documents[:200]]).lower()
        
        for domain, keywords in self.domain_keywords.items():
            for keyword in keywords:
                domain_scores[domain] += sample_text.count(keyword)
        
        # å®ä½“å¯†åº¦ä¼°ç®— (ç®€å•çš„å¤§å†™è¯ç»Ÿè®¡)
        words = sample_text.split()
        capitalized_words = [word for word in words if word[0].isupper() and len(word) > 2]
        entity_density = len(capitalized_words) / len(words) if words else 0
        
        return {
            'domain_keywords': dict(domain_scores),
            'entity_density': float(entity_density)
        }
    
    def _assess_retrieval_difficulty(self, basic: Dict, complexity: Dict, domain: Dict) -> Dict[str, Any]:
        """è¯„ä¼°æ£€ç´¢éš¾åº¦"""
        # éš¾åº¦è¯„åˆ†å› å­
        difficulty_score = 0
        
        # æ–‡æ¡£æ•°é‡å› å­
        if basic['doc_count'] > 10000:
            difficulty_score += 0.3
        elif basic['doc_count'] > 1000:
            difficulty_score += 0.2
        
        # å¹³å‡æ–‡æ¡£é•¿åº¦å› å­
        if basic['avg_doc_length'] > 500:
            difficulty_score += 0.2
        elif basic['avg_doc_length'] > 200:
            difficulty_score += 0.1
        
        # è¯­ä¹‰å¤æ‚åº¦å› å­
        difficulty_score += complexity['semantic_complexity'] * 0.3
        
        # æŸ¥è¯¢æ–‡æ¡£é‡å åº¦å› å­ (é‡å åº¦ä½è¡¨ç¤ºéš¾åº¦é«˜)
        difficulty_score += (1 - complexity['query_document_overlap']) * 0.2
        
        # ç¡®å®šéš¾åº¦ç­‰çº§
        if difficulty_score < 0.3:
            difficulty = "easy"
            approaches = ["bm25_focused", "simple_dense"]
        elif difficulty_score < 0.6:
            difficulty = "medium"
            approaches = ["balanced_fusion", "query_expansion"]
        else:
            difficulty = "hard"
            approaches = ["dense_focused", "graph_enhanced", "reranking"]
        
        return {
            'retrieval_difficulty': difficulty,
            'recommended_approach': approaches
        }
    
    def generate_analysis_report(self, features: AdvancedDatasetFeatures, save_path: str = None) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
        report = f"""
# æ•°æ®é›†ç‰¹å¾åˆ†ææŠ¥å‘Š

## ğŸ“Š åŸºç¡€ç»Ÿè®¡
- **æ–‡æ¡£æ•°é‡**: {features.doc_count:,}
- **æŸ¥è¯¢æ•°é‡**: {features.query_count:,}
- **å¹³å‡æ–‡æ¡£é•¿åº¦**: {features.avg_doc_length:.1f} è¯
- **å¹³å‡æŸ¥è¯¢é•¿åº¦**: {features.avg_query_length:.1f} è¯

## ğŸ“ˆ åˆ†å¸ƒç‰¹å¾
- **æ–‡æ¡£é•¿åº¦å››åˆ†ä½æ•°**: {features.doc_length_quartiles}
- **æŸ¥è¯¢é•¿åº¦åˆ†å¸ƒ**: 
  - çŸ­æŸ¥è¯¢(â‰¤3è¯): {features.query_length_distribution['short']}
  - ä¸­ç­‰æŸ¥è¯¢(4-8è¯): {features.query_length_distribution['medium']}
  - é•¿æŸ¥è¯¢(>8è¯): {features.query_length_distribution['long']}

## ğŸ“š è¯æ±‡ç‰¹å¾
- **è¯æ±‡è¡¨å¤§å°**: {features.vocab_size:,}
- **å¹³å‡è¯é¢‘**: {features.avg_word_frequency:.2f}
- **é«˜é¢‘è¯**: {features.top_frequent_words[:5]}

## ğŸŒ è¯­è¨€ç‰¹å¾
- **è¯­è¨€åˆ†å¸ƒ**: {features.language_distribution}
- **å¹³å‡å¥å­é•¿åº¦**: {features.avg_sentence_length:.1f} è¯

## ğŸ§  å¤æ‚åº¦ç‰¹å¾
- **è¯æ±‡å¤šæ ·æ€§**: {features.lexical_diversity:.3f}
- **è¯­ä¹‰å¤æ‚åº¦**: {features.semantic_complexity:.3f}
- **æŸ¥è¯¢æ–‡æ¡£é‡å åº¦**: {features.query_document_overlap:.3f}

## ğŸ·ï¸ é¢†åŸŸç‰¹å¾
- **é¢†åŸŸå…³é”®è¯åˆ†å¸ƒ**: {features.domain_keywords}
- **å®ä½“å¯†åº¦**: {features.entity_density:.3f}

## ğŸ¯ æ£€ç´¢éš¾åº¦è¯„ä¼°
- **éš¾åº¦ç­‰çº§**: {features.retrieval_difficulty}
- **æ¨èç­–ç•¥**: {features.recommended_approach}

## ğŸ’¡ ä¼˜åŒ–å»ºè®®
"""
        
        # æ ¹æ®ç‰¹å¾æ·»åŠ å…·ä½“å»ºè®®
        if features.retrieval_difficulty == "easy":
            report += """
- ä½¿ç”¨æ ‡å‡†BM25å‚æ•°å³å¯è·å¾—è‰¯å¥½æ•ˆæœ
- Denseæ£€ç´¢å™¨å¯ä»¥ä½œä¸ºè¡¥å……
- æ— éœ€å¤æ‚çš„å›¾æ£€ç´¢æˆ–é‡æ’åº
"""
        elif features.retrieval_difficulty == "medium":
            report += """
- å¹³è¡¡ä½¿ç”¨BM25å’ŒDenseæ£€ç´¢å™¨
- è€ƒè™‘æŸ¥è¯¢æ‰©å±•æŠ€æœ¯
- å¯ä»¥å°è¯•è½»é‡çº§çš„å›¾æ£€ç´¢
"""
        else:
            report += """
- é‡ç‚¹ä½¿ç”¨Denseæ£€ç´¢å™¨å’Œè¯­ä¹‰åŒ¹é…
- å¼ºçƒˆæ¨èä½¿ç”¨å›¾æ£€ç´¢å™¨
- è€ƒè™‘æ·»åŠ é‡æ’åºæ¨¡å—
- å¯èƒ½éœ€è¦æŸ¥è¯¢é‡å†™å’Œæ‰©å±•
"""
        
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {save_path}")
        
        return report
    
    def visualize_features(self, features: AdvancedDatasetFeatures, save_path: str = None):
        """å¯è§†åŒ–æ•°æ®é›†ç‰¹å¾"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle(f'æ•°æ®é›†ç‰¹å¾å¯è§†åŒ–', fontsize=16)
        
        # 1. æŸ¥è¯¢é•¿åº¦åˆ†å¸ƒ
        query_dist = features.query_length_distribution
        axes[0,0].bar(query_dist.keys(), query_dist.values())
        axes[0,0].set_title('æŸ¥è¯¢é•¿åº¦åˆ†å¸ƒ')
        axes[0,0].set_ylabel('æ•°é‡')
        
        # 2. è¯­è¨€åˆ†å¸ƒ
        lang_dist = features.language_distribution
        axes[0,1].pie(lang_dist.values(), labels=lang_dist.keys(), autopct='%1.1f%%')
        axes[0,1].set_title('è¯­è¨€åˆ†å¸ƒ')
        
        # 3. é¢†åŸŸå…³é”®è¯
        domain_data = features.domain_keywords
        if domain_data:
            axes[0,2].bar(domain_data.keys(), domain_data.values())
            axes[0,2].set_title('é¢†åŸŸå…³é”®è¯åˆ†å¸ƒ')
            axes[0,2].tick_params(axis='x', rotation=45)
        
        # 4. å¤æ‚åº¦ç‰¹å¾
        complexity_metrics = [
            features.lexical_diversity,
            features.semantic_complexity,
            features.query_document_overlap
        ]
        complexity_labels = ['è¯æ±‡å¤šæ ·æ€§', 'è¯­ä¹‰å¤æ‚åº¦', 'æŸ¥è¯¢æ–‡æ¡£é‡å åº¦']
        axes[1,0].bar(complexity_labels, complexity_metrics)
        axes[1,0].set_title('å¤æ‚åº¦ç‰¹å¾')
        axes[1,0].tick_params(axis='x', rotation=45)
        
        # 5. é«˜é¢‘è¯
        if features.top_frequent_words:
            words, freqs = zip(*features.top_frequent_words[:10])
            axes[1,1].barh(words, freqs)
            axes[1,1].set_title('é«˜é¢‘è¯ (Top 10)')
        
        # 6. æ–‡æ¡£é•¿åº¦åˆ†å¸ƒ
        quartiles = features.doc_length_quartiles
        axes[1,2].boxplot([quartiles], labels=['æ–‡æ¡£é•¿åº¦'])
        axes[1,2].set_title('æ–‡æ¡£é•¿åº¦åˆ†å¸ƒ (å››åˆ†ä½æ•°)')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ç‰¹å¾å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        
        plt.show()

def test_feature_analyzer():
    """æµ‹è¯•ç‰¹å¾åˆ†æå™¨"""
    print("ğŸ§ª æµ‹è¯•æ•°æ®é›†ç‰¹å¾åˆ†æå™¨")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿæ•°æ®
    from modules.utils.interfaces import Document, Query
    
    documents = [
        Document(doc_id=f"doc_{i}", title=f"Medical Research {i}", 
                text=f"Clinical study on patient treatment and disease diagnosis. " * (5 + i % 15))
        for i in range(200)
    ]
    
    queries = [
        Query(query_id=f"q_{i}", text=f"treatment disease diagnosis patient clinical research")
        for i in range(30)
    ]
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = DatasetFeatureAnalyzer()
    
    # åˆ†æç‰¹å¾
    features = analyzer.analyze_comprehensive_features(documents, queries, "test_medical")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_analysis_report(features, "analysis/test_dataset_analysis.md")
    
    # å¯è§†åŒ–
    analyzer.visualize_features(features, "analysis/test_dataset_features.png")
    
    print("\nğŸ‰ ç‰¹å¾åˆ†ææµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    test_feature_analyzer()