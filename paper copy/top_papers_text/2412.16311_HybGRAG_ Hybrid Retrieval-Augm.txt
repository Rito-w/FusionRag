Title: HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases
arXiv ID: 2412.16311
Score: 0.9871
Total Pages: 15
Extraction Time: 2025-06-21 01:03:09
================================================================================


--- Page 1 ---
arXiv:2412.16311v2  [cs.LG]  2 Jun 2025HYBGRAG: Hybrid Retrieval-Augmented Generation on Textual and
Relational Knowledge Bases
Meng-Chieh Lee1,*, Qi Zhu2,†, Costas Mavromatis2, Zhen Han2, Soji Adeshina2,
Vassilis N. Ioannidis2,†, Huzefa Rangwala2, Christos Faloutsos1,2,†
1Carnegie Mellon University,2Amazon
{mengchil,christos}@cs.cmu.edu ,
{qzhuamzn,mavrok,zhenhz,adesojia,ivasilei,rhuzefa}@amazon.com
Abstract
Given a semi-structured knowledge base
(SKB), where text documents are intercon-
nected by relations, how can we effectively re-
trieve relevant information to answer user ques-
tions? Retrieval-Augmented Generation (RAG)
retrieves documents to assist large language
models (LLMs) in question answering; while
Graph RAG (GRAG) uses structured knowl-
edge bases as its knowledge source. However,
many questions require both textual and rela-
tional information from SKB — referred to as
“hybrid” questions — which complicates the
retrieval process and underscores the need for
a hybrid retrieval method that leverages both
information. In this paper, through our empiri-
cal analysis, we identify key insights that show
why existing methods may struggle with hybrid
question answering (HQA) over SKB. Based
on these insights, we propose HYBGRAG for
HQA, consisting of a retriever bank and a critic
module, with the following advantages: (1)
Agentic, it automatically refines the output by
incorporating feedback from the critic module,
(2) Adaptive, it solves hybrid questions requir-
ing both textual and relational information with
the retriever bank, (3) Interpretable, it justi-
fies decision making with intuitive refinement
path, and (4) Effective, it surpasses all base-
lines on HQA benchmarks. In experiments on
theSTARKbenchmark, HYBGRAG achieves
significant performance gains, with an average
relative improvement in Hit@ 1of51%.
1 Introduction
Retrieval-Augmented Generation (RAG) (Lewis
et al., 2020; Guu et al., 2020) enables large lan-
guage models (LLMs) to access the informa-
tion from an unstructured document database.
This allows LLMs to address unknown facts and
solve Open-Domain Question Answering (ODQA)
with additional textual information. Graph RAG
*The work is done while being an intern at Amazon.
†Corresponding authors.(GRAG) has extended this concept by retrieving in-
formation from structured knowledge bases, where
documents are interconnected by relationships. Ex-
isting GRAG methods focus on two directions:
extracting relational information from knowledge
graphs (KGs) and leveraging LLMs for Knowledge
Base Question Answering (KBQA) (Yasunaga
et al., 2021; Sun et al., 2024; Jin et al., 2024; Mavro-
matis and Karypis, 2024), and building relation-
ships between documents in the database to im-
prove ODQA performance (Li et al., 2024a; Dong
et al., 2024; Edge et al., 2024).
Recently, an emerging problem concentrates on
“hybrid ” question answering (HQA), where a ques-
tion requires both relational and textual information
to be answered correctly, given a semi-structured
knowledge base (SKB) (Wu et al., 2024b). SKB
consists of a structured knowledge base, i.e., knowl-
edge graph (KG), and unstructured text documents,
where the text documents are associated with en-
tities of KG. In Fig. 1 top, an example of hybrid
questions is given, which involves both the textual
aspect (paper topic) and the relational aspect (paper
author), and SKBs are the cylinders.
Nevertheless, through our empirical analysis, we
uncover two critical insights showing that exist-
ing methods that perform RAG or GRAG cannot
effectively tackle HQA, which requires a synergy
between the two retrieval methods. First, they fo-
cus solely on retrieving either textual or relational
information. As shown in Fig. 1(a) and (b), this lim-
itation reduces their applicability when the synergy
between the two modalities is required. Second, in
hybrid questions, the aspects required to retrieve
different types of information may not be easily
distinguishable. In Fig. 1(c), question routing (Li
et al., 2024b) is performed to identify the aspects of
the question. However, in an unsuccessful routing,
confusion between the textual aspect “nanofluid
heat transfer papers” and the relational aspect “by
John Smith”, leads to incorrect retrieval.

--- Page 2 ---
Hybrid Question:
What nanofluid heat transfer papers have been published by John Smith?(c) HybGRAG  (Ours)
(b) GRAG (a) RAG
Refine
RetrieveRetrieve RetrieveTextual Aspect:
What nanofluid heat transfer papers
have been published by John Smith.Relational Aspect:
nanofluid heat transfer papers,
by John SmithRelational Aspect:
nanofluid heat transfer papers,
by John Smith
Textual Aspect:
None
Relational Aspect:
by John Smith
Textual Aspect:
nanofluid heat transfer papers
Figure 1: HYBGRAG solves hybrid questions in SKB , which are semi-structured, involving textual and relational
aspects. (a) RAG overlooks the interconnections between documents and does not meet the requirements specified
by the relational aspect. (b) GRAG relies solely on the relational aspect and misidentifies the textual aspect as part
of the relational one. (c) HYBGRAG refines the question routing through self-reflection and successfully retrieves
the target document in SKB, addressing both textual and relational aspects.
Table 1: HYBGRAG matches all properties , while
baselines miss more than one property.
Property
Regular RAG
Think-on-Graph
AVATAR
HYBGRAG
1.Agentic ✔✔
2.Adaptive2.1. Questions in ODQA ✔ ✔ ✔
2.2. Questions in KBQA ✔ ✔
2.3. Questions in HQA ✔
3.Interpretable ?✔ ✔ ✔
21%
Higher10%
Higher
Figure 2: HYBGRAG wins inSTARK, outperforming
baselines by up to 21% in Hit@ 1.
To solve HQA in SKB, we propose HYBGRAG .
HYBGRAG handles hybrid questions with a re-
triever bank, which leverages both textual and re-
lational information simultaneously. To improve
the accuracy of the retrieval, HYBGRAG performs
self-reflection (Renze and Guven, 2024), which iter-
atively improves its question routing based on feed-
back from a carefully designed critic module. Sim-
ilarly to chain-of-thought (CoT) (Wei et al., 2022),
which is widely regarded as interpretable, HYB-
GRAG ’s refinement path provides intuitive expla-
nations for the performance improvement. Last but
not least, the framework of HYBGRAG is designed
to be flexible, and can easily be adapted to differ-
ent problems. We summarize the contributions of
HYBGRAG as follows:1.Agentic : it automatically refines the question
routing with self-reflection;
2.Adaptive : it solves textual, relational and hy-
brid questions with a unified framework;
3.Interpretable : it justifies the decision making
with intuitive refinement path; and
4.Effective : it outperforms all baselines on real-
world HQA benchmarks.
As shown in Table 1, HYBGRAG is the only work
that satisfies all the properties and solves HQA
in SKB. As shown in Fig. 2, when evaluated on
the HQA benchmark STARK,HYBGRAG outper-
forms the second-best baseline, achieving relative
improvements in Hit@ 1of47% onSTARK-MAG
and55% on ST ARK-P RIME , respectively.

--- Page 3 ---
2 Proposed Insights: Challenges in HQA
What new challenges in HQA over SKB remain
unsolved by existing methods? In this section, we
first define the problem HQA, and then conduct
experiments to uncover two critical insights, laying
the foundation for designing our method for HQA.
2.1 Problem Definition
A semi-structured knowledge base (SKB) consists
of a KG G= (E,R), where EandRrepresent
the sets of entities and relations. It also includes
a set of text documents D=S
i∈EDi, where Di
is the document associated with entity i. Entity
and relation types are denoted by TEandTR, re-
spectively. Each hybrid question qin SKB involves
semi-structured information, namely, textual and
relational information. We define hybrid question
answering (HQA) as follows:
•Given a SKB consisting of G= (E,R)and
D, and a hybrid question q.
•Retrieve a set of documents X ⊆ E , where
each document satisfies the requirements spec-
ified by the relational and textual aspects of q.
2.2 C1: Hybrid-Sourcing Question
To investigate if it is necessary to leverage both
textual and relational information to answer hybrid
questions, we conduct an experiment to show that
text documents and KG contain useful but non-
overlapping information. As a retriever that uses
only textual information, vector similarity search
(VSS) (Karpukhin et al., 2020) performs retrieval
and ranking by comparing the question and docu-
ments in the embedding space (“ada-002”); as a re-
triever that uses only relational information, Person-
alized PageRank (PPR) (Andersen et al., 2006) per-
forms random walks from the topic entities identi-
fied by an LLM (Claude 3 Sonnet) and ranks neigh-
boring entities based on their connectivity in KG.
In Table 2, the text and the graph retrievers
have competitive performance. Interestingly, if
an optimal routing always picks the retriever that
gives the correct result, the performance is signifi-
cantly higher, indicating little overlap between the
strengths of the text and graph retrievers. This high-
lights the importance of a solution to leverage both
textual and relational information simultaneously
by synergizing these two retrievers. In Fig. 1(c), we
show a hybrid question that requires both textual
and relational information to be answered. Based
on this observation, we uncover the first challenge:Table 2: Textual and relational information are both
useful to answer hybrid question in ST ARK-MAG.
Method Hit@ 1Hit@ 5
Text Retriever: VSS 0.2908 0.4961
Graph Retriever: PPR 0.2533 0.5523
Optimal Routing 0.4522 0.7463
Table 3: LLMs frequently extracts a subgraph from KG
in SKB without target entities in ST ARK-MAG.
# of Iterations Feedback Type Hit Rate
1 N/A 0.6769
2 Simple 0.7914
2 Corrective 0.9231
Challenge 1 (Hybrid-Sourcing Question) .In HQA,
there are questions that require both textual and
relational information to be answered.
2.3 C2: Refinement-Required Question
The success of KBQA often relies on the assump-
tion that the target entities are within an extracted
subgraph from KG (Lan et al., 2022). Similarly,
answering a question in HQA requires extracting
a subgraph from KG in SKB. As hybrid questions
involve both textual and relational aspects, they
can be challenging for an LLM to comprehend.
To study this, we test if an LLM can extract a sub-
graph from KG that contains the target entities (hit).
More specifically, an LLM (Claude 3 Sonnet) is
prompted to identify the relational aspect in the
question, i.e. topic entities and useful relations used
to extract the subgraph. An oracle is used to instruct
LLM to perform an extra iteration with feedback if
the target entities are not included in the subgraph.
In Table 3, if the result is incorrect, simply
prompting LLM to redo the extraction gives a much
better hit ratio. Moreover, if the LLM receives
feedback that points out the erroneous part of the
extraction (e.g., extracted topic entity is wrong), it
significantly improves the result. This is because in
hybrid questions that contain both textual and rela-
tional aspects, LLM can falsely identify the textual
aspect as the relational one. In Fig. 1 (c), there is an
error in retrieving the correct reference from LLM,
as it confuses the textual aspect as an entity of type
“field of study” on the first attempt. Based on this
observation, we uncover the second challenge:
Challenge 2 (Refinement-Required Question) .In
HQA, LLM struggles to distinguish between the
textual and relational aspects of the question on
the first attempt, necessitating further refinements.

--- Page 4 ---
3 Proposed Method: H YBGRAG
To solve HQA, we propose HYBGRAG , consist-
ing of the retriever bank and the critic module , to
address Challenge 1 and Challenge 2, respectively.
3.1 Retriever Bank (for C1)
To solve Challenge 1, we propose the retriever
bank, composed of multiple retrieval modules and
a router. Given a question q, the router determines
the selection and usage of the retrieval module, a
process known as question routing. The selected re-
trieval module then retrieves the top- Kreferences
X, as elaborated in the next paragraph.
Retrieval Modules We design two retrieval mod-
ules, namely text and hybrid retrieval modules, to
retrieve information from text documents and SKB,
respectively. Each retrieval module includes a re-
triever and a ranker, which offers the flexibility to
cover a wide range of questions.
The text retrieval module retrieves documents
using similarity search for a given question q, such
as dense retrieval, which is designed to directly
find answers within text documents. We use VSS
between question qand documents Din the embed-
ding space as both the retriever and the ranker. This
is typically used when nothing can be extracted
from the hybrid retrieval module.
The hybrid retrieval module takes the identified
topic entities ˆEand useful relations ˆRas input. It
uses a graph retriever to extract entities in the ego-
graph of ˆE, connected by ˆR. For example, in Fig. 1,
{ˆE={John Smith },ˆR={author writes paper }}
and the graph retriever extracts the entities/papers
connected by the path “John Smith -> author writes
paper -> {paper(s)}”. If more than one ego-graph
is extracted, their intersection is used as the final
result. Finally, to solve hybrid questions, we pro-
pose ranking the documents associated with the
extracted entities using a VSS ranker between ques-
tionqand documents D. This ensures the synergy
between the relational and textual information.
Router Given a question q, the LLM router per-
forms question routing to determine the selection
and usage of the retrieval module. More specifi-
cally, the router first identifies the relational aspect,
i.e., topic entities ˆEand useful relations ˆRbased on
the types of entities TEand the types of relation TR
using few shot examples (Brown, 2020). The router
then makes the selection st, determining whether to
use a text or a hybrid retrieval module. IdentifyingˆEandˆRbefore determining stimproves the qual-
ity of st. For example, if there is no entity extracted
ˆE=∅, a text retrieval module is a better option.
3.2 Critic Module (for C2)
Given a hybrid question q, the router is asked to per-
form question routing, including identifying topic
entities ˆEand useful relations ˆR. However, as
mentioned in Challenge 2, they may be incorrectly
identified in the first iteration.
To solve this, we propose the critic module,
which provides feedback to help the router perform
better question routing. Instead of using a single
LLM to complete this complicated task, we divide
the critic into two parts, an LLM validator Cvalto
validate the correctness of the retrieval X, and an
LLM commenter Ccomto provide feedback ftif
the retrieval is incorrect. This divide-and-conquer
step, similar to previous works (Gao et al., 2022;
Asai et al., 2024), is crucial to our critic module, of-
fering two key advantages: (1) By breaking a diffi-
cult task into two easier ones, we can now leverage
pre-trained LLMs to solve them while maintaining
good performance. This resolves the issue when
the labels are not available for fine-tuning an LLM
critic. (2) Since the tasks of CvalandCcomare
independent, they can each have their own exclu-
sive contexts, preventing the inclusion of irrelevant
information and avoiding the “lost in the middle”
phenomenon (Shi et al., 2023; Liu et al., 2024).
Validator The LLM validator Cvalaims to val-
idate if the top references retrieved Xmeet the
requirements specified by the question q, which is
a binary classification task. To improve accuracy,
we provide an additional validation context for the
validator. We use the reasoning paths between topic
entities and entities in the extracted ego-graph as
the validation context, which are used to verify
whether the output satisfies certain requirements in
the question. The reasoning paths are verbalized as
“{topic entity} →{useful relation} →...→{useful
relation} →{neighboring entity}”. For example,
if a hybrid question asks for a paper (i.e. a doc-
ument) from a specific author, then the context
including the reasoning paths “{author} →{writes}
→{paper}” is essential for verification.
Commenter The LLM commenter Ccomaims to
provide feedback fto help the router refine ques-
tion routing. To effectively guide the router, we
construct corrective feedback that it can easily un-
derstand. In more detail, it points out the error(s) in

--- Page 5 ---
Table 4: Corrective feedback of the critic module in H YBGRAG for ST ARK.
Error Source Error Type Feedback
IdentificationIncorrect Entity/Relation Entity/relation {name} is incorrect. Please remove or substitute this entity/relation.
Missing Entity There is only one entity but there may be more. Please extract one more entity and relation.
No Entity There is no entity extracted. Please extract at least one entity and one relation.
No IntersectionThere is no intersection between the entities. Please remove or substitute one entity and
relation.
Incorrect IntersectionThere is an intersection between the entities, but the answer is not within it. Please remove
or substitute one entity and relation.
Selection Incorrect Retrieval ModuleThe retrieved document is incorrect. The current retrieval module may not be helpful to
narrow down the search space.
each action, such as incorrect identification of topic
entities, as shown in Table 4. Unlike natural lan-
guage feedback, which may cause uncertainty or
inconsistency depending on the LLM used, our cor-
rective feedback provides clear guidance on how to
refine the question routing. Furthermore, it lever-
ages in-context learning (ICL) to provide sophis-
ticated feedback. We collect a small number of
successful experiences ( ≈30) in the training set as
examples, with each experience {st,ˆEt,ˆRt, ft+1}
comprising a pair of router action and feedback,
which is verified by ground truth. During inference,
the commenter gives high-quality feedback based
on multiple pre-collected examples.
3.3 Overall Algorithm
The algorithm of HYBGRAG is in Algo. 1. Given
a question q, in iteration t, the router determines
st,ˆEtandˆRtto retrieve the references Xtfrom
bothGandDin SKB, or only D, with the selected
retrieval module. The validator Cvalin the critic
module then decides whether to accept Xtas the
final answer or reject it. If Xtis rejected, the com-
menter Ccomgenerates feedback ft+1for the router
to assist in refining its action in iteration t+ 1.
4 Experiments
We conduct experiments to answer the following
research questions (RQ):
RQ1. Effectiveness: How well does HYBGRAG
perform in real-world benchmarks?
RQ2. Ablation Study: Are all the design choices
in H YBGRAG necessary?
RQ3. Interpretability: How does HYBGRAG re-
fine its question routing based on feedback?
Benchmarks We conduct experiments on two
QA benchmarks: STARK1(Wu et al., 2024b),
which serves as the primary evaluation benchmark
1Due to legal issue, only STARK-MAG andSTARK-
PRIME are used in this article.Algorithm 1: HYBGRAG
Input: Question q, a SKB with GandD,
Entity Types TE, Relation Types TR,
and Maximum Iterations T
1f1="";
2fort= 1, . . . , T do
3 /* Retriever Bank */
4 st,ˆEt,ˆRt=Router (q,TE,TR, ft);
5 ifstis hybrid retrieval module then
6 Xt=HybridRM (q, G,D,ˆEt,ˆRt);
7 else
8 Xt=TextRM (q,D);
9 /* Validator */
10 ifCval(q,Xt) =True then
11 Return Xt;
12 else
13 /* Commenter */
14 ft+1=Ccom(q, st,ˆEt,ˆRt);
15Return Xt;
and focuses on retrieval, and CRAG (Yang et al.,
2024), a complementary benchmark to evaluate
end-to-end RAG performance. While STARKfo-
cuses on HQA, CRAG encompasses both ODQA
and KBQA as sub-problems. Detailed benchmark
descriptions are provided in Appx. A.
4.1 Retrieval Evaluation on ST ARK
We use the default evaluation metrics provided by
STARK, which are Hit@ 1, Hit@ 5, Recall@ 20
and mean reciprocal rank (MRR), to evaluate the
performance of the retrieval task. We compare
HYBGRAG with various baselines, including re-
cent GRAG methods (QAGNN (Yasunaga et al.,
2021) and Think-on-Graph (Sun et al., 2024)); tra-
ditional RAG approaches; and self-reflective LLMs
(ReAct (Yao et al., 2023), Reflexion (Shinn et al.,
2023), and AVATAR(Wu et al., 2024a)). The de-
tails of the implementation are in Appx. C.

--- Page 6 ---
Table 5: Retrieval Evaluation on ST ARK: H YBGRAG wins. ‘*’ denotes that only 10% of the testing questions
are evaluated due to the high latency and cost of the methods. denotes our proposed method.
MethodSTARK-MAG STARK-P RIME
Hit@1 Hit@5 Recall@20 MRR Hit@1 Hit@5 Recall@20 MRR
QAGNN 0.1288 0.3901 0.4697 0.2912 0.0885 0.2123 0.2963 0.1473
Think-on-Graph* 0.1316 0.1617 0.1130 0.1418 0.0607 0.1571 0.1307 0.1017
Dense Retriever 0.1051 0.3523 0.4211 0.2134 0.0446 0.2185 0.3013 0.1238
VSS (Text Retrieval Module) 0.2908 0.4961 0.4836 0.3862 0.1263 0.3149 0.3600 0.2141
Multi-VSS 0.2592 0.5043 0.5080 0.3694 0.1510 0.3356 0.3805 0.2349
VSS w/ LLM Reranker* 0.3654 0.5317 0.4836 0.4415 0.1779 0.3690 0.3557 0.2627
ReAct 0.3107 0.4949 0.4703 0.3925 0.1528 0.3195 0.3363 0.2276
Reflexion 0.4071 0.5444 0.4955 0.4706 0.1428 0.3499 0.3852 0.2482
AVATAR 0.4436 0.5966 0.5063 0.5115 0.1844 0.3673 0.3931 0.2673
Hybrid Retrieval Module (Ours) 0.5028 0.5820 0.5031 0.5373 0.2492 0.3274 0.3366 0.2842
HYBGRAG (Ours) 0.6540 0.7531 0.6570 0.6980 0.2856 0.4138 0.4358 0.3449
Relative Improvement 47.4% 26.2% 29.3% 36.5% 54.9% 12.7% 10.9% 29.0%
Hit@1 Hit@5 Recall@20 MRR0.50.60.70.80.91.0ST aRK-MAG
Val. w/o Context
Com. w/ 5-Shot
HybGRAG
Oracle
Hit@1 Hit@5 Recall@20 MRR0.10.20.30.40.50.6ST aRK-Prime
Figure 3: Design choices in H YBGRAG are necessary inSTARK. We compare HYBGRAG with two variants: a
validator without validation context, and a commenter with only 5-shot. Oracle uses ground truth during inference.
1 2 3 4
# of Iterations0.500.550.600.65Hit@1
0.260.28
ST aRK-Prime
ST aRK-MAG
Figure 4: HYBGRAG improves its question routing
thanks to the critic module.
Table 6: HYBGRAG maintains strong performance
with a less powerful LLM model in ST ARK-MAG.
Base Model Hit@1 Hit@5 Recall@20 MRR Speedup
Claude 3 Haiku 0.6019 0.7084 0.6067 0.6483 1.96×
Claude 3 Sonnet 0.6540 0.7531 0.6570 0.6980 1.00×
4.1.1 Effectiveness (RQ1)
In Table 5, HYBGRAG outperforms all baselines
significantly in both datasets in STARK. Most base-
lines are designed to handle ODQA and KBQA,
and the results have shown that they cannot han-
dle HQA effectively (Challenge 1). Our hybrid
retrieval module is the second-best performing
method, highlighting the importance of designing
a synergized retrieval module that uses both textualTable 7: HYBGRAG performs best with multi-agent
design inSTARK-MAG. “Router for SR” baseline
performs self-reflection using a single LLM router.
Method Setting Hit@1 Hit@5 Recall@20 MRR
Hybrid RM No-Agent 0.5028 0.5820 0.5031 0.5373
Router for SR Single-Agent 0.6206 0.7069 0.6187 0.6587
HYBGRAG Multi-Agent 0.6540 0.7531 0.6570 0.6980
and relational information simultaneously. In ad-
dition, HYBGRAG performs significantly better
than the hybrid retrieval module, indicating that the
extracted entity and relation are frequently incor-
rect in the first iteration (Challenge 2). By tackling
Challenge 1 and 2 with our retriever bank and critic
module respectively, HYBGRAG has a significant
improvement in performance.
4.1.2 Ablation Study (RQ2)
Critic Module We compare HYBGRAG variants
with a validator without validation context, a com-
menter with only five shots, and those with oracles.
The oracle has access to the ground truth, which
gives the optimal judgement on the correctness of
the output and the error type of question routing, if
there is any. In Fig. 3, we show that HYBGRAG

--- Page 7 ---
Q: Any 2012 publications from Netaji Subhash Engineering 
College on optical TALU implementations in electronic circuits?
Action 1: 
Selection: Hybrid retrieval module
Entity: Netaji Subhash Engineering College (institution), optical 
TALU implementations in electronic circuits (field of study)
Relation: author affiliated with institution, author writes paper, 
paper has topic field of study
Feedback 1: The retrieved document is incorrect. 
Entity “optical TALU implementations in electronic circuits” and 
relation “paper has topic field of study” are incorrect. Please 
remove or substitute one entity and relation.
Action 2: 
Selection: Hybrid retrieval module
Entity: Netaji Subhash Engineering College (institution)
Relational: author affiliated with institution, author writes paper
Feedback 2: Accept. ✅Q: Are there any 2016 publications by co-authors of "A Low Abundance of 
135Cs in the Early Solar System from Barium Isotopic Signatures" which 
discuss the comparison of Earth's chemical composition with that of chondrites?
Action 1: 
Selection: Hybrid retrieval module
Entity: A Low Abundance of 135Cs in the Early Solar System from Barium 
Isotopic Signatures (paper)
Relation: author writes paper
Feedback 1: The retrieved document is incorrect. 
There is only one entity but there may be more. Please extract one more entity 
and relation.
Action 2: 
Selection: Hybrid retrieval module
Entity: A Low Abundance of 135Cs in the Early Solar System from Barium 
Isotopic Signatures (paper), chondrites (field of study)
Relational: author writes paper, paper has topic field of study
Feedback 2: Accept. ✅(a) Error Type: Incorrect Entity/Relation (b) Error Type: Missing EntityFigure 5: HYBGRAG is interpretable. In examples from STARK-MAG ,HYBGRAG successfully refines its
entity and relation extraction based on corrective feedback from the critic module.
Table 8: End-to-End RAG Evaluation on CRAG: H YBGRAG wins. All baselines (except CoT LLM) share our
retriever bank, but use different critics to provide feedback. denotes our proposed method.
MethodLlama 3.1 70B Claude 3 Sonnet
Accuracy ↑Halluc. ↓Missing Score a↑Accuracy ↑Halluc. ↓Missing Score a↑
CoT LLM 0.4607 0.5026 0.0367 -0.0419 0.3910 0.4052 0.2038 -0.0142
Text-Only RAG 0.4105 0.3685 0.2210 0.0420 0.5034 0.3955 0.1011 0.1079
Graph-Only RAG 0.4861 0.4442 0.0697 0.0419 0.5303 0.2974 0.1723 0.2329
Text & Graph RAG 0.4120 0.3790 0.2090 0.0330 0.5820 0.3416 0.0764 0.2404
ReAct 0.1745 0.2360 0.5895 -0.0615 0.4352 0.4075 0.1573 0.0277
Corrective RAG 0.4509 0.4652 0.0839 -0.0143 0.4674 0.3333 0.1993 0.1341
HYBGRAG (Ours) 0.5206 0.3588 0.1206 0.1618 0.6322 0.2959 0.0719 0.3363
performs the best with all our design choices, ap-
proaching the performance of an oracle.
Self-Reflection In Fig. 4, we demonstrate that with
more self-reflection iterations, the performance of
HYBGRAG improves further. Performance im-
proves significantly when increasing the number
of iterations from 1to2, where no self-reflection
is performed in iteration 1. It is also shown that
a few iterations are sufficient, as the improvement
diminishes over iterations.
Model Size Although we do not have access to
Claude 3 Opus, we conduct experiments with
Claude 3 Haiku, a more cost-efficient but less pow-
erful alternative to Claude 3 Sonnet2. In Table 6,
HYBGRAG maintains strong performance even
with Claude 3 Haiku. The results also follow the
scaling law of LLMs (Kaplan et al., 2020).
Multi-Agent Perspective Since HYBGRAG can
be interpreted as a multi-agent system, we add a
single-agent baseline, which relies on the router
to make decisions and provide feedback for self-
2https://www.anthropic.com/news/claude-3-familyreflection. In Table 7, HYBGRAG outperforms
both single-agent and no-agent baselines. This
highlights that self-reflection is essential for achiev-
ing strong performance in HQA, as pointed out
in Challenge 2. Moreover, unlike the plain text
feedback generated by the single-agent baseline,
the feedback generated by HYBGRAG more ef-
fectively guides the router in refining its decision,
thanks to our carefully designed critic module.
4.1.3 Interpretability (RQ3)
Fig. 5 illustrates examples of the interaction be-
tween the router in the retriever bank and the critic
module in STARK-MAG . In the first iteration of
Fig. 5(a), the router misidentifies a “optical TALU
implementations in electronic circuits” as a topic
entity representing the field of study (relational
aspect). Since the ego-graph extracted based on
this entity has no intersection with the ego-graph
extracted based on “Netaji Subhash Engineering
College”, the critic module recognizes that the for-
mer entity has a higher chance of being a textual
aspect. Thus, it gives the feedback to the router, and

--- Page 8 ---
Table 9: Number of API calls and tokens for ST ARK.
HYBGRAG API Call # Token # for Token # for Token # for
Component per for Examples Examples
Iteration Prompts in MAG in Prime
Router 2 159 2709 3018
Validator 1 39 1383 2107
Commenter 1 52 1215 1583
the router addresses it accordingly. This refinement
path of HYBGRAG is similar to CoT, making it in-
terpretable and easy for the user to understand. Ex-
amples in ST ARK-Prime are given in Appx. B.1.
4.2 End-to-End RAG Evaluation on CRAG
We modify HYBGRAG to adapt to CRAG (details
in Appx. C). We use default evaluation metrics
with an LLM evaluator to label answers as accurate
(1), incorrect/hallucination ( −1), or missing ( 0),
yielding Score a. We compare HYBGRAG with
CoT LLM, text-only RAG, graph-only RAG, and
RAG that concatenates text and graph references.
We include an agentic LLM (ReAct) and a self-
reflective LLM (Corrective RAG), both of which
share our retriever bank but use different critics.
In Table 8, HYBGRAG outperforms all base-
lines in CRAG . RAGs with a single retrieval mod-
ule cannot handle both types of questions. RAG
with a concatenated reference also distracts by ir-
relevant content in the long reference. Although
our retriever bank is provided, agentic and self-
reflective baselines still struggle to refine their ac-
tions. Since ReAct relies on the LLM’s ability to
think and provide natural language feedback, it
often lacks clear guidance. Without a fine-tuned
retrieval evaluator, Corrective RAG cannot effec-
tively identify the usefulness of a reference.
4.3 Model Cost Analysis
We report the number of API calls and token
consumption (excluding references) for each step
in an iteration of HYBGRAG in Table 9 and 14
forSTARKandCRAG , respectively. While most
of the token consumption arises from the examples
used for ICL, the prompts themselves require
very few tokens. Moreover, since HYBGRAG
uses the chat LLM as the router, the examples
for ICL only need to be given once. Compared to
the state-of-the-art baseline AVATARinSTARK,
which requires at least 500 API calls during
training, our hybrid retrieval module achieves a
relative improvement 24% in Hit@1 with only 2
API calls, while HYBGRAG achieves 51% with
at most 14API calls, both without training.5 Related Works
Graph RAG (GRAG) Various settings have been
explored for GRAG (Peng et al., 2024), and can
be roughly divided into three directions. The first
focuses on KBQA, taking advantage of the LLM
capability (Yasunaga et al., 2021; Sun et al., 2024;
Jin et al., 2024; Mavromatis and Karypis, 2024).
The second focuses on ODQA, building relation-
ships between documents to improve retrieval (Li
et al., 2024a; Dong et al., 2024; Edge et al., 2024).
The last assumes that a subgraph is given when
answering a question (He et al., 2024; Hu et al.,
2024). In contrast, this paper focuses on solving
HQA in SKB, and previous GRAG methods are
not easily generalized to HQA.
Agentic and Self-Reflective LLMs LLM agents
(Yao et al., 2023; Wu et al., 2024a) facilitate
planning in complex reasoning tasks. Among
them, AVATARis the most recent, proposing itera-
tive prompt optimization via contrastive reasoning.
However, they may still struggle to generate the
correct output on the first attempt. Self-reflection
addresses this limitation by iteratively optimizing
the output based on feedback, typically provided by
a critic implemented using various approaches: pre-
trained LLMs (Shinn et al., 2023; Madaan et al.,
2023), external tools (Gou et al., 2024; Qiao et al.,
2024), or fine-tuned LLMs (Paul et al., 2024; Asai
et al., 2024; Yan et al., 2024). Nevertheless, they
do not generalize to HQA for two reasons. First,
they lack appropriate retrieval tools and guidance
on how to refine retrieval effectively. Second, in the
absence of external tools or labels for fine-tuning,
using pre-trained LLMs as critics without careful
design results in suboptimal self-evaluation and
overly implicit feedback.
6 Conclusions
To solve hybrid question answering (HQA), we pro-
pose HYBGRAG , driven by insights from our em-
pirical analysis, which has following advantages:
1.Agentic : it refines question routing with self-
reflection by our critic module;
2.Adaptive : it solves textual, relational and hy-
brid questions by our retriever bank;
3.Interpretable : it justifies the decision making
with intuitive refinement path; and
4.Effective : it significantly outperforms all the
baselines on HQA benchmarks.
Applied on STARK,HYBGRAG achieves an aver-
age relative improvement 51% in Hit@ 1.

--- Page 9 ---
Limitations
While HYBGRAG is capable of outperforming ex-
isting RAG and GRAG methods on HQA, it still
has some limitations: (1) HYBGRAG uses only
the simplest retrieval modules, and various alter-
natives are not explored. For example, the ranker
in the retrieval modules could be replaced with a
cross-encoder ranker, and the retriever in the hy-
brid retrieval module could use the top- Kentities
from PPR instead. (2) HYBGRAG does not offer
significant advantages in terms of domain adapta-
tion. In experiments, although HYBGRAG outper-
forms baselines, its performance on STARK-Prime
is worse than in STARK-MAG, where the aca-
demic domain is generally considered less complex
than the medicinal domain. (3) The commentor in
HYBGRAG selects random experiences when per-
forming ICL. For example, selecting experiences
with questions most relevant to the current one may
yield better performance. Although these limita-
tions point out areas for potential improvement,
they also present future directions to further en-
hance the capabilities of H YBGRAG.
References
Reid Andersen, Fan Chung, and Kevin Lang. 2006. Lo-
cal graph partitioning using pagerank vectors. In
2006 47th Annual IEEE Symposium on Foundations
of Computer Science (FOCS’06) , pages 475–486.
IEEE.
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2024. Self-RAG: Learning to
retrieve, generate, and critique through self-reflection.
InThe Twelfth International Conference on Learning
Representations .
Tom B Brown. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165 .
Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu
Lian, and Zheng Liu. 2024. Bge m3-embedding:
Multi-lingual, multi-functionality, multi-granularity
text embeddings through self-knowledge distillation.
arXiv preprint arXiv:2402.03216 .
Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F Yang,
and Anton Tsitsulin. 2024. Don’t forget to connect!
improving rag with graph-based reranking. arXiv
preprint arXiv:2405.18414 .
Darren Edge, Ha Trinh, Newman Cheng, Joshua
Bradley, Alex Chao, Apurva Mody, Steven Truitt,
and Jonathan Larson. 2024. From local to global: A
graph rag approach to query-focused summarization.
arXiv preprint arXiv:2404.16130 .Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-
cent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,
et al. 2022. Rarr: Researching and revising what
language models say, using language models. arXiv
preprint arXiv:2210.08726 .
Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen,
Yujiu Yang, Nan Duan, and Weizhu Chen. 2024.
CRITIC: Large language models can self-correct
with tool-interactive critiquing. In The Twelfth Inter-
national Conference on Learning Representations .
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Mingwei Chang. 2020. Retrieval augmented
language model pre-training. In International confer-
ence on machine learning , pages 3929–3938. PMLR.
Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla,
Thomas Laurent, Yann LeCun, Xavier Bresson, and
Bryan Hooi. 2024. G-retriever: Retrieval-augmented
generation for textual graph understanding and ques-
tion answering. arXiv preprint arXiv:2402.07630 .
Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan,
Chen Ling, and Liang Zhao. 2024. Grag: Graph
retrieval-augmented generation. arXiv preprint
arXiv:2405.16506 .
Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar
Roy, Yu Zhang, Suhang Wang, Yu Meng, and Jiawei
Han. 2024. Graph chain-of-thought: Augmenting
large language models by reasoning on graphs. arXiv
preprint arXiv:2404.07103 .
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. arXiv
preprint arXiv:2001.08361 .
Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. arXiv preprint
arXiv:2004.04906 .
Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang,
Wayne Xin Zhao, and Ji-Rong Wen. 2022. Complex
knowledge base question answering: A survey. IEEE
Transactions on Knowledge and Data Engineering ,
35(11):11196–11215.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems , 33:9459–9474.
Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu,
Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yang-
guang Li, Wanli Ouyang, et al. 2024a. Graphreader:
Building graph-based agent to enhance long-context
abilities of large language models. arXiv preprint
arXiv:2406.14550 .

--- Page 10 ---
Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei,
and Michael Bendersky. 2024b. Retrieval augmented
generation or long-context llms? a comprehensive
study and hybrid approach. In Proceedings of the
2024 Conference on Empirical Methods in Natural
Language Processing: Industry Track , pages 881–
893.
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2024. Lost in the middle: How language mod-
els use long contexts. Transactions of the Association
for Computational Linguistics , 12:157–173.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
Shashank Gupta, Bodhisattwa Prasad Majumder,
Katherine Hermann, Sean Welleck, Amir Yazdan-
bakhsh, and Peter Clark. 2023. Self-refine: Itera-
tive refinement with self-feedback. In Thirty-seventh
Conference on Neural Information Processing Sys-
tems.
Costas Mavromatis and George Karypis. 2024. Gnn-
rag: Graph neural retrieval for large language model
reasoning. arXiv preprint arXiv:2405.20139 .
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-
riz Borges, Antoine Bosselut, Robert West, and Boi
Faltings. 2024. Refiner: Reasoning feedback on in-
termediate representations. In Proceedings of the
18th Conference of the European Chapter of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 1100–1126.
Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo,
Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang
Tang. 2024. Graph retrieval-augmented generation:
A survey. arXiv preprint arXiv:2408.08921 .
Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo,
Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei
Lv, and Huajun Chen. 2024. Autoact: Automatic
agent learning from scratch via self-planning. arXiv
preprint arXiv:2401.05268 .
Matthew Renze and Erhan Guven. 2024. Self-reflection
in llm agents: Effects on problem-solving perfor-
mance. arXiv preprint arXiv:2405.06682 .
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan
Scales, David Dohan, Ed H. Chi, Nathanael Schärli,
and Denny Zhou. 2023. Large language models can
be easily distracted by irrelevant context. In Proceed-
ings of the 40th International Conference on Machine
Learning , volume 202 of Proceedings of Machine
Learning Research , pages 31210–31227. PMLR.
Noah Shinn, Federico Cassano, Ashwin Gopinath,
Karthik R Narasimhan, and Shunyu Yao. 2023. Re-
flexion: language agents with verbal reinforcement
learning. In Thirty-seventh Conference on Neural
Information Processing Systems .Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo
Wang, Chen Lin, Yeyun Gong, Lionel Ni, Heung-
Yeung Shum, and Jian Guo. 2024. Think-on-graph:
Deep and responsible reasoning of large language
model on knowledge graph. In The Twelfth Interna-
tional Conference on Learning Representations .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in neural
information processing systems , 35:24824–24837.
Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang,
Michihiro Yasunaga, Kaidi Cao, Vassilis N Ioan-
nidis, Karthik Subbian, Jure Leskovec, and James
Zou. 2024a. Avatar: Optimizing llm agents for
tool-assisted knowledge retrieval. arXiv preprint
arXiv:2406.11200 .
Shirley Wu, Shiyu Zhao, Michihiro Yasunaga, Kexin
Huang, Kaidi Cao, Qian Huang, Vassilis N Ioanni-
dis, Karthik Subbian, James Zou, and Jure Leskovec.
2024b. Stark: Benchmarking llm retrieval on tex-
tual and relational knowledge bases. arXiv preprint
arXiv:2404.13207 .
Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling.
2024. Corrective retrieval augmented generation.
arXiv preprint arXiv:2401.15884 .
Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla,
Xiangsen Chen, Sajal Choudhary, Rongze Daniel
Gui, Ziran Will Jiang, Ziyu Jiang, et al. 2024.
Crag–comprehensive rag benchmark. arXiv preprint
arXiv:2406.04744 .
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R Narasimhan, and Yuan Cao. 2023.
React: Synergizing reasoning and acting in language
models. In The Eleventh International Conference
on Learning Representations .
Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut,
Percy Liang, and Jure Leskovec. 2021. Qa-gnn: Rea-
soning with language models and knowledge graphs
for question answering. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL) .

--- Page 11 ---
A Appendix: Benchmarks
A.1 ST ARK
We use two datasets from the STARKbenchmark,
STARK-MAG andSTARK-P RIME . Each dataset
contains a knowledge graph (KG) and unstructured
documents associated with some types of entities.
The task is to retrieve a set of documents from
the database that satisfy the requirements speci-
fied in the question. Noting that the majority of
questions are hybrid questions, and there are very
few textual questions. We use the testing set from
STARKfor evaluation, which contains 2665 and
2801 questions for STARK-MAG andSTARK-
PRIME , respectively. The KG of STARK-MAG is
an academic KG, and the one of STARK-P RIME
is a precision medicine KG. Their types of entity
and relations are provided in the benchmark.
A.2 CRAG
In the CRAG benchmark, there are KGs from 5
different domains that can be utilized to retrieve
useful reference. For each question, a database that
includes 50retrieved web pages and all 5KGs is
given, but the answer is not guaranteed to be on the
web pages, KGs, or both. The task is to generate
the answer to the question, with or without the help
of the retrieved reference. There are textual and re-
lational questions, covering various question types,
such as simple, simple with condition, comparison,
and multi-hop. We use the testing set from CRAG
for evaluation. There are 1335 textual and relation
questions, covering various question types, such as
simple, comparison, and multi-hop.
B Appendix: Experiments
B.1 Interpretability (RQ3) in ST ARK-Prime
Fig. 6 shows two examples that HYBGRAG re-
fines its question routing in STARK-Prime. In the
example of Fig. 6(a), HYBGRAG selects to use
the text retrieval module in the first iteration, and
the retrieved document is rejected by the valida-
tor.HYBGRAG then takes the feedback from the
commentor and turns to using the hybrid retrieval
module, and refines the extraction of topic entities
and useful relations in the next two iterations.
B.2 Ablation Study on Critic Module
We compare HYBGRAG variants with validators
without validator context, commentors with few or
zero shots, and those with oracles. The oracle hasaccess to the ground truth, which gives the optimal
judgement on the correctness of the output and the
error type of the action, if there is any. In Table 10
and 11, we show that HYBGRAG performs the
best with all our design choices, approaching the
performance of an oracle.
C Appendix: Reproducibility
C.1 Experimental Details
All the experiments are conducted on an AWS
EC2 P4 instance with NVIDIA A100 GPUs. Most
LLMs are implemented with Amazon Bedrock3,
and Llama 3.1 is implemented with Ollama4.
C.1.1 H YBGRAG Implementation
STARK The examples in the prompts are col-
lected from the training set provided by STARK.
We use the default entity and relation types pro-
vided by STARK. The radius of the extracted ego-
graph is no more than two. Four self-reflection
iterations have been done. When extracting the
entity name from the question, multiple entities
in the knowledge base may have exactly the same
name. In these cases, we select the entity that has
the answer in its one-hop neighborhood for disam-
biguation, since it is not the focus of our paper.
Moreover, these cases rarely happen, where only
3.83% and 0.07% of questions have this issue in
STARK-MAG and ST ARK-P RIME , respectively.
CRAG In the text retrieval module, the web
search based on the question is used as the retriever,
which is done by CRAG ahead of time. The VSS
ranker ranks the web pages based on their simi-
larity to the question in the embedding space. In
this module, we provide an additional choice for
the router. If the output generated based on the
current batch of retrieved web pages is rejected by
the validator, the router can choose to move on to
the next batch in the ranking list. In CRAG, since
there is no hybrid question, the hybrid retrieval
module is replaced by the graph retrieval module to
be prepared for relational questions. In the graph re-
trieval module, the retriever extracts the ego-graph
connected by the useful relations for each topic
entity. As there is no document associated with
entity, the retriever retrieves the reasoning paths
from topic entities to entities in the extracted ego-
graphs. Reasoning paths are verbalized as “{topic
3https://aws.amazon.com/bedrock/
4https://github.com/ollama/ollama

--- Page 12 ---
Q: Which autosomal dominant diseases are linked to the development 
of supernumerary breast tissue?
Action 1: 
Selection: Text retrieval module
Feedback 1: The retrieved document is incorrect. 
The current retrieval module may not be helpful to narrow down the 
search space.
Action 2: 
Selection: Hybrid retrieval module
Entity: autosomal dominant diseases (disease)
Relational: phenotype present
Feedback 2: The retrieved document is incorrect.
There is only one entity but there may be more. Please extract one 
more entity and relation.
Action 3: 
Selection: Hybrid retrieval module
Entity: autosomal dominant diseases (disease), supernumerary breast 
tissue (effect/phenotype)
Relational: phenotype present, associated with
Feedback 3: Accept. ✅(a) Error Type: Incorrect Module & Missing Entity
Q: Please find me genes or proteins linked to pediatric liver cancer, 
interacting with PAXIP1, highly expressed in lung adenocarcinoma, with 
a nuclear signal and similarity to GAGE/PAGE proteins.
Action 1: 
Selection: Hybrid retrieval module
Entity: PAXIP1 (gene/protein), pediatric liver cancer (disease), lung 
adenocarcinoma (disease)
Relational: ppi, associated with, expression present
Feedback 1: The retrieved document is incorrect. 
Entities “pediatric liver cancer” and “lung adenocarcinoma” are incorrect. 
Please remove or substitute these entities. 
Relations “associated with” and “expression present” are incorrect. 
Please remove or substitute these relations. 
Action 2: 
Selection: Hybrid retrieval module
Entity: PAXIP1 (gene/protein)
Relational: ppi
Feedback 2: Accept. ✅(b) Error Type: Incorrect Entity/RelationFigure 6: HYBGRAG is interpretable. In examples from STARK-P RIME ,HYBGRAG successfully refines its
entity and relation extraction based on corrective feedback from the critic module.
Table 10: The design choices in H YBGRAG are necessary in ST ARK. denotes the settings of HYBGRAG ,
and denotes the baseline that use ground truth during inference.
Validator CommentorSTARK-MAG STARK-P RIME
Hit@1 Hit@5 Recall@20 MRR Hit@1 Hit@5 Recall@20 MRR
w/o Context ICL 0.6105 0.7073 0.6245 0.6541 0.1946 0.2592 0.2685 0.2251
w/ Context 5-Shot 0.6465 0.7407 0.6458 0.6884 0.2406 0.3006 0.3038 0.2676
w/ Context ICL 0.6540 0.7531 0.6570 0.6980 0.2856 0.4138 0.4358 0.3449
Oracle Oracle 0.7193 0.7824 0.6840 0.7479 0.3606 0.4320 0.4358 0.3932
entity}→{useful relation} →...→{useful relation}
→{neighboring entity}”, and ranked by VSS.
The retrieved reference is used as the valida-
tion context to check if it is reliable to answer the
question. The validator takes the output of the gen-
erator and the validation context as the input. As
the prompts for the generator and the validator are
specialized for different tasks, this allows the val-
idator to offer meaningful validation. Although
the ground truth of the retrieval is not available in
CRAG , we construct corrective feedback based on
the router’s action and the evaluation, as shown in
Table 12. If the graph retrieval module is used and
the evaluation is incorrect, then either the retrieval
input (extracted entity and relation or the domain)
is incorrect, or selecting graph retrieval module is
incorrect; if the text retrieval module is used and
the evaluation is incorrect, then the information in
the current batch of documents is considered as not
useful to answer the question.
The examples in the prompts are collected fromthe validation set provided by CRAG . Since the
entity and relation types are not given by CRAG ,
and the KGs are only accessible with the provided
API, we collect them from the questions in the
validation set, as shown in Table 13. The radius of
the extracted ego-graph is no more than two. Four
self-reflection iterations have been done. A batch
contains five web pages.
C.1.2 Baseline Implementation
STARK We use “ada-002” as the embedding
model for dense retrieval and ranking, as used in
the paper. HYBGRAG uses Claude 3 Sonnet as
the base model, while ReAct, Reflexion, AVATAR,
and VSS with LLM reranker use Claude 3 Opus,
which is designed to be more powerful than Claude
3 Sonnet5. For QAGNN and Dense Retriever, be-
cause of the need of training, RoBERTa is used
as the base model. In experiments where the base
LLM is not specified, we default to using Claude 3
5https://www.anthropic.com/news/claude-3-family

--- Page 13 ---
Table 11: The design choices in H YBGRAG are necessary in CRAG. denotes the settings of HYBGRAG ,
and denotes the baseline that use ground truth during inference.
Validator Commentor Accuracy ↑Halluc. ↓Missing Score a↑
w/o Context ICL 0.5581 0.3461 0.0958 0.2120
w/ Context 0-Shot 0.6277 0.3004 0.0719 0.3273
w/ Context ICL 0.6322 0.2959 0.0719 0.3363
Oracle Oracle 0.7813 0.1640 0.0547 0.6173
Table 12: Design of critic module in H YBGRAG for CRAG.
Error Source Error Type Feedback
InputIncorrect Question TypeThe predicted question type is wrong. Please answer again. Which
type is this question?
Incorrect Question DynamismThe predicted dynamism of the question is wrong. Please answer
again. Which dynamism is this question?
Incorrect Question DomainThe predicted domain of the question is wrong. Please answer again.
Which domain is this question from?
Incorrect Entity and RelationThe topic entities and useful information extracted from the question
are incorrect. Please extract them again.
Selection Incorrect Retrieval ModuleThe reference does not contain useful information for solving the
question. Should we use knowledge graph as reference source based
on newly extracted entity and relation, or use the next batch of text
documents as reference source?
Sonnet. We implement Think-on-Graph with their
provided code6, using Claude 3 Sonnet as the base
model. As running the full experiment takes more
than a week, we evaluated it with only 10% of the
testing data, as is done for the LLM reranker in the
STARK paper.
CRAG We use Claude 3 Sonnet as the LLM
evaluator, and CoT prompting (Wei et al., 2022)
for all generator LLMs. We use “BAAI/bge-m3”
(Chen et al., 2024) as the embedding model for
dense retrieval and ranking. ReAct and Corrective
RAG share the same backbone with HYBGRAG ,
while having different critics. ReAct has three ac-
tions, “search web”, “search KG”, and “extract
entity relation domain”, and is given some exam-
ples. The process iterates among action, observa-
tion, and thought for four iterations as HYBGRAG .
While Corrective RAG requires a fine-tuned re-
trieval evaluator, we implement a version with only
a pre-trained LLM. It starts with the text retrieval
module and validates if the retrieved reference is
correct, ambiguous, or incorrect. If incorrect, it
uses the graph retrieval module instead. An final
answer is generated based on the reference with
CoT prompting.
6https://github.com/GasolSun36/ToGC.2 Prompts
STARK The prompt of the router for the first
decision making is:
You are a helpful, pattern-following assistant.
Given the following question, extract the informa-
tion from the question as requested. Rules: 1. The
Relational information must come from the given
relational types. 2. Each entity must exactly have
one category in the parentheses.
<<< {10 examples for entity and relation
extraction} >>>
Given the following question, based on the
entity type and the relation type, extract the topic
entities and useful relations from the question.
Entity Type: <<< {entity types} >>>
Relation Type: <<< {relation types} >>>
Question: <<< {question} >>>
Documents are required to answer the
given question, and the goal is to search the
useful documents. Each entity in the knowledge
graph is associated with a document. Based on
the extracted entities and relations, is knowledge
graph or text documents helpful to narrow down
the search space? You must answer with either of
them with no more than two words.
The prompt of the router for reflection is:

--- Page 14 ---
Table 13: Type of entity and relation in the CRAG benchmark.
Domain Type Content
FinanceEntity company_name, ticker_symbol, market_capitalization, earnings_per_share, price_to_earnings_ratio, datetime
Relationget_company_ticker, get_ticker_dividends, get_ticker_market_capitalization, get_ticker_earnings_per_share,
get_ticker_price_to_earnings_ratio, get_ticker_history_last_year_per_day,
get_ticker_history_last_week_per_minute, get_ticker_open_price, get_ticker_close_price, get_ticker_high_price,
get_ticker_low_price, get_ticker_volume, get_ticker_financial_information
SportsEntity nba_team_name, nba_player, soccer_team_name, datetime_day, datetime_month, datetime_year
Relationget_nba_game_on_date, get_soccer_previous_games_on_date, get_soccer_future_games_on_date,
get_nba_team_win_by_year
MusicEntity artist, lifespan, song, release_date, release_country, birth_place, birth_date, grammy_award_count, grammy_year
Relationgrammy_get_best_artist_by_year, grammy_get_award_count_by_artist, grammy_get_award_count_by_song,
grammy_get_best_song_by_year, grammy_get_award_date_by_artist, grammy_get_best_album_by_year,
get_artist_birth_place, get_artist_birth_date, get_members, get_lifespan, get_song_author,
get_song_release_country, get_song_release_date, get_artist_all_works
MovieEntity actor, movie, release_date, original_title, original_language, revenue, award_category
Relationact_movie, has_birthday, has_character, has_release_date, has_original_title, has_original_language, has_revenue,
has_crew, has_job, has_award_winner, has_award_category
EncyclopediaEntity encyclopedia_entity
Relation get_entity_information
The retrieved document is incorrect.
Feedback: <<< {feedback on extracted entity
and relation} >>>
Question: <<< {question} >>>
The retrieved document is incorrect. An-
swer again based on newly extracted topic entities
and useful relations. Is knowledge graph or text
documents helpful to narrow down the search
space? You must answer with either of them with
no more than two words.
The prompt of the validator is:
You are a helpful, pattern-following assistant.
<<< {examples for retrieval validation, 2 for each
type of entity} >>>
### Question: <<< {question} >>>
### Document: <<< {content of document and
reasoning paths} >>>
### Task: Is the document aligned with the
requirements of the question? Reply with only yes
or no.
The prompt of the commentor is:
You are a helpful, pattern-following assistant.
<<< {30 examples of action and feedback
pair}>>>
Question: <<< {question} >>>
Topic Entities: <<< {extracted entities} >>>
Useful Relations: <<< {extracted relations} >>>
Please point out the wrong entity or relation
extracted from the question, if there is any.
CRAG The prompt of the router for the first
decision making is:You are a helpful, pattern-following assistant.
Given the following question, extract the infor-
mation from the question as requested. Rules: 1.
Each entity must exactly have one category in the
parentheses. 2. Strictly follow the examples.
<<< {examples of entity and relation extraction,
5 for each domain} >>>
### Question Type: simple, simple_w_condition,
set, comparison, aggregation, multi_hop,
post_processing, false_premise.
### Question: <<< {question} >>>
### Task: Which type is this question? Answer
must be one of them.
### Dynamism: real-time, fast-changing,
slow-changing, static.
### Question: <<< {question} >>>
### Task: Which category of dynamism is this
question? Answer with one word and the answer
must be one of them.
### Domain: music, movie, finance, sports,
encyclopedia.
### Question: <<< {question} >>>
### Task: Which domain is this question from?
Answer with one word and the answer must be
one of them.
Given the following question, based on the
entity type and the relation type, extract the topic
entities and useful relations from the question.
Entity Type: <<< {entity types} >>>
Relation Type: <<< {relation types} >>>
Question: <<< {question} >>>
### Reference Source: knowledge graph,
text documents.
### Question: <<< {question} >>>
### Task: Based on the extracted entity, which
reference source is useful to answer the question?
You must pick one of them and answer with no
more than two words.

--- Page 15 ---
The prompt of the router for reflection is:
### Question Type: simple, simple_w_condition,
set, comparison, aggregation, multi_hop,
post_processing, false_premise.
### Question: <<< {question} >>>
### Task: The predicted question type is wrong.
Please answer again. Which type is this question?
Answer with one word and the answer must be
one of them.
### Dynamism: real-time, fast-changing,
slow-changing, static.
### Question: <<< {question} >>>
### Task: The predicted dynamism of the
question is wrong. Please answer again. Which
dynamism is this question? Answer with one
word and the answer must be one of them.
### Domain: music, movie, finance, sports,
encyclopedia.
### Question: <<< {question} >>>
### Task: The predicted domain of the question
is wrong. Please answer again. Which domain is
this question from? Answer with one word and
the answer must be one of them.
The topic entities and useful information
extracted from the question are incorrect. Please
extract them again. Given the following question,
based on the entity type and the relation type,
extract the topic entities and useful relations from
the question.
Entity Type: <<< {entity types} >>>
Relation Type: <<< {relation types} >>>
Question: <<< {question} >>>
### Reference Source: knowledge graph,
text documents.
### Question: <<< {question} >>>
### Task: The answer is incorrect. The reference
does not contain useful information for solving
the question. Please answer again, should we use
knowledge graph as reference source based on
newly extracted entity and relation, or use the
next batch of text documents as reference source?
You must pick one of them and answer with no
more than two words.
The prompt of the validator is:
### Reference: <<< {reference} >>>
### Prediction: <<< {output of generator} >>>
### Question: <<< {question} >>>
### Query Time: <<< {question time} >>>
### Task: The prediction is generated based on
the reference. Does the prediction answer the ques-
tion? Answer with one word, yes or no.
The prompt of the commentor is:You are a helpful, pattern-following assistant.
<<< {5 examples of action and feedback
pair}>>>
### Reference Source: <<< {source} >>>
### Question: <<< {question} >>>
### Query Time: <<< {question time} >>>
### Query Type: <<< {question type} >>>
### Query Dynamism: <<< {dynamism} >>>
### Query Domain: <<< {domain} >>>
### Task: Please point out the wrong information
about the question (Reference Source, Query
Type, Query Dynamism, Query Domain), if there
is any. The answer must be one of them.
The prompt of the generator is:
You are a helpful, pattern-following assistant.
<<< {1 chain-of-though prompt example} >>>
### Reference: <<< {reference} >>>
### Reference Source: <<< {source} >>>
### Question: <<< {question} >>>
### Query Time: <<< {question time} >>>
### Query Type: <<< {question type} >>>
### Query Dynamism: <<< {dynamism} >>>
### Query Domain: <<< {domain} >>>
### Task: You are given a Question, References
and the time when it was asked in the Pacific
Time Zone (PT), referred to as Query Time.
The query time is formatted as mm/dd/yyyy,
hh:mm:ss PT. The reference may help answer the
question. If the question contains a false premise
or assumption, answer “invalid question”. First,
list systematically and in detail all the problems in
this problem that need to be solved before we can
arrive at the correct answer. Then, solve each sub
problem using the answers of previous problems
and reach a final solution.
What is the final answer?
The prompt of the evaluator is:
### Question: <<< {question} >>>
### True Answer: <<< {ground truth
answer} >>>
### Predicted Answer: <<< {output of
generator} >>>
### Task: Based on the question and the
true answer, is the predicted answer accurate,
incorrect, or missing? The answer must be one of
them and is in one word.
Table 14: Number of API calls and tokens for CRAG.
HYBGRAG API Call # Token # for Token # for
Component / Iteration Prompts Examples
Router 4 266 5752
Validator 1 56 0
Commenter 1 78 598
Generator 2 168 553
