arXiv:2506.00054v1  [cs.IR]  28 May 2025This is a preprint under review at ACM TOIS. Do not redistribute the final version without permission.
Retrieval-Augmented Generation: A Comprehensive Survey of Architectures,
Enhancements, and Robustness Frontiers
CHAITANYA SHARMA, Independent Researcher, United States
Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to enhance large language models (LLMs) by conditioning
generation on external evidence retrieved at inference time. While RAG addresses critical limitations of parametric knowledge
storage—such as factual inconsistency and domain inflexibility—it introduces new challenges in retrieval quality, grounding fidelity,
pipeline efficiency, and robustness against noisy or adversarial inputs. This survey provides a comprehensive synthesis of recent
advances in RAG systems, offering a taxonomy that categorizes architectures into retriever-centric, generator-centric, hybrid, and
robustness-oriented designs. We systematically analyze enhancements across retrieval optimization, context filtering, decoding control,
and efficiency improvements, supported by comparative performance analyses on short-form and multi-hop question answering tasks.
Furthermore, we review state-of-the-art evaluation frameworks and benchmarks, highlighting trends in retrieval-aware evaluation,
robustness testing, and federated retrieval settings. Our analysis reveals recurring trade-offs between retrieval precision and generation
flexibility, efficiency and faithfulness, and modularity and coordination. We conclude by identifying open challenges and future
research directions, including adaptive retrieval architectures, real-time retrieval integration, structured reasoning over multi-hop
evidence, and privacy-preserving retrieval mechanisms. This survey aims to consolidate current knowledge in RAG research and serve
as a foundation for the next generation of retrieval-augmented language modeling systems.
CCS Concepts: •Information systems →Retrieval models and ranking ;Evaluation of retrieval results ;Information retrieval
query processing ;Retrieval tasks and goals ;Document representation .
Additional Key Words and Phrases: Retrieval-Augmented Generation, Query Reformulation, Context Filtering, Reranking, Multi-hop
Reasoning, Hallucination Mitigation, Robustness, Dynamic Retrieval, Evaluation Benchmarks, Federated Retrieval, Faithfulness,
Efficiency Optimization, Document Ranking, LLM Alignment, Open-Domain QA
1 Introduction
Large Language Models (LLMs) have demonstrated impressive generalization across natural language tasks, but
their reliance on static, parametric knowledge remains a fundamental limitation. This restricts their ability to handle
queries requiring up-to-date, verifiable, or domain-specific information, often resulting in hallucinations or factual
inconsistencies [19, 40].
Retrieval-Augmented Generation (RAG) addresses this issue by coupling pretrained language models with non-
parametric retrieval modules that fetch external evidence during inference. By conditioning generation on retrieved
documents, RAG systems offer greater transparency, factual grounding, and adaptability to evolving knowledge bases.
These properties have made RAG central to tasks such as open-domain QA, biomedical reasoning, knowledge-grounded
dialogue, and long-context summarization.
However, integrating retrieval with generation introduces unique challenges: retrieval noise and redundancy can
degrade output quality; misalignment between retrieved evidence and generated text can lead to hallucinations;
and pipeline inefficiencies and latency make deployment costly at scale. Moreover, balancing modularity with tight
retrieval–generation interaction remains an open architectural trade-off.
In this survey, we first present a high-level taxonomy of RAG architectures based on where core innovations
occur—within the retriever, the generator, or through their joint coordination (Section 3). We begin with a background
Author’s Contact Information: Chaitanya Sharma, Independent Researcher, United States.
Manuscript submitted to ACM 1
Retrieval-Augmented Generation: A Survey 2
Fig. 1. Retrieval-Augmented Generation (RAG) workflow. A user query is processed by the retriever, which may perform
query expansion before retrieving documents from external knowledge sources (e.g., databases, APIs, or document stores). Retrieved
documents are re-ranked by relevance, and the Top-K are passed to the generator as factual context. The generator synthesizes
a response conditioned on both the query and retrieved content. An optional post-processing step (e.g., ranking, rewriting, or
fact-checking) may further refine the output, enhancing factual consistency, real-time adaptability, and overall response quality in
large language models (LLMs).
on RAG’s mathematical formulation and components (Section 2.2), and then explore advances in retrieval strategies,
filtering, and control mechanisms (Section 4). We further analyze how RAG systems are benchmarked (Section 6),
compare prominent frameworks (Section 5), and conclude with open research challenges and future directions (Section 7).
2 Background and foundations of retrieval-augmented generation
Retrieval-Augmented Generation (RAG) is a framework that augments large language models (LLMs) with external
knowledge access via document retrieval. It builds on the intuition that generating grounded and verifiable responses
requires not only parametric knowledge stored in model weights, but also non-parametric access to a dynamic evidence
corpus. This section outlines the core components of RAG systems and presents the mathematical formulation that
underpins their design.
2.1 Components of a RAG System
At a high level, a RAG system consists of three modules:
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 3
Query Encoder: Encodes the input 𝑥into a query representation 𝑞, which is used to retrieve relevant documents.
This can be either a neural encoder or a rule-based template.
Retriever: Given the query 𝑞, the retriever fetches a ranked list of documents 𝑑1, 𝑑2, . . . , 𝑑 𝑘from a corpusC.
Retrievers may be sparse (e.g., BM25 [54]), dense (e.g., DPR [36]), hybrid, or generative.
Generator: The generator conditions on the input 𝑥and the retrieved documents 𝑑𝑖to produce the final output 𝑦.
This is typically a pretrained transformer model (e.g., T5 [51], BART [39], GPT [5]).
2.2 Mathematical Formulation
Formally, the generation process in Retrieval-Augmented Generation (RAG) can be expressed as modeling the conditional
distribution:
𝑃(𝑦|𝑥)=∑︁
𝑑∈C𝑃(𝑦|𝑥, 𝑑)·𝑃(𝑑|𝑥) (1)
where:
•𝑥is the input (e.g., a question or prompt),
•𝑑is a retrieved document from corpus C,
•𝑦is the generated response.
In practice, the summation is approximated by retrieving the top- 𝑘documents 𝑑1, . . . , 𝑑 𝑘, yielding:
𝑃(𝑦|𝑥)≈𝑘∑︁
𝑖=1𝑃(𝑦|𝑥, 𝑑 𝑖)·𝑃(𝑑𝑖|𝑥) (2)
This decomposition reflects two key probabilities:
•𝑃(𝑑𝑖|𝑥): the relevance score of document 𝑑𝑖given the input 𝑥, often derived from a retriever or reranker.
•𝑃(𝑦|𝑥, 𝑑 𝑖): the probability of generating output 𝑦conditioned on 𝑥and document 𝑑𝑖, modeled by the language
model.
Variants of RAG differ in how they estimate and combine these components. Some use a fixed retriever and let the
generator handle noisy inputs, while others jointly optimize retrieval and generation to maximize downstream utility.
3 Taxonomy of RAG Architectures
To contextualize recent advances in Retrieval-Augmented Generation (RAG), we propose a taxonomy that categorizes
existing systems based on their architectural focus—retriever-centric, generator-centric, hybrid, and robustness-oriented
designs. This classification highlights key design patterns and illustrates how different frameworks tackle the core
challenges of retrieval, grounding, and reliability.
3.1 Retriever-Based RAG Systems
Retriever-based Retrieval-Augmented Generation (RAG) systems delegate architectural responsibility primarily to
the retriever, treating the generator as a passive decoder. These systems operate under the premise that the fidelity
and relevance of the retrieved context are the most critical factors for generating accurate and grounded outputs.
Innovations in this space typically fall into one of three design patterns: input-side query enhancement, retriever-side
adaptation, and retrieval granularity optimization.
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 4
Fig. 2. Figure 2: Taxonomy of Retrieval-Augmented Generation (RAG) Systems. This taxonomy categorizes RAG frameworks
based on their primary architectural orientation—retriever-based, generator-based, hybrid, and robustness-focused designs. Retriever-
based models are further grouped into query-centric, retriever-centric, and granularity-aware approaches, while generator-based
models include faithfulness-aware decoding, context compression, and retrieval-guided generation. Hybrid systems are organized by
retrieval dynamics (e.g., multi-round, utility-driven), and robustness-oriented models address challenges such as noise, hallucination,
and adversarial vulnerabilities. This structure highlights the diverse innovations shaping the RAG landscape.
Query-Driven Retrieval: A prominent strategy focuses on refining and structuring user intent before retrieval to
maximize alignment with relevant corpus segments. This includes decomposition, rewriting, generative reformulation,
and the incorporation of structured priors to guide retrieval. Notable examples include RQ-RAG (Refine Query for
RAG) [ 6], which decomposes multi-hop queries into latent sub-questions, and GMR (Generative Multi-hop Retrieval) [ 38],
which uses a generative LLM to autoregressively formulate complex multi-hop queries. RAG-Fusion [ 50] further
improves recall by combining results from multiple reformulated queries through reciprocal rank fusion [ 13]. Structured
approaches have also emerged: KRAGEN (Knowledge Retrieval Augmented Generation ENgine) [ 43] introduces graph-of-
thoughts prompting to decompose complex queries into subproblems, retrieving relevant subgraphs to guide multi-hop
reasoning. Additionally, LQR (Layered Query Retrieval) [ 25] implements hierarchical planning over multi-hop questions,
while Sparse Context Selection [90] emphasizes efficient sparse reformulations for both recall and speed.
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 5
Retriever-Centric Adaptation: Another line of work modifies the retriever itself through architectural enhance-
ments or task-specific learning. Re2G (Retrieve, Rerank, Generate) [ 20] blends symbolic and neural retrieval via reranking
layers, while SimRAG (Self-Improving RAG) [ 73] employs self-training over synthetic QA pairs to improve domain
generalization. Frameworks like RankRAG [ 83] and uRAG (unified RAG) [ 57] emphasize retriever versatility—either by
unifying reranking and generation within a single backbone or by training general-purpose retrievers across varied
downstream tasks. Additionally, systems such as ToolRerank [ 88] dynamically adapt retrieval strategies based on query
semantics, optimizing tool selection in specialized retrieval settings. Relatedly, SEER (Self-Aligned Evidence Extraction
for RAG) [ 87] focuses on post-retrieval adaptation, advancing corpus-based evidence extraction by aligning evidence
selection with faithfulness, helpfulness, and conciseness criteria, thereby improving evidence quality for open-domain
and temporally sensitive queries.
Granularity-Aware Retrieval: This pattern addresses retrieval precision by optimizing the unit of retrieval—from
full documents to fine-grained, semantically aligned segments. LongRAG [ 31] exemplifies this by retrieving compressed
long-context chunks, constructed through document grouping, to better exploit long-context language models. Similarly,
FILCO (Filter Context) [ 69] enhances retrieval granularity by filtering irrelevant or low-utility spans from retrieved
passages before generation, improving the faithfulness and efficiency of RAG outputs. In parallel, the Sufficient Context
analysis framework [ 34] offers a complementary lens, evaluating whether retrieved contexts contain enough information
to support accurate generation, thereby highlighting the importance of granular retrieval quality for system robustness.
Each of these patterns anchors its innovation in the retriever, preserving modularity and interpretability. However,
they also introduce trade-offs in latency, redundancy, and sensitivity to ambiguous or underspecified queries. Sec-
tion 4 elaborates on how downstream enhancements—such as reranking, adaptive filtering, and utility-based context
selection—further address these limitations.
3.2 Generator-Based RAG Systems
Generator-based RAG systems concentrate architectural innovation on the decoding process, assuming the retrieved
content is sufficiently relevant and shifting the burden of factual grounding and integration to the language model.
These systems enhance output quality through mechanisms for self-verification, compression, and controlled generation.
We identify three recurring design patterns within this category: faithfulness-aware decoding, context compression and
utility filtering, and retrieval-conditioned generation control.
Faithfulness-Aware Decoding: To reduce hallucinations and improve factual grounding, several systems embed
mechanisms for self-reflection, verification, or correction during generation. SELF-RAG (Self-Reflective RAG) [ 1]
introduces a critique–generate loop, allowing the model to assess and revise its outputs before finalization. SelfMem [ 9]
builds on this by incorporating a self-memory module that enables the model to revisit and refine prior generations.
INFO-RAG [ 76] treats the LLM as a denoising module trained with contrastive objectives. Collectively, these systems
decouple output faithfulness from retrieval fidelity, enabling recovery even when retrieval is suboptimal.
Context Compression and Utility Filtering: To address context window limitations, several systems optimize
retrieval inputs into denser or more structured forms. FiD-Light [ 24], a streamlined variant of the Fusion-in-Decoder
(FiD) architecture [ 27], improves decoding efficiency by compressing encoder outputs across retrieved passages and
pruning cross-passage attention without modifying retrieval mechanisms. xRAG [ 10] projects document embeddings
directly into the model’s representation space, minimizing token overhead through modality fusion. Rich Answer
Encoding (RAE) [ 26] enhances retrieval relevance by embedding answer-aligned semantics into retriever outputs rather
than relying on token overlap. GenRT [ 75] further refines retrieval utility by reranking and dynamically truncating
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 6
retrieved lists, retaining only the most contextually valuable candidates for generation. Complementing these designs,
an information bottleneck-based filtering approach [ 89] selectively preserves evidence most informative for generation.
Together, these strategies advance decoding efficiency and context quality, particularly for long-form and multi-hop
RAG tasks.
Retrieval-Guided Generation: A third strategy modulates generation based on retrieval metadata, task-specific
cues, or agentic decision-making. AU-RAG (Agent-based Universal RAG) [ 28] exemplifies this by using an agent to
decide dynamically between retrieved and parametric knowledge across diverse data environments. RAG-Ex [ 62]
perturbs retrieval context to analyze how variability influences model behavior and reliance on external evidence. R2AG
(Retrieval information into RAG) [ 82] extends this by recursively reranking candidates during generation, dynamically
prioritizing evidence based on the evolving answer state. In high-stakes domains, Confidence-Calibrated RAG [ 48]
shows that document ordering and prompt structure affect output certainty, highlighting the need for calibration
alongside factual accuracy.
These architectures are particularly suited to domains where factual correctness, reasoning transparency, or structured
output formats are essential—such as biomedical QA, finance, and enterprise workflows. While they leave the retriever
fixed, many of their techniques are complementary to retrieval-side enhancements and can be layered atop other RAG
variants. Section 4.2 further explores compression, reranking, and decoding control strategies in these systems.
3.3 Hybrid RAG Systems
Hybrid RAG systems tightly couple the retriever and generator, moving beyond modular architectures to treat retrieval
and generation as co-adaptive reasoning agents. These systems emphasize iterative feedback, utility-aware coordination,
and dynamic control over retrieval actions, particularly in open-domain, multi-hop, and evolving knowledge contexts.
We identify three dominant architectural patterns: iterative or multi-round retrieval, utility-driven joint optimization,
and retrieval-aware generation control.
Iterative or Multi-Round Retrieval: These systems interleave retrieval and generation across multiple reasoning
steps, allowing for evidence refinement and progressive answer construction. IM-RAG (Inner Monologue RAG) [ 80]
simulates an “inner monologue” by alternating between generation and retrieval phases, supporting multi-step reasoning.
Generate-Then-Ground (GenGround) [ 59] follows a similar philosophy, generating a provisional answer first and then
retrieving supporting evidence to substantiate or revise it—improving factuality and interpretability in multi-hop settings.
G-Retriever [ 22] retrieves graph-structured subcomponents as generation unfolds, enhancing complex reasoning over
textual graphs.
Utility-Driven Joint Optimization: Several frameworks seek to align retriever outputs with their downstream
utility for generation through joint objectives or reinforcement learning. Stochastic RAG [ 84] treats retrieval as an
expected utility maximization problem, updating both retriever and generator end-to-end using REINFORCE-based
gradients. M-RAG [ 70] applies multi-agent reinforcement learning, coordinating distributed retrievers and generators
via shared memory and task-specific roles. MedGraphRAG [ 72] integrates knowledge graphs into the joint learning loop,
facilitating domain-specific reasoning with structured priors. These systems improve factuality and answer consistency,
particularly in biomedical and enterprise domains.
Dynamic Retrieval Triggering: A growing class of systems dynamically controls when and how to retrieve, condi-
tioned on generation uncertainty, task complexity, or intermediate outputs. DRAGIN (Dynamic Retrieval Augmented
Generation based on the Information Needs of LLMs) [ 61] triggers retrieval at the token level using entropy-based
confidence signals, while FLARE (Forward-Looking Active REtrieval augmented generation () [ 32] selectively retrieves
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 7
based on low-confidence predictions during sentence generation. SELF-ROUTE [ 41] dynamically routes tasks between
retrieval and generation modules based on model self-assessed difficulty, and AU-RAG [ 28] leverages agentic decision-
making to mediate between diverse retrieval sources and procedural knowledge. TA-ARE (Time-Aware Adaptive
REtrieval) [ 86] introduces a retrieval trigger classifier that adaptively determines when retrieval is necessary and adjusts
the granularity of evidence based on query needs. A related approach, CRAG (Corrective RAG) [ 79], evaluates retrieved
evidence quality before generation and dynamically decides whether to proceed with generation, re-trigger retrieval, or
decompose the input into simpler sub-queries. This corrective mechanism positions CRAG within the hybrid class, as it
tightly coordinates retrieval assessment with adaptive generation pathways under uncertainty.
These architectures reflect a broader trend toward treating retrieval as a controllable, contextualized act rather than
a fixed preprocessing step. Their strength lies in adaptivity, coordination, and the capacity to handle under-specified
or evolving queries. However, they introduce new challenges in training stability, latency, and system transparency—
especially when retrieval is performed mid-decoding. These trade-offs, as well as efficiency-oriented enhancements like
pipelining and reranking, are further explored in Section 4.
3.4 Robustness and Security-Oriented RAG Systems
Robustness- and security-oriented RAG systems are designed to preserve output quality in the face of noisy, irrelevant,
or adversarially manipulated retrieval contexts. Unlike models that optimize retrieval or generation under ideal
assumptions, these systems explicitly address worst-case scenarios—such as hallucination under misleading evidence,
retrieval failures, or corpus poisoning. We identify three major design strategies in this category: noise-adaptive training,
hallucination-aware decoding constraints, and adversarial robustness.
Noise-Adaptive Training Objectives: These systems aim to make RAG outputs resilient to degraded or spurious
input evidence by training under perturbed, irrelevant, or misleading contexts. RAAT [ 18] classifies retrieved passages
into relevant, irrelevant, or counterfactual categories and introduces an adversarial training objective to maximize worst-
case performance. Bottleneck Noise Filtering [ 89] applies information bottleneck theory to identify the intersection
of useful and noisy information, compressing retrieved context into minimal, high-utility representations. These
approaches are particularly effective in retrieval-heavy pipelines where context precision cannot be guaranteed.
Hallucination-Aware Decoding Constraints: To mitigate factual inaccuracies in generation, several systems in-
troduce decoding-time constraints or architectures designed to enforce grounding. RAGTruth [ 46] provides benchmarks
and evaluation protocols for hallucination detection, guiding system-level design. Structured retrieval-based approaches
have also been explored: one method [ 24] retrieves executable templates (e.g., JSON workflows) to constrain output
generation, minimizing reliance on generative interpolation and reducing domain-specific hallucination. RAG-Ex [ 62]
simulates retrieval variability by injecting perturbed documents during training, improving robustness to inconsistent
or adversarial context. In high-stakes domains such as healthcare, Confidence-Calibrated RAG [ 48] explores how
document ordering and prompt design affect both answer accuracy and model certainty.
Adversarial Robustness and Security: Emerging work also highlights new vulnerabilities. BadRAG [ 78] and
TrojanRAG [ 8] demonstrate that adversarially poisoned passages can serve as semantic backdoors, triggering spe-
cific behaviors in LLM outputs even when base models remain unmodified. These attacks rely on stealthy corpus
manipulations that are hard to detect and pose significant threats in open-domain or API-exposed RAG systems.
Collectively, these systems complement retrieval- and generation-oriented architectures by offering essential safety
guarantees in real-world deployments. Their robustness strategies—ranging from retrieval verification and context
compression to constrained generation—are modular and often integrable into existing RAG pipelines.
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 8
4 Enhancements in RAG
Recent advancements in Retrieval-Augmented Generation (RAG) increasingly focus on targeted enhancements across
the retrieval–generation pipeline. Beyond architectural baselines, these enhancements address key limitations in
retrieval quality, context integration, computational efficiency, robustness to perturbations, and ranking precision.
This section delineates five core areas of optimization—retrieval, filtering, efficiency, robustness, and reranking—each
contributing to the development of more reliable and performant RAG systems, and collectively summarized in Table 1,
which compares representative methods based on their mechanisms, strengths, limitations, and ideal use cases.
4.1 Retrieval Enhancement
RAG systems have increasingly adopted smarter retrieval strategies to mitigate inefficiencies such as redundant lookups,
irrelevant context, and computational overhead. These improvements can be categorized into four major families:
adaptive retrieval, multi-source retrieval, query refinement, and hybrid or structured retrieval. Each addresses a distinct
bottleneck in the retrieval pipeline, offering trade-offs in latency, scalability, and faithfulness.
Adaptive retrieval dynamically adjusts when to retrieve based on model uncertainty or predictive confidence.
TA-ARE replaces static thresholds with a learned estimator, reducing redundant retrievals by 14.9% in short-form tasks.
DRAGIN takes this further by applying retrieval at the token level, using entropy signals to detect knowledge gaps and
triggering retrieval through a self-attentive query formulation process. Though it improves multi-hop QA precision,
DRAGIN introduces notable inference costs, mitigated through adaptive frequency thresholds. FLARE proactively
anticipates knowledge needs before uncertainty arises, improving faithfulness but requiring careful thresholding to
avoid excessive retrieval.
Multi-source retrieval targets adaptability across evolving corpora or specialized domains. AU-RAG introduces
agent-based retrieval, dynamically selecting sources based on metadata heuristics. This improves domain coverage
but necessitates hierarchical pipelines to manage source prioritization. SimRAG enhances retrieval precision using
self-supervised learning on synthetic QA pairs, filtered via round-trip consistency. While it achieves 1.2–8.6% accuracy
gains across datasets, it risks overfitting, mitigated by human-in-the-loop validation.
Query refinement techniques enhance retrieval relevance by modifying ambiguous or underspecified queries.
RQ-RAG uses perplexity-driven decomposition and rewriting to improve relevance, especially in multi-fact scenarios.
However, this incurs inference overhead, mitigated through selective refinement based on query ambiguity. R2AG
improves post-retrieval alignment by injecting retrieval metadata into prompts, bridging the retriever–generator
semantic gap. Though effective, it adds computational cost, addressed by only enabling metadata prompting when
retrieval scores fall below a relevance threshold.
Hybrid and structured retrieval approaches improve coherence by integrating unstructured and structured
sources. M-RAG clusters knowledge into semantic partitions, with dual agents selecting and refining content. It reduces
noise but introduces latency, mitigated by dynamic partition expansion. KRAGEN retrieves subgraphs from knowledge
graphs, using Graph-of-Thoughts prompting for relational reasoning. This reduces hallucinations by 20–30%, though it
increases memory overhead, controlled via selective node expansion.
Extending hybrid retrieval designs, the Dual-Pathway KG-RAG framework [ 74] combines structured retrieval
from knowledge graphs with unstructured corpus retrieval in parallel, enhancing factual consistency and reducing
hallucinations by 18% in biomedical QA tasks. Similarly, Graph RAG [ 16] constructs entity-centric graphs from retrieved
passages and uses community summarization to scale RAG to large corpora, improving multi-hop QA recall by 6.4
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 9
points compared to baseline retrieval. Likewise, Customer Service QA [ 77] integrates RAG with knowledge graphs
constructed from issue-tracking tickets, achieving a 77.6% improvement in retrieval MRR and a 28.6% reduction in
resolution time when deployed at LinkedIn’s customer service team.
In a complementary direction, Doan et al. [ 15] propose a lightweight hybrid retrieval strategy that combines
unstructured text embeddings with structured knowledge graph embeddings without requiring complex retriever
re-training, achieving up to 13.1% improvements in retrieval correctness and ranking precision in domain-specific RAG
deployments.
4.2 Enhancing Context Relevance through Filtering
Despite advances in retrieval models, RAG systems often integrate irrelevant, redundant, or semantically noisy docu-
ments that degrade generation quality. Filtering techniques aim to reduce hallucinations and improve answer relevance
by selecting only contextually appropriate content. These methods vary in supervision, granularity, and efficiency, and
can be categorized into three groups: lexical/statistical filters, information-theoretic optimizers, and self-supervised
passage scoring.
Lexical filters such as FILCO apply word overlap and statistical relevance scoring. Using STRINC and CXMI metrics,
FILCO removes low-relevance passages and reduces hallucinations by up to 64%, improving EM by +8.6. However, its
reliance on lexical similarity limits its adaptability across domains and query styles.
Information-theoretic methods like IB Filtering [ 89] use principles from the information bottleneck framework
to retain only high-utility input features while discarding noise. Though computation-heavy, IB Filtering improves
EM by +3.2 with a 2.5% compression ratio, offering a balance between precision and conciseness. Similarly, Stochastic
Filtering models retrieval as an expected utility maximization problem and re-ranks passages based on marginal value,
achieving consistent retrieval effectiveness gains with minimal retriever changes.
Self-supervised methods like SEER and RAG-Ex use internal feedback signals to filter noisy retrievals. SEER
applies label-free training and generates pseudo-relevance judgments, improving F1 by 13.5% and achieving a 9.25 ×
reduction in context length. RAG-Ex perturbs retrieved passages and compares generation outcomes, selecting those
that maximize semantic consistency. It aligns with human-assessed faithfulness 76.9% of the time and is model-agnostic,
though computationally expensive due to multiple inference passes.
Collectively, these methods balance retrieval compression, answer faithfulness, and domain adaptability. While
lexical filters are efficient, self-supervised models provide deeper semantic filtering and support long-form reasoning.
4.3 Efficiency Enhancements
While Retrieval-Augmented Generation (RAG) significantly enhances factual consistency in large language models
(LLMs) by integrating external document retrieval, it introduces new inefficiencies. These include increased memory
overhead, latency from retrieval-processing pipelines, and redundancy in passage selection. This section synthesizes
key research efforts aimed at improving retrieval efficiency across four areas: sparse retrieval and context selection,
inference acceleration, caching and redundancy reduction, and retrieval faithfulness.
Sparse context selection and retrieval-aware generation techniques aim to reduce the input length and improve
semantic alignment without sacrificing output quality. Sparse RAG addresses this by filtering low-relevance content
before self-attention, retaining only high-signal tokens via parallel encoding. While it builds on Fusion-in-Decoder (FiD),
it improves efficiency by avoiding dense input concatenation. However, it may discard useful context under suboptimal
retrieval, requiring fine-tuning to maintain robustness. R2AGtakes a complementary approach by embedding retrieval
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 10
representations directly into the LLM’s context space, enhancing semantic alignment. Unlike prompt-based methods
(e.g., REPLUG [ 58]),R2AGbypasses explicit concatenation, reducing redundant processing. Both approaches enhance
efficiency at different stages but require retriever fine-tuning and increase model complexity.
Inference acceleration strategies focus on reducing decoding latency in autoregressive models by minimizing
redundant token processing. FiD-Light achieves this through token-level passage compression, which lowers decoding
time while preserving key information. Though effective, aggressive filtering can marginally reduce retrieval precision.
Speculative Pipelining [ 71] further reduces latency by overlapping retrieval and generation. It incrementally processes
top-𝑘candidates before retrieval completes, lowering time-to-first-token (TTFT) by 20–30%. However, it risks speculative
hallucinations unless controlled by fallback mechanisms and selective decoding checkpoints. This line of work opens the
door for future speculative decoding architectures—discussed in Section 8—that balance responsiveness and reliability
in low-latency applications.
Caching and redundancy reduction techniques aim to eliminate recomputation overhead in repetitive or high-
throughput workloads. RAGCache [ 33] introduces a hierarchical caching system that stores key-value tensors from
prior retrievals. PGDSF extends this with prefix-aware eviction that prioritizes frequent and important documents.
While these methods significantly improve efficiency in common-query settings, their impact diminishes on long-tail
distributions and introduces cache complexity.
Retrieval faithfulness and answer relevance methods go beyond lexical similarity to ensure that retrieved
documents are factually aligned with the generated output. Rich Answer Encoding (RAE) addresses this using a
Retriever-as-Answer Classifier (RAC) and Dense Knowledge Similarity (DKS), which rescore documents based on their
plausibility. RAE reduces hallucinations and improves grounding but requires retriever retraining, increasing cost.
Taken together, these optimization strategies enhance efficiency across the RAG pipeline: Sparse RAG and R2AG
improve alignment between retrieved documents and generation; FiD-Light and Speculative Pipelining reduce latency
during inference; RAGCache and PGDSF minimize recomputation in high-throughput environments; and RAE advances
retrieval faithfulness. Collectively, they represent a move toward more scalable, accurate, and computationally efficient
RAG systems.
4.4 Enhancing Robustness
RAG systems improve factual accuracy in language models by retrieving external information. However, they remain
vulnerable to retrieval noise, hallucinations, and adversarial attacks. While past research has addressed these challenges
separately—such as noise resilience, hallucination control, and retrieval security—a unified perspective is essential. This
section groups robustness techniques into three areas: noise mitigation, hallucination reduction, and security defenses.
Empirical studies further support this need for a unified view; a recent study identifies seven recurrent failure
points in operational RAG systems, spanning retrieval errors, context consolidation failures, hallucinated outputs, and
incomplete answers [4].
Noise mitigation strategies target irrelevant, misleading, or adversarial content that can degrade RAG accuracy.
However, recent work challenges the assumption that all retrieval noise is detrimental; Cuconasu et al. [ 14] demonstrate
that carefully positioned random documents can paradoxically improve LLM reasoning and answer quality by promoting
evidence selection behaviors. Two contrasting approaches address this: RAAT and CRAG. RAAT uses adversarial
pretraining to expose models to subtle and counterfactual retrieval noise, improving F1/EM scores by 20–30%. Its
high training cost limits it to static, high-stakes domains. CRAG filters low-confidence retrievals at inference time and
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 11
works well in real-time systems, reducing retrieval errors by 12–18%. However, it struggles with “soft noise,” where
superficially relevant content misleads the model.
Hallucination reduction techniques such as Structured RAG [ 2] and IM-RAG aim to improve the faithfulness of
generated content. Structured RAG constrains retrieval to verified corpora, lowering hallucination rates by 30–40% with
minimal compute cost. Its drawback is poor adaptability, requiring manual updates. IM-RAG uses iterative retrieval
refinement, achieving +5.3 F1 / +7.2 EM on HotPotQA. Though more accurate in evolving domains, it is computationally
intensive and slower at inference.
Security defenses focus on adversarial threats such as data poisoning and backdoor attacks. Research into BadRAG
shows that poisoning just 0.04% of a corpus can lead to a 98.2% attack success rate and 74.6% system failure. Defenses like
cryptographic document signing or adversarial filtering are only partially effective. TrojanRAG embeds backdoors in
retrieval embeddings, bypassing traditional sanitization. Stronger mitigations—secure training and integrity validation—
are needed but require proactive design. Beyond adversarial attacks, privacy vulnerabilities in RAG systems have also
been identified; Zeng et al. [ 85] show that both retrieval databases and pretraining corpora can be exploited through
structured prompting, although retrieval can paradoxically help reduce memorization leakage by acting as a grounding
mechanism.
4.5 Enhancements and Optimizations in Reranking
Reranking plays a vital role in improving the relevance and faithfulness of Retrieval-Augmented Generation (RAG)
outputs. While initial retrieval stages often return noisy results, reranking refines document ordering before passage
selection and generation, reducing hallucinations and improving response accuracy. Recent work advances reranking
across three key areas: adaptive reranking, unified pipelines, and fusion-based reranking.
Adaptive reranking methods dynamically adjust the number of documents reranked based on query complexity.
RLT [ 44] uses ranked list truncation to improve MRR/nDCG while reducing retrieval noise by 15%. ToolRerank further
adapts reranking depth based on familiarity with seen vs. unseen tools, boosting recall by 12% in hierarchical retrieval
tasks. These methods optimize computation by avoiding unnecessary reranking in low-complexity scenarios.
Unified reranking pipelines combine retrieval, document ranking, and generation within a single architecture.
RankRAG fine-tunes a language model to jointly score documents and generate answers, improving MRR@10 by 7.8%
while reducing latency. uRAG extends this to multiple tasks—like QA and fact verification—using shared reranking
logic and user-feedback signals, improving cross-task generalization by 8% MRR@10. These approaches eliminate the
overhead of separate ranking modules and increase retrieval consistency.
Fusion-based reranking strategies aggregate evidence from multiple query variants to improve answer robustness.
RAG-Fusion generates multiple subqueries and applies reciprocal rank fusion, improving answer accuracy by 9%. R2AG
refines these rankings iteratively, reducing irrelevant retrievals by 15% through recursive feedback. These models are
especially effective for multi-hop and ambiguous tasks.
Reranking methods significantly boost the efficiency and faithfulness of RAG systems. Future work may explore
hybrid approaches combining adaptive truncation with fusion-based aggregation, as well as domain-adaptive reranking
for enterprise scalability. As RAG expands to more tasks and domains, reranking will remain essential to enabling
context-aware, trustworthy generation.
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 12
Table 1. Summary of RAG System Enhancements. This table categorizes enhancements across five dimensions—retrieval, filtering,
efficiency, robustness, and reranking. Each entry specifies the enhancement type, method, mechanism, key strengths, known
limitations, and ideal use cases.
Enhancement
TypeCategory Method Mechanism Strengths Limitations Best Use Case
RetrievalAdaptive TA-ARE Dynamic confidence estima-
tionReduces redundant retrieval Estimator latency Short-form QA
Adaptive DRAGIN Token-level entropy-based trig-
gersImproves multi-hop QA preci-
sionHigh inference cost Multi-hop QA
Adaptive FLARE Preemptive uncertainty detec-
tionEnhances faithfulness Risk of over-retrieval Long-form generation
Multi-source AU-RAG Agent-based source selection High domain adaptability Source management overhead Evolving corpora
Multi-source SimRAG Synthetic QA + round-trip fil-
teringCross-domain accuracy gains Overfitting risk Specialized domains
Query RQ-RAG Perplexity-based query rewrit-
ingImproves query clarity and rel-
evanceAdditional inference steps Multi-fact queries
Query R2AG Retrieval-aware prompt injec-
tionEnhances factual grounding Prompt expansion overhead Low-confidence queries
Hybrid M-RAG Semantic partitioning + dual
agentsReduces retrieval noise Partition latency Context-heavy reasoning
Hybrid KRAGEN Knowledge graph subgraph re-
trievalImproves structured reasoning Memory and compute inten-
siveBiomedical, graph-based tasks
FilteringLexical FILCO STRINC + CXMI scoring +8.6 EM, 64% hallucination re-
ductionQuery-style bias Structured QA
Info-Theoretic IB Filtering Bottleneck-based compression +3.2 EM, 2.5% compression Computation overhead High-precision QA
Info-Theoretic Stochastic
FilteringUtility-maximizing re-ranking Improves effectiveness Needs custom scoring Lightweight retrieval tasks
Self-Supervised SEER Pseudo-relevance via self-
training+13.5% F1, 9.25x context reduc-
tionHigh training cost Open-domain QA
Self-Supervised RAG-Ex Generation perturbation com-
parison76.9% human-aligned faithful-
nessMultiple inference passes Faithful generation
EfficiencySparse Selection Sparse RAG Retains high-signal tokens Reduces memory, improves rel-
evanceMay discard useful docs Long-context tasks
Sparse Selection R2AG Context-aware retrieval injec-
tionEnhances coherence, lowers re-
dundancyRetriever fine-tuning needed Knowledge-intensive QA
Inference Acceler-
ationFiD-Light Compresses passages Faster decoding Slight loss in recall Low-latency applications
Caching Speculative
PipeliningOverlaps retrieval and genera-
tion20–50% TTFT reduction Risk of hallucination Real-time applications
Caching RAGCache Hierarchical cache w/ PGDSF Eliminates recomputation Cache complexity in long-tail High-throughput workloads
Retrieval Quality RAE Retriever-as-answer scorer Boosts grounding and preci-
sionRequires scoring/retraining Factual QA
RobustnessNoise Mitigation RAAT Adversarial training +20–30% F1/EM High training cost Offline pretraining
Noise Mitigation CRAG Inference-time filtering +12–18% precision gain Ineffective on “soft” noise Real-time support
Hallucination
ControlStructured
RAGCurated corpus retrieval 30–40% hallucination reduction Low adaptability Static domains
Hallucination
ControlIM-RAG Iterative retrieval refinement +5.3 F1 / +7.2 EM Inference latency Multi-hop QA
Security BadRAG Adversarial retrieval poisoning Demonstrates corpus-level
threatNeeds stronger filtering Security evaluation
Security TrojanRAG Embedding-level backdoor Persistent attack vector Requires secure training Security-sensitive pipelines
RerankingAdaptive RLT Dynamic list truncation +15% noise reduction Heuristic tuning needed Real-time QA
Adaptive ToolRerank Familiarity-aware reranking +12% recall for unseen tools Complexity for un-
seen/frequent toolsTool-aware retrieval
Unified Pipeline RankRAG Joint rerank + generate +7.8% MRR@10 Domain-specific tuning End-to-end QA systems
Unified Pipeline uRAG Shared reranking engine +8% MRR@10, task generaliza-
tionHigher setup cost Multi-task enterprise RAG
Fusion-based RAG-Fusion Reciprocal rank fusion +9% accuracy Query explosion risk Complex multi-hop QA
Fusion-based R2AG Recursive reranking refine-
ment15% irrelevant retrieval reduc-
tionHigher latency Iterative reasoning
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 13
Table 2. Comparative Performance of Retrieval-Augmented Generation Frameworks Across Multi-Hop and Short-Form
QA Benchmarks. This table reports relative performance improvements achieved by each RAG framework over two baselines: (i) the
raw backbone language model (B) and (ii) the same model augmented with a standard retrieval module (B+R). Results are shown
across multi-hop benchmarks (HotpotQA [ 81], 2Wiki [ 23], MuSiQue [ 67]) and short-form QA datasets (PopQA [ 42], TriviaQA [ 35],
ARC-Challenge [ 12], NQ [ 37]), with metrics including F1, Exact Match (EM), and Accuracy (Acc). Frameworks are grouped by
architectural category: retriever-based, generator-based, and hybrid. A “–” indicates that the corresponding score was not reported in
the original publication. Backbone LLMs referenced in this table include LLaMA 2 [ 66], LLaMA 3 [ 21], GPT-3.5/4 [ 47], Vicuna [ 11],
Mistral [29], Mixtral [30], Gemini [53], and Gemma [64].
Framework Backbone HotpotQA 2Wiki MusiQue PopQA TriviaQA ARC-Challenge NQ
B/B+R B/B+R B/B+R B/B+R B/B+R B/B+R B/B+R
Retriever-Based RAG
RQ-RAG LLaMA2-7B 8.485/2.749 (F1) 1.8/1.396 (F1) 12.9/4.635 (F1) 2.884/0.434 (Acc) -/- 2.133/1.379 (Acc) -/-
SimRAG LLaMA3-8B -/- -/- -/- -/- -/- 0.145/- (Acc) -/-
SimRAG Gemma2-27B -/- -/- -/- -/- -/- 0.034/- (Acc) -/-
SEER LLaMA2-7B-Chat 0.104/0.037 (F1) -/- -/- -/- -/- -/- -/-
RankRAG LLaMA3-8B -/0.079 (F1) -/0.323 (F1) -/- -/- -/- -/- -/-
RankRAG LLaMA3-70B -/0.242 (F1) -/0.376 (F1) -/- -/- -/- -/- -/-
LQR LLaMA3-8B 2.081/0.516 (F1) 0.706/0.141 (F1) 2.922/0.841 (F1) -/- -/- -/- -/-
LongRAG GPT-4o 0.517/- (EM) -/- -/- -/- -/- -/- -/-
LongRAG Gemini-1.5-Pro 0.696/- (EM) -/- -/- -/- -/- -/- -/-
FILCO LLaMA2-7B -/0.057 (EM) -/- -/- -/- -/0.056 (EM) -/- -/0.298 (EM)
Re2G BART Large -/- -/- -/- -/- 0.251/- (Acc) -/- 0.144/-
Generator-Based RAG
xRAG Mistral-7B 0.26/-0.122 (EM) -/- -/- -/- 0.152/-0.002 (EM) -/- 0.293/-0.085 (EM)
xRAG Mixtral-8x7B 0.207/-0.087 (EM) -/- -/- -/- 0.043/0.054 (EM) -/- 0.126/0.047 (EM)
INFO-RAG LLaMA2-7B 0.182/- (EM) -/- 0.163/- (EM) -/- -/- -/- -/-
INFO-RAG LLaMA2-13B 0.222/- (EM) -/- 0.358/- (EM) -/- -/- -/- -/-
INFO-RAG LLaMA2-13B-chat 0.011/- (EM) -/- 0.018/- (EM) -/- -/- -/- -/-
SELF-RAG LLaMA2-7B -/- -/- -/- 2.735/0.437 (Acc) 1.177/0.562 (Acc) 2.092/0.404 (Acc) -0.505/- (Acc)
SELF-RAG LLaMA2-13B -/- -/- -/- 2.796/0.221 (Acc) 0.8/0.474 (Acc) -/- -/-
FiD-Light FiD+DPR -/- -/- -/- -/- 0.185/- (EM) -/- 0.27/- (EM)
R2AG LLaMA2-7B 3.231/- (F1) 34.52/4.445 (F1) -/- -/- -/- -/- 0.824/- (Acc)
Hybrid RAG
DRAGIN LLaMA2-7B-chat 0.218/0.338 (F1) 0.311/0.148 (F1) -/- -/- -/- -/- -/-
DRAGIN LLaMA2-13B-chat 0.368/0.144 (F1) 0.445/0.169 (F1) -/- -/- -/- -/- -/-
DRAGIN Vicuna-13B-v1.5 0.279/0.179 (F1) 0.575/0.371 (F1) -/- -/- -/- -/- -/-
FLAREdirect GPT-3.5 -/- 0.622/0.223 (F1) -/- -/- -/- -/- -/-
FLAREinstruct GPT-3.5 -/- 0.353/0.02 (F1) -/- -/- -/- -/- -/-
GenGround GPT-3.5 0.236/0.093 (F1) 0.219/0.122 (F1) 0.359/0.361 (F1) -/- -/- -/- -/-
Stochastic RAG FiD-Light (T5-Base) 0.066/- (F1) -/- -/- -/- -/0.036 (EM) -/- -/0.013 (EM)
Stochastic RAG FiD-Light (T5-XL) 0.065/- (F1) -/- -/- -/- -/0.016 (EM) -/- -/0.037 (EM)
CRAG LLaMA2-7B -/- -/- -/- 3.034/0.471 (Acc) -/- 1.514/0.173 (Acc) 0.045/- (Acc)
Self-CRAG LLaMA2-7B -/- -/- -/- 3.204/0.533 (Acc) -/- 2.083/0.439 (Acc) -/-
TA-ARE GPT-3.5 -/- -/- -/- -/- -/- -/- -/-
TA-ARE GPT-4 -/- -/- -/- -/- -/- -/- -/-
TA-ARE LLaMA2-7B -/- -/- -/- -/- -/- -/- -/-
5 Comparative Analysis
To assess the empirical effectiveness of design innovations in Retrieval-Augmented Generation (RAG), this section
presents a comparative analysis of representative frameworks across three key evaluation settings: short-form question
answering, multi-hop reasoning, and robustness under retrieval perturbations. Results are reported as relative improve-
ments over both raw and retrieval-augmented baselines, normalized for model and dataset variability. Additionally, we
review ablation studies from the literature to disentangle the contributions of specific components such as retrieval
triggers, filtering layers, reranking mechanisms, and robustness modules. These insights offer a clearer understand-
ing of which enhancements most significantly impact performance, faithfulness, and efficiency across diverse RAG
configurations.
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 14
5.1 Comparative Analysis of Framework Performance on Short-Form QA
This section presents a comparative analysis of Retrieval-Augmented Generation (RAG) frameworks in short-form
question answering, emphasizing their relative improvements over raw large language model (LLM) baselines and
retrieval-augmented baselines. As shown in Table 2, these comparisons focus on relative gains (e.g., a value of 2.7
indicates a 270% improvement) rather than absolute performance metrics, which normalize for variations in backbone
architectures, prompting strategies, and evaluation protocols. This approach enables a meaningful comparison across
diverse experimental setups.
Among generator-based RAG systems primarily optimized for accuracy, SELF-RAG consistently demonstrates
substantial gains across multiple datasets. It achieves over a 270% improvement from the raw LLM baseline on PopQA [ 42]
and over 200% on ARC-Challenge [ 12], illustrating the effectiveness of deep context integration for enhancing short-
form factual recall. FiD-Light, although also a generator-side enhancement, adopts a different optimization philosophy
centered on lightweight, efficient fusion of retrieved documents during decoding, yielding more moderate improvements
of 18–27% across TriviaQA [ 35] and NQ [ 37]. R2AG, another generator-based approach, shows promising gains, with
over 80% improvement from the baseline on NQ, further validating the benefits of integrating retrieval signals within
generation. We note that generator-based frameworks primarily designed for efficiency, such as xRAG, are discussed
separately due to their distinct optimization focus.
Retriever-based frameworks such as RQ-RAG and SimRAG also demonstrate notable gains. RQ-RAG achieves a
288% improvement on PopQA and over 210% on ARC-Challenge, reaffirming the importance of retrieval quality in
evidence-centric QA. SimRAG also shows strong improvements on ARC, although gains are more modest (approximately
14%). Additionally, retriever-side re-ranking approaches like FILCO deliver moderate but meaningful gains, with 5–30%
improvements across NQ and TriviaQA, further highlighting the incremental value of retrieval refinement strategies.
Hybrid frameworks exhibit a more heterogeneous pattern. CRAG and Self-CRAG achieve impressive gains, with
Self-CRAG delivering a 320% improvement on PopQA and a 208% improvement on ARC-Challenge, suggesting that
combining retrieval refinement with generation adaptation can be highly effective when well aligned. However, TA-ARE,
despite achieving a significant 28 ×improvement over raw baselines on RetrievalQA, occasionally underperforms relative
to the standard retrieval baseline, indicating that retrieval frequency reduction strategies, while efficient, may introduce
trade-offs. Stochastic RAG frameworks, meanwhile, display relatively modest gains (typically under 4%), reflecting that
introducing retrieval randomness increases diversity without consistently boosting short-form QA accuracy.
Efficiency-focused generator-based systems such as xRAG exhibit mixed results. While xRAG achieves 10–29%
improvements over raw LLM baselines on datasets such as NQ and TriviaQA, its gains over retrieval baselines are
marginal or occasionally negative. This suggests that while resource-efficient designs are promising for scaling RAG
systems, further optimization is needed to maintain competitive factual accuracy in short-form tasks.
Finally, robustness-oriented frameworks such as RAAT demonstrate strong performance, with a 116% improvement
from the raw baseline and over 27% gain compared to retrieval on RAG-Bench—a robustness-focused variant of NQ,
WebQ, and TriviaQA. Although evaluated under challenging retrieval noise conditions, RAAT’s results suggest that
robustness-driven retrieval strategies can effectively complement factual QA objectives.
Overall, retrieval- and generation-enhanced frameworks deliver substantial relative gains in short-form QA, while
hybrid and efficiency-focused approaches offer promising but variable results depending on dataset and retrieval
complexity. These findings underscore the critical role of retrieval optimization and generation-adaptive strategies in
advancing retrieval-augmented short-form question answering.
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 15
5.2 Comparative Analysis of Framework Performance on Multi-Hop QA
A comparative evaluation of various Retrieval-Augmented Generation (RAG) frameworks reveals distinct patterns in
their ability to enhance multi-hop question answering, assessed through improvements over both raw large language
models (LLMs) and standard retrieval-augmented baselines. Similar to the previous section, this analysis focuses on
relative gains rather than absolute scores to normalize for architectural and experimental variations. The results,
summarized in Table 2, enable a consistent comparison of framework contributions across diverse multi-hop QA
settings.
Among retrieval-based RAG systems, models such as RQ-RAG, RankRAG, LQR, and LongRAG demonstrate substantial
relative gains. Notably, RQ-RAG achieves over an 800% improvement from its raw LLM baseline on HotpotQA [ 81],
and a 275% improvement over standard retrieval, highlighting the effectiveness of sophisticated query decomposition
techniques in multi-hop settings. Similarly, LQR achieves a 292% improvement from the raw baseline and an 84%
improvement over retrieval in the MuSiQue dataset [ 67], suggesting that intelligent retrieval-ranking substantially
boosts multi-hop reasoning. LongRAG also exhibits strong performance, improving by over 50% from the raw LLM
baseline on HotpotQA, further emphasizing the value of extended retrieval for complex question answering. These
patterns collectively affirm that optimizing retrieval quality remains a dominant driver of performance gains in multi-hop
RAG applications.
Generator-based RAG frameworks, including R2AG, INFO-RAG, and xRAG, display more varied relative improve-
ments. R2AG shows consistent strong gains, improving by over 300% relative to the baseline on HotpotQA, demonstrating
the benefits of tightly integrating retrieval signals into the generation process. In contrast, INFO-RAG exhibits more
modest improvements, with relative gains around 16–35% across different backbones and datasets, suggesting that while
generator-side augmentations enhance output faithfulness, their standalone effect may be limited without concurrent
retrieval refinement. xRAG, while improving from raw baselines by approximately 20–26%, shows negative or marginal
gains compared to the retrieval baseline in some settings, indicating that extreme context compression, although
efficient, may compromise the model’s ability to utilize retrieved evidence effectively for complex multi-hop reasoning.
Hybrid RAG frameworks, such as DRAGIN, FLARE, GenGround, and Stochastic RAG, present a diverse range of
outcomes. DRAGIN frameworks achieve moderate improvements, typically ranging between 22–44% over raw LLMs
and 14–34% over retrieval baselines, reflecting the incremental gains from dynamically adapting retrieval to evolving
information needs. FLAREdirect stands out, achieving a 62% improvement from the raw LLM and a 22% improvement
over standard retrieval on 2Wiki [ 23], suggesting that model-guided active retrieval significantly strengthens multi-hop
evidence gathering. GenGround reports relatively smaller improvements (13–36% from the baseline) but is evaluated
against already-strong baselines, which partially accounts for the more conservative gains. Stochastic RAG frameworks
offer consistent yet modest gains (6%), indicating that introducing randomness into retrieval can modestly diversify and
enhance evidence coverage without destabilizing performance.
Overall, retrieval-based RAG frameworks demonstrate the most consistent and substantial improvements across
multi-hop QA tasks, particularly when retrieval quality, ranking, and query decomposition are optimized. Generator-
based adaptations, while beneficial in specific cases, often require complementary retrieval-side enhancements to
realize their full potential. Hybrid frameworks offer promising but more variable results, underscoring the challenge
of harmonizing retrieval and generation strategies dynamically. These findings highlight retrieval optimization as a
critical lever for advancing complex reasoning capabilities in RAG systems.
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 16
5.3 Comparative Robustness Analysis: Framework Gains Over Retrieval-Only Baselines
To assess robustness in Retrieval-Augmented Generation (RAG) systems, we report incremental improvements each
framework achieves over its retrieval-augmented LLM baseline. This isolates the added value of mechanisms such as
critique, reranking, and filtering, independent of the baseline retrieval gain. Evaluations span multiple datasets and
focus on gains in precision, recall, and FactScore. By standardizing on relative improvements, the analysis enables fair
comparisons across models with differing backbone architectures. A summary of these results is provided in Table 3.
Among hybrid systems, the most substantial gains in factual consistency are observed. Self-CRAG yields the highest
FactScore improvement—+0.456 on the Biography dataset [ 45]—significantly surpassing other frameworks, most of
which report≤0.05 gains. The multi-sentence compositional nature of the Biography task likely benefits from Self-
CRAG’s feedback-based reranking and correction loop, which aligns generation with retrieved evidence. Comparable
improvements are evident with Self-RAG and CRAG, reporting +0.372 and +0.252 gains on the same dataset, underscoring
the importance of evidence-aware generation refinement. On 2Wiki, Flare-Direct improves both precision and recall
by +21.6%, while Flare-Instruct—despite using the same retrieval backbone—offers negligible gains, illustrating how
prompt design alone can meaningfully impact robustness in multi-hop settings. In contrast, Stochastic RAG shows only
marginal FactScore gains ( ≤+0.008) on Fever, suggesting that entropy-driven retrieval without subsequent verification
may be insufficient to ensure factual reliability.
Generator-based systems present more variable, task-dependent performance. SELF-RAG, evaluated on ASQA [ 60],
achieves sizable improvements in precision (+22–30%) and recall (+16–19%), though its FactScore gains remain modest
(+0.03–0.04), implying improved evidence usage without equivalent advances in factual accuracy. DRAGIN similarly
improves precision and recall by +9–22% on HotPotQA, leveraging entropy-based token-level triggers suited for multi-
hop reasoning. However, lacking reported FactScore, its contribution to factual consistency remains indeterminate.
Other generator-oriented systems, including GenRT and Rich Answer Encoding, achieve smaller recall gains ( ≤+0.1) on
datasets such as TriviaQA, KILT-WoW [ 49], and MSMARCO [ 3]. These modest improvements suggest better document
selection but limited post-retrieval validation, constraining their robustness impact.
Retriever-based systems exhibit consistent yet comparatively modest gains. Re2G reports +17.8% precision and
+15.9% recall on TriviaQA, reflecting the benefits of retrieval-aware prompt optimization. FILCO, by contrast, improves
precision by +3.25% on Fever but fails to enhance recall or FactScore, indicating that filtering irrelevant context improves
selectivity, but without downstream verification, its robustness contribution is limited. Not all frameworks report all
three metrics across datasets; while relative improvement facilitates normalization, incomplete coverage—particularly
of FactScore—may obscure the full extent of a system’s capabilities.
In sum, Self-CRAG on Biography delivers the strongest FactScore gain (+0.456), while SELF-RAG on ASQA achieves
the best precision (+29.56%) and recall (+18.81%) improvements. Flare-Direct, outperforming Flare-Instruct by over 20%
on 2Wiki, highlights the sensitivity of robustness to prompt design. At the lower end, Stochastic RAG on FEVER [ 65]
records the smallest impact ( ≤+0.008 FactScore), reinforcing the necessity of combining retrieval strategies with
downstream verification to enhance factual fidelity.
Collectively, these findings affirm that retrieval alone is insufficient for robust generation. The most effective
frameworks tightly couple retrieval, generation, and verification in iterative loops, ensuring that generation is guided
by critique and alignment rather than treated as a terminal step.
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 17
Table 3. Comparative Robustness Analysis of RAG Frameworks Across Architectures. Relative improvements in precision,
recall, and FactScore over retrieval-augmented baselines across multiple datasets. A dash (–) denotes missing values in the original
paper.
Taxonomy Framework Backbone Dataset Precision Recall FactScore
Retriever-based RAGRe2G KGI0 NQ 0.096984 0.074569 –
Re2G KGI1 TriviaQA 0.177981 0.159062 –
Re2G KGI2 Fever 0.120986 0.073732 –
FILCO RAG Fever 3.25 – –
Generation-based RAGSELF-RAG LLaMA2-7B ASQA 22.06897 15.95 0.041026
SELF-RAG LLaMA2-7B ASQA 29.56522 18.80556 0.034839
Rich Answer Encoding RAG MSMARCO – 0.086957 –
Rich Answer Encoding RAG KILT-WoW – 0.107293 –
DRAGIN LLaMA2-13B HotPotQA 0.185934 0.09893 –
DRAGIN VICUNA-13B HotPotQA 0.222447 0.105114 –
GenRT RAG NQ – 0.023232 –
GenRT RAG TriviaQA – 0.026239 –
Hybrid RAGCRAG LLaMA2-7B Biography – – 0.251689
Self-CRAG LLaMA2-7B Biography – – 0.456081
Flare-Instruct GPT-3.5 2Wiki 0.010288 0.019417 –
Flare-Direct GPT-3.5 2Wiki 0.216049 0.215534 –
Stochastic RAG FiD-Light (T5-Base) Fever – – 0.008685
Stochastic RAG FiD-Light (T5-XL) Fever – – 0.00355
5.4 Ablation Studies
Ablation studies serve as a crucial methodological lens for disentangling the contributions of individual components
in Retrieval-Augmented Generation (RAG) frameworks. Across the surveyed literature, these studies primarily target
retrieval triggers, filtering layers, reranking strategies, compression modules, and corrective mechanisms, offering
empirical insights into performance, efficiency, and robustness.
Adaptive Retrieval and Query Reformulation. Frameworks such as TA-ARE, FLARE, and IM-RAG demonstrate
that retrieval adaptivity is central to long-form and multi-hop reasoning. Ablating dynamic query triggers (e.g., forward-
looking or self-reflective prompts) consistently results in degraded factual accuracy and increased hallucinations,
confirming the value of retrieval-awareness throughout the generation process.
Filtering, Reranking, and Evidence Quality. Systems like SEER, CRAG, and Re2G show that context filtering,
reranking, and correction layers significantly influence downstream performance. Ablations reveal that removing
context evaluators or decomposing mechanisms leads to verbosity and reduced grounding fidelity. Notably, reranking-
truncation co-designs (e.g., in GenRT and ToolRerank) outperform static top- 𝑘approaches by improving answer
faithfulness and retrieval precision.
Compression and Efficiency Trade-offs. FiD-Light and RAGCache demonstrate that passage compression and
caching can substantially reduce latency without compromising accuracy. Ablating vector sparsity or caching mecha-
nisms (e.g., speculative pipelining or prefix-aware replacement) increases inference time up to 4 ×, underscoring the
operational significance of architectural optimization in production RAG systems.
Robustness and Security. Studies like BadRAG and TrojanRAG emphasize that security-focused ablations reveal
novel vulnerabilities. Even lightweight retrieval poisoning or trigger crafting can steer model outputs, while mitigation
strategies (e.g., summarization, distance thresholds) offer partial resilience but require further study.
Synthesis. Ablation studies consistently reinforce that high-performing RAG frameworks are modular, with com-
plementary retrieval, filtering, and generation components. Performance degradation in ablation settings not only
validates novel modules but also guides design toward more interpretable, efficient, and secure RAG pipelines.
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 18
6 Evaluation and Benchmarking of RAG Systems
Retrieval-Augmented Generation (RAG) systems introduce unique challenges for evaluation due to their hybrid
architecture combining a retriever and a generator. Accurate evaluation demands assessing multiple interdependent
components, including retrieval relevance, faithfulness of generated responses, and overall answer utility. In this section,
we synthesize recent advancements in automated evaluation frameworks, retrieval quality assessment techniques, and
benchmark construction to provide a comprehensive overview of evaluation practices in RAG systems.
6.1 Evaluation Dimensions
The core dimensions [55] used to evaluate RAG systems include:
(1)Context Relevance: Measures how pertinent the retrieved documents are to the input query.
(2)Answer Faithfulness: Assesses whether the generated output remains grounded in the retrieved evidence.
(3)Answer Relevance: Evaluates whether the output adequately addresses the user query.
These dimensions are interdependent: poor context relevance often cascades into reduced faithfulness and answer
relevance, underscoring the need for joint evaluation. Frameworks such as ARES and RAGAS have formalized these
dimensions, incorporating both automated judgment and reference-free evaluation.
6.2 Automated Evaluation Frameworks
ARES [ 55] introduces an LLM-based judge system that uses few-shot prompted language models to generate synthetic
datasets. These judges are trained on three classification tasks corresponding to the core dimensions and use prediction-
powered inference (PPI) to align model-based scoring with human judgment. ARES shows significant improvements in
accuracy and annotation efficiency, outperforming RAGAS [17] by up to 59.3 percentage points in context relevance.
RAGAS employs a modular framework that decomposes generated answers into atomic factual statements, then
evaluates each against the retrieved context using LLMs. This structure provides high-resolution feedback, revealing
which parts of an answer are hallucinated.
These frameworks automate the evaluation of faithfulness, grounding, and contextual relevance—enabling scalable,
reference-free analysis of RAG performance.
6.3 Evaluating Retrieval Quality
eRAG [ 56] challenges traditional relevance label techniques by applying the RAG generator to each retrieved document
individually. The performance of each document, assessed via downstream task metrics, serves as a relevance label.
This method provides a retrieval-aware, document-level granularity and has shown significantly improved correlation
with actual RAG performance.
INFO-RAG introduces an unsupervised training paradigm that improves the LLM’s ability to refine retrieved
information under three scenarios: redundant, noisy, or insufficient context. By viewing the LLM as an “information
refiner,” it enables the model to extract relevant content, reject misinformation, and infer missing details—enhancing
retrieval robustness without supervised relevance labels.
uRAG proposes a unified retrieval system that serves multiple RAG models across diverse downstream tasks. It
introduces a shared reranker trained on feedback signals (e.g., EM, accuracy) from various black-box LLMs, treating
each LLM as a user of the search engine. uRAG’s training protocol enables evaluation and optimization of retrieval
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 19
based on downstream task performance, offering retrieval diagnostics grounded in actual utility rather than surface
similarity.
6.4 Benchmarking RAG Capabilities
As RAG systems mature, a growing suite of benchmarks has emerged to evaluate them across dimensions like robustness,
factuality, adaptivity, and domain sensitivity. These benchmarks not only reflect the evolving needs of real-world RAG
deployments but also shape future directions by surfacing recurrent failure modes and task-specific limitations.
Robustness to retrieval noise is a core requirement in operational RAG systems. RGB [ 7] evaluates four fundamental
capacities—noise robustness, negative rejection, information integration, and counterfactual resistance—revealing
consistent weaknesses in LLMs when handling distracting or misleading context. Complementing this, RAG-Bench [ 18]
introduces a noise-centric benchmark simulating three retrieval corruption types—relevant-but-incomplete, irrelevant,
and counterfactual—and applies adaptive adversarial training to improve model tolerance. These benchmarks enable
fine-grained analysis of how retrieval perturbations degrade end-task performance and inform robust retrieval-policy
design.
Faithfulness and hallucination detection benchmarks have taken center stage in evaluating generation quality.
RAGTruth [ 46] provides nearly 18,000 annotated examples from QA, summarization, and data-to-text generation,
offering both response- and span-level hallucination labels across four types: subtle vs. evident, and conflict vs. baseless
information. Uniquely, it supports training hallucination detectors and benchmarking span-level detection precision
and recall—tasks not addressed by coarse-grained metrics. This makes it foundational for measuring factual integrity in
RAG outputs.
Reasoning and retrieval chaining are central to multi-hop question answering, where evidence spans multiple
documents. MultiHop-RAG [ 63] targets this challenge through linked question-answer pairs, bridge entities, and explicit
multi-hop query types, enabling systematic assessment of retrieval chaining, evidence linking, and document-level
reasoning—all key bottlenecks in complex RAG workflows.
Adaptive retrieval and necessity estimation are benchmarked in RetrievalQA [ 86], which mixes queries requiring
external retrieval with those answerable via the base LLM alone. This design tests whether models can intelligently
toggle retrieval based on query uncertainty, supporting the development of resource-efficient, retrieval-aware systems
that avoid introducing unnecessary context.
Domain-specific evaluation is exemplified by MIRAGE [ 48], a benchmark tailored to medical RAG. It contains 7,663
questions sourced from five clinical and biomedical QA datasets and incorporates real-world evaluation constraints:
zero-shot generalization, multiple-choice formats, retrieval necessity assessment, and question-only retrieval. This
multi-faceted setup tests reliability under high-stakes conditions where factual errors can be consequential.
Cross-corpus and federated retrieval are explored in FeB4RAG [ 68], a benchmark constructed from 16 BEIR sub-
collections. It evaluates federated retrieval through 790 conversational queries with LLM-graded relevance judgments
and quantifies the impact of resource selection and result merging strategies. This benchmark surfaces key risks in
multi-source RAG pipelines, especially retrieval inconsistency and hallucination amplification due to poor corpus
coordination.
Evaluation infrastructure and reproducibility are addressed by BERGEN [ 52], a benchmarking library designed
to unify assessment across RAG components. It offers modular templates for measuring retrieval precision, generation
faithfulness, and their interplay across datasets and model configurations. BERGEN facilitates consistent and extensible
RAG benchmarking in both academic and applied settings.
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 20
Table 4. Emerging Benchmarks for Evaluating Retrieval-Augmented Generation (RAG) Systems. This table summarizes recent
benchmarks developed to assess key aspects of RAG systems, including robustness, multi-hop reasoning, medical-domain adaptation,
and federated retrieval. These benchmarks differ in evaluation granularity—ranging from query-level to document-level—and employ
varied annotation methods such as manual labeling, programmatic perturbation, and LLM-based scoring. Distinctive features, such as
noise stress-testing (RGB), zero-shot medical QA (MIRAGE), and federated source merging (FeB4RAG), support targeted evaluations
of both retriever components and full RAG pipelines.
Benchmark Evaluation Focus Granularity Annotation
TypeUnique Features Evaluation Tar-
get
RGB Robustness (noise, inte-
gration, hallucination)Query-context
pairNone Stress tests for noise,
contradiction, and multi-
source fusionFull pipeline
MultiHop-RAG Multi-hop reasoning
and retrieval chainingDocument-level Manual + derived Linked multi-hop queries
and bridge-entity chainingFull pipeline
RAGTruth Hallucination detec-
tion and factuality
evaluationResponse-level
(yes/no), span-
level (exact)Human-labeled 18,000+ examples, 4 hallu-
cination types, span-level
F1Generator
MIRAGE Medical domain QA
under real-world con-
straintsQuery-level Dataset-native Zero-shot, multi-choice,
question-only retrieval
(MEDRAG)Full pipeline
FeB4RAG Federated retrieval
evaluationDocument + re-
sourceLLM-labeled Measures retrieval + merg-
ing across 16 BEIR sourcesRetriever
RetrievalQA Adaptive retrieval ne-
cessity detectionQuery-level Derived Queries with and without
need for retrievalRetriever
RAG-Bench Retrieval robustness to
noiseQuery-level Programmatic Irrelevant, incomplete, and
counterfactual retrieval
noiseFull pipeline
BERGEN Retrieval, generation,
and joint evaluationQuery-context
and document-
levelConfigurable
(task-dependent)Unified benchmarking li-
brary across datasets and
modelsFull pipeline
This section outlines the rapidly evolving landscape of RAG evaluation and benchmarking. Future RAG development
hinges not only on improving generation quality but also on designing principled, scalable, and interpretable evaluation
strategies that reflect real-world usage and complexities. To advance the field meaningfully, the community must
prioritize the creation of standardized, efficient, and adaptive evaluation protocols that can serve both research and
production contexts.
7 Future Directions
As Retrieval-Augmented Generation (RAG) systems continue to evolve, a number of unresolved challenges remain that
limit their deployment in dynamic, open-ended, and high-stakes applications. These challenges span retrieval efficiency,
semantic misalignment, hallucination control, generalization, and trust. Based on the synthesis of contemporary research
gaps, we outline five interrelated future directions that represent promising trajectories for advancing the field.
7.1 Retrieval Adaptivity and Semantic Alignment
Current RAG architectures often rely on static retrieval policies and fixed embedding transformations, limiting their
adaptability to complex or evolving user queries. Future systems must support dynamically calibrated retrieval strategies
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 21
that adjust depth, modality, and source selection in response to task difficulty and contextual cues. This calls for co-
optimized retriever–generator pipelines that leverage reinforcement signals, uncertainty estimates, or semantic control
layers to align evidence retrieval with generative intent in real time.
7.2 Robustness under Noise and Adversarial Conditions
Despite recent advances in noise filtering and adversarial training, RAG systems remain vulnerable to retrieval per-
turbations, misleading content, and corpus-level poisoning attacks. Future work should move toward retrieval-aware
adversarial defenses that incorporate noise-aware loss functions, retrieval-type-specific regularization, and semantic
provenance filtering. This includes evaluation protocols that stress-test systems against contextually plausible yet
misleading passages and group-triggered semantic attacks, as exemplified by recent backdoor threat models.
7.3 Multi-Hop Reasoning and Structured Compositionality
Many knowledge-intensive tasks require aggregating evidence across multiple retrieval steps and reasoning over entity
or schema-level structures. Current models exhibit limited capacity for compositional inference or procedural synthesis.
Future RAG systems should support multi-turn retrieval–generation loops, structured subgoal decomposition, and
graph-augmented reasoning pipelines that maintain discourse coherence and entity consistency across long-range
dependencies.
7.4 Cross-Domain Generalization and Temporal Adaptivity
RAG performance often degrades in the face of domain shifts, novel schema, or temporal drift. Addressing this
will require pretraining retrieval modules on diverse proxy tasks, developing meta-retrievers capable of adapting to
unseen query distributions, and incorporating recency-aware document scoring. Additionally, the design of temporally
evolving benchmarks and evaluation suites will be necessary to assess the robustness of RAG systems under realistic,
time-sensitive knowledge conditions.
7.5 Explainability, Personalization, and Trust Calibration
As RAG systems are increasingly integrated into user-facing applications, demands for interpretability, personalization,
and secure behavior intensify. Future architectures should expose transparent interfaces for explaining retrieval
decisions and generation provenance, while supporting privacy-preserving personalization through user-clustered
retrieval, memory-efficient modeling, or differential privacy mechanisms. Furthermore, integrating retrieval calibration
signals—such as factual salience, source trustworthiness, or hallucination risk—can enhance user trust and system
accountability.
References
[1]Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through
Self-Reflection. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=hSyW5go0v8
[2]Orlando Ayala and Patrice Bechard. 2024. Reducing hallucination in structured outputs via Retrieval-Augmented Generation. In Proceedings
of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6:
Industry Track) , Yi Yang, Aida Davani, Avi Sil, and Anoop Kumar (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 228–238.
doi:10.18653/v1/2024.naacl-industry.19
[3]Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri
Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. MS MARCO: A Human Generated MAchine Reading
COmprehension Dataset. arXiv:1611.09268 [cs.CL] https://arxiv.org/abs/1611.09268
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 22
[4]Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. 2024. Seven Failure Points When Engineering a
Retrieval Augmented Generation System. In Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for
AI(Lisbon, Portugal) (CAIN ’24) . Association for Computing Machinery, New York, NY, USA, 194–199. doi:10.1145/3644815.3644945
[5]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey
Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam
McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the 34th International
Conference on Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS ’20) . Curran Associates Inc., Red Hook, NY, USA, Article 159,
25 pages.
[6]Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. RQ-RAG: Learning to Refine Queries for Retrieval
Augmented Generation. In First Conference on Language Modeling . https://openreview.net/forum?id=tzE7VqsaJ4
[7]Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented generation. In Proceedings
of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and
Fourteenth Symposium on Educational Advances in Artificial Intelligence (AAAI’24/IAAI’24/EAAI’24) . AAAI Press, Article 1980, 9 pages. doi:10.1609/
aaai.v38i16.29728
[8]Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Haodong Zhao, Ping Yi, Zhuosheng Zhang, and Gongshen Liu. 2024. TrojanRAG:
Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models. https://openreview.net/forum?id=RfYD6v829Y
[9]Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2023. Lift yourself up: retrieval-augmented text generation with
self-memory. In Proceedings of the 37th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS ’23) . Curran
Associates Inc., Red Hook, NY, USA, Article 1899, 20 pages.
[10] Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao. 2024. xRAG: Extreme Context
Compression for Retrieval-augmented Generation with One Token. In The Thirty-eighth Annual Conference on Neural Information Processing Systems .
https://openreview.net/forum?id=6pTlXqrO0p
[11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-
30-vicuna/
[12] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have Solved
Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv:1803.05457 [cs.AI] https://arxiv.org/abs/1803.05457
[13] Gordon V. Cormack, Charles L A Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning
methods. In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval (Boston, MA, USA)
(SIGIR ’09) . Association for Computing Machinery, New York, NY, USA, 758–759. doi:10.1145/1571941.1572114
[14] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri.
2024. The Power of Noise: Redefining Retrieval for RAG Systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and
Development in Information Retrieval (Washington DC, USA) (SIGIR ’24) . Association for Computing Machinery, New York, NY, USA, 719–729.
doi:10.1145/3626772.3657834
[15] Nguyen Nam Doan, Aki Härmä, Remzi Celebi, and Valeria Gottardo. 2024. A Hybrid Retrieval Approach for Advancing Retrieval-Augmented
Generation Systems. In Proceedings of the 7th International Conference on Natural Language and Speech Processing (ICNLSP 2024) , Mourad Abbas and
Abed Alhakim Freihat (Eds.). Association for Computational Linguistics, Trento, 397–409. https://aclanthology.org/2024.icnlsp-1.41/
[16] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa
Ness, and Jonathan Larson. 2025. From Local to Global: A Graph RAG Approach to Query-Focused Summarization. arXiv:2404.16130 [cs.CL]
https://arxiv.org/abs/2404.16130
[17] Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. RAGAs: Automated Evaluation of Retrieval Augmented Generation. In
Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations , Nikolaos Aletras
and Orphee De Clercq (Eds.). Association for Computational Linguistics, St. Julians, Malta, 150–158. https://aclanthology.org/2024.eacl-demo.16/
[18] Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu. 2024. Enhancing Noise Robustness of Retrieval-Augmented Language
Models with Adaptive Adversarial Training. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 10028–10039.
doi:10.18653/v1/2024.acl-long.540
[19] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-
augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 2 (2023), 1.
[20] Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Naik, Pengshan Cai, and Alfio Gliozzo. 2022. Re2G: Retrieve, Rerank,
Generate. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies , Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle,
United States, 2701–2715. doi:10.18653/v1/2022.naacl-main.194
[21] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan
Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev,
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 23
Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie
Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne
Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu,
Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily
Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind
Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay
Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie
Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik
Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal
Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan,
Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,
Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan,
Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy,
Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He,
Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari,
Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini,
Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng
Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin
Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor
Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish
Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang,
Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song,
Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava,
Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei
Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu,
Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury,
Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd,
Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido,
Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu,
Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu,
Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil,
Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei
Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella
Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid
Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan
Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice
Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian
Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U,
Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun
Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron
Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso,
Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik
Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal,
Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich
Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan
Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad
Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan,
Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun
Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay,
Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal,
Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer
Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler,
Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez,
Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang,
Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen,
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 24
Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi
He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The Llama 3 Herd of Models.
arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783
[22] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-Retriever: Retrieval-
Augmented Generation for Textual Graph Understanding and Question Answering. In The Thirty-eighth Annual Conference on Neural Information
Processing Systems . https://openreview.net/forum?id=MPJ3oXtTZl
[23] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing A Multi-hop QA Dataset for Comprehensive Evaluation
of Reasoning Steps. In Proceedings of the 28th International Conference on Computational Linguistics , Donia Scott, Nuria Bel, and Chengqing Zong
(Eds.). International Committee on Computational Linguistics, Barcelona, Spain (Online), 6609–6625. doi:10.18653/v1/2020.coling-main.580
[24] Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023. FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation.
InProceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (Taipei, Taiwan) (SIGIR ’23) .
Association for Computing Machinery, New York, NY, USA, 1437–1447. doi:10.1145/3539618.3591687
[25] Jie Huang, Mo Wang, Yunpeng Cui, Juan Liu, Li Chen, Ting Wang, Huan Li, and Jinming Wu. 2024. Layered Query Retrieval: An Adaptive
Framework for Retrieval-Augmented Generation in Complex Question Answering for Large Language Models. Applied Sciences 14, 23 (2024).
doi:10.3390/app142311014
[26] Wenyu Huang, Mirella Lapata, Pavlos Vougiouklis, Nikos Papasarantopoulos, and Jeff Pan. 2023. Retrieval Augmented Generation with Rich Answer
Encoding. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of
the Association for Computational Linguistics (Volume 1: Long Papers) , Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti,
and Adila Alfa Krisnadhi (Eds.). Association for Computational Linguistics, Nusa Dua, Bali, 1012–1025. doi:10.18653/v1/2023.ijcnlp-main.65
[27] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. In
Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , Paola Merlo, Jorg Tiedemann,
and Reut Tsarfaty (Eds.). Association for Computational Linguistics, Online, 874–880. doi:10.18653/v1/2021.eacl-main.74
[28] Jisoo Jang and Wen-Syan Li. 2024. AU-RAG: Agent-based Universal Retrieval Augmented Generation. In Proceedings of the 2024 Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (Tokyo, Japan) (SIGIR-AP 2024) . Association
for Computing Machinery, New York, NY, USA, 2–11. doi:10.1145/3673791.3698416
[29] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang,
Timothée Lacroix, and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL] https://arxiv.org/abs/2310.06825
[30] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las
Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne
Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang,
Timothée Lacroix, and William El Sayed. 2024. Mixtral of Experts. arXiv:2401.04088 [cs.LG] https://arxiv.org/abs/2401.04088
[31] Ziyan Jiang, Xueguang Ma, and Wenhu Chen. 2024. LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs.
arXiv:2406.15319 [cs.CL] https://arxiv.org/abs/2406.15319
[32] Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active
Retrieval Augmented Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , Houda Bouamor, Juan
Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 7969–7992. doi:10.18653/v1/2023.emnlp-main.495
[33] Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, and Xin Jin. 2024. RAGCache: Efficient Knowledge Caching for Retrieval-
Augmented Generation. arXiv:2404.12457 [cs.DC] https://arxiv.org/abs/2404.12457
[34] Hailey Joren, Jianyi Zhang, Chun-Sung Ferng, Da-Cheng Juan, Ankur Taly, and Cyrus Rashtchian. 2025. Sufficient Context: A New Lens on Retrieval
Augmented Generation Systems. In The Thirteenth International Conference on Learning Representations . https://openreview.net/forum?id=Jjr2Odj8DJ
[35] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading
Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Regina Barzilay
and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 1601–1611. doi:10.18653/v1/P17-1147
[36] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval
for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , Bonnie
Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 6769–6781. doi:10.18653/v1/2020.emnlp-
main.550
[37] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob
Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.
2019. Natural Questions: A Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics 7 (2019),
452–466. doi:10.1162/tacl_a_00276
[38] Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. 2022. Generative Multi-hop Retrieval. In Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu
Dhabi, United Arab Emirates, 1417–1436. doi:10.18653/v1/2022.emnlp-main.92
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 25
[39] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics , Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for
Computational Linguistics, Online, 7871–7880. doi:10.18653/v1/2020.acl-main.703
[40] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, et al .2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems 33
(2020), 9459–9474.
[41] Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Retrieval Augmented Generation or Long-Context LLMs? A
Comprehensive Study and Hybrid Approach. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry
Track , Franck Dernoncourt, Daniel Preoţiuc-Pietro, and Anastasia Shimorina (Eds.). Association for Computational Linguistics, Miami, Florida, US,
881–893. doi:10.18653/v1/2024.emnlp-industry.66
[42] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When Not to Trust Language Models:
Investigating Effectiveness of Parametric and Non-Parametric Memories. In Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics,
Toronto, Canada, 9802–9822. doi:10.18653/v1/2023.acl-long.546
[43] Nicholas Matsumoto, Jay Moran, Hyunjun Choi, Miguel E Hernandez, Mythreye Venkatesan, Paul Wang, and Jason H Moore. 2024. KRAGEN: a
knowledge graph-enhanced RAG framework for biomedical problem solving using large language models. Bioinformatics 40, 6 (2024), btae353.
[44] Chuan Meng, Negar Arabzadeh, Arian Askari, Mohammad Aliannejadi, and Maarten de Rijke. 2024. Ranked List Truncation for Large Language
Model-based Re-Ranking. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval
(Washington DC, USA) (SIGIR ’24) . Association for Computing Machinery, New York, NY, USA, 141–151. doi:10.1145/3626772.3657864
[45] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. In Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore,
12076–12100. doi:10.18653/v1/2023.emnlp-main.741
[46] Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, KaShun Shum, Randy Zhong, Juntong Song, and Tong Zhang. 2024. RAGTruth: A Hallucination
Corpus for Developing Trustworthy Retrieval-Augmented Language Models. In Proceedings of the 62nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,
Bangkok, Thailand, 10862–10878. doi:10.18653/v1/2024.acl-long.585
[47] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff
Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa
Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson,
Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester
Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien
Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,
Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha
Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse
Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton,
Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino
Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan
Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk,
Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim
Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney,
Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin,
Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,
Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano,
Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power,
Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri
Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam,
Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama,
Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe,
Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ
Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 26
Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan
Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4 Technical
Report. arXiv:2303.08774 [cs.CL] https://arxiv.org/abs/2303.08774
[48] Shintaro Ozaki, Yuta Kato, Siyuan Feng, Masayo Tomita, Kazuki Hayashi, Wataru Hashimoto, Ryoma Obara, Masafumi Oyamada, Katsuhiko Hayashi,
Hidetaka Kamigaito, and Taro Watanabe. 2025. Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the
Medical Domain. arXiv:2412.20309 [cs.CL] https://arxiv.org/abs/2412.20309
[49] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin,
Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks. In
Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies ,
Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and
Yichao Zhou (Eds.). Association for Computational Linguistics, Online, 2523–2544. doi:10.18653/v1/2021.naacl-main.200
[50] Zackary Rackauckas. 2024. Rag-Fusion: A New Take on Retrieval Augmented Generation. International Journal on Natural Language Computing 13,
1 (Feb. 2024), 37–47. doi:10.5121/ijnlc.2024.13103
[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 1–67.
[52] David Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Stéphane Clinchant, and Vassilina Nikoulina. 2024. BERGEN:
A Benchmarking Library for Retrieval-Augmented Generation. In Findings of the Association for Computational Linguistics: EMNLP 2024 , Yaser
Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 7640–7663. doi:10.18653/v1/
2024.findings-emnlp.449
[53] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou,
Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese,
Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom
Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer,
Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc,
Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et al. 2024. Gemini 1.5:
Unlocking multimodal understanding across millions of tokens of context. CoRR abs/2403.05530 (2024). https://doi.org/10.48550/arXiv.2403.05530
[54] Stephen Robertson, Hugo Zaragoza, et al .2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends ®in Information
Retrieval 3, 4 (2009), 333–389.
[55] Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2024. ARES: An Automated Evaluation Framework for Retrieval-Augmented
Generation Systems. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies (Volume 1: Long Papers) , Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics,
Mexico City, Mexico, 338–354. doi:10.18653/v1/2024.naacl-long.20
[56] Alireza Salemi and Hamed Zamani. 2024. Evaluating Retrieval Quality in Retrieval-Augmented Generation. In Proceedings of the 47th International
ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR ’24) . Association for Computing
Machinery, New York, NY, USA, 2395–2400. doi:10.1145/3626772.3657957
[57] Alireza Salemi and Hamed Zamani. 2024. Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language
Models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA)
(SIGIR ’24) . Association for Computing Machinery, New York, NY, USA, 741–751. doi:10.1145/3626772.3657733
[58] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024. REPLUG:
Retrieval-Augmented Black-Box Language Models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association
for Computational Linguistics, Mexico City, Mexico, 8371–8384. doi:10.18653/v1/2024.naacl-long.463
[59] Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, and Zhaochun Ren. 2024. Generate-then-Ground in Retrieval-
Augmented Generation for Multi-hop Question Answering. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand,
7339–7353. doi:10.18653/v1/2024.acl-long.397
[60] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid Questions Meet Long-Form Answers. In Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing , Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for
Computational Linguistics, Abu Dhabi, United Arab Emirates, 8273–8288. doi:10.18653/v1/2022.emnlp-main.566
[61] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time
Information Needs of Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 12991–13013.
doi:10.18653/v1/2024.acl-long.702
[62] Viju Sudhi, Sinchana Ramakanth Bhat, Max Rudat, and Roman Teucher. 2024. RAG-Ex: A Generic Framework for Explaining Retrieval Augmented
Generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC,
USA) (SIGIR ’24) . Association for Computing Machinery, New York, NY, USA, 2776–2780. doi:10.1145/3626772.3657660
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 27
[63] Yixuan Tang and Yi Yang. 2024. MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries. In First Conference on
Language Modeling . https://openreview.net/forum?id=t4eB3zYWBK
[64] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex
Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan,
Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan,
George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James
Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine
Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai
Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona
Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,
Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed,
Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin
Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen
Kenealy. 2024. Gemma: Open Models Based on Gemini Research and Technology. arXiv:2403.08295 [cs.CL] https://arxiv.org/abs/2403.08295
[65] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset for Fact Extraction and
VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers) , Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, New Orleans,
Louisiana, 809–819. doi:10.18653/v1/N18-1074
[66] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor
Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi
Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina
Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,
Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]
https://arxiv.org/abs/2307.09288
[67] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop Questions via Single-hop Question
Composition. Transactions of the Association for Computational Linguistics 10 (2022), 539–554. doi:10.1162/tacl_a_00475
[68] Shuai Wang, Ekaterina Khramtsova, Shengyao Zhuang, and Guido Zuccon. 2024. FeB4RAG: Evaluating Federated Search in the Context of Retrieval
Augmented Generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval
(Washington DC, USA) (SIGIR ’24) . Association for Computing Machinery, New York, NY, USA, 763–773. doi:10.1145/3626772.3657853
[69] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023. Learning to filter context for retrieval-augmented
generation. arXiv preprint arXiv:2311.08377 (2023).
[70] Zheng Wang, Shu Teo, Jieer Ouyang, Yongjun Xu, and Wei Shi. 2024. M-RAG: Reinforcing Large Language Model Performance through Retrieval-
Augmented Generation with Multiple Partitions. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 1966–1978.
doi:10.18653/v1/2024.acl-long.108
[71] Zilong Wang, Zifeng Wang, Long Le, Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang,
Chen-Yu Lee, and Tomas Pfister. 2025. Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting. In The Thirteenth International
Conference on Learning Representations . https://openreview.net/forum?id=xgQfWbV6Ey
[72] Junde Wu, Jiayuan Zhu, and Yunli Qi. 2024. Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented
Generation. CoRR abs/2408.04187 (2024). https://doi.org/10.48550/arXiv.2408.04187
[73] Ran Xu, Hui Liu, Sreyashi Nag, Zhenwei Dai, Yaochen Xie, Xianfeng Tang, Chen Luo, Yang Li, Joyce C. Ho, Carl Yang, and Qi He. 2025. SimRAG:
Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains. In Proceedings of the 2025 Conference
of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) ,
Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, Albuquerque, New Mexico, 11534–11550. https:
//aclanthology.org/2025.naacl-long.575/
[74] Sheng Xu, Mike Chen, and Shuwen Chen. 2024. Enhancing Retrieval-Augmented Generation Models with Knowledge Graphs: Innovative Practices
Through a Dual-Pathway Approach. In Advanced Intelligent Computing Technology and Applications: 20th International Conference, ICIC 2024, Tianjin,
China, August 5–8, 2024, Proceedings, Part VI (Tianjin, China). Springer-Verlag, Berlin, Heidelberg, 398–409. doi:10.1007/978-981-97-5678-0_34
[75] Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, and Xueqi Cheng. 2024. List-aware Reranking-Truncation Joint Model for Search and Retrieval-
augmented Generation. In Proceedings of the ACM Web Conference 2024 (Singapore, Singapore) (WWW ’24) . Association for Computing Machinery,
New York, NY, USA, 1330–1340. doi:10.1145/3589334.3645336
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 28
[76] Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, and Jie Zhou. 2024. Unsupervised Information Refinement Training
of Large Language Models for Retrieval-Augmented Generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok,
Thailand, 133–145. doi:10.18653/v1/2024.acl-long.9
[77] Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, and Zheng Li. 2024. Retrieval-Augmented
Generation with Knowledge Graphs for Customer Service Question Answering. In Proceedings of the 47th International ACM SIGIR Conference on
Research and Development in Information Retrieval (SIGIR 2024) . ACM, 2905–2909. doi:10.1145/3626772.3661370
[78] Jiaqi Xue, Mengxin Zheng, Yebowen Hu, Fei Liu, Xun Chen, and Qian Lou. 2024. BadRAG: Identifying Vulnerabilities in Retrieval Augmented
Generation of Large Language Models. https://openreview.net/forum?id=G2p8TLuJgy
[79] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective Retrieval Augmented Generation. https://openreview.net/forum?id=
JnWJbrnaUE
[80] Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, and Yi Zhang. 2024. IM-RAG: Multi-Round Retrieval-Augmented
Generation Through Learning Inner Monologues. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in
Information Retrieval (Washington DC, USA) (SIGIR ’24) . Association for Computing Machinery, New York, NY, USA, 730–740. doi:10.1145/3626772.
3657760
[81] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A
Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing , Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium,
2369–2380. doi:10.18653/v1/D18-1259
[82] Fuda Ye, Shuangyin Li, Yongqi Zhang, and Lei Chen. 2024. R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation. In
Findings of the Association for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for
Computational Linguistics, Miami, Florida, USA, 11584–11596. doi:10.18653/v1/2024.findings-emnlp.678
[83] Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro. 2024. RankRAG: Uni-
fying Context Ranking with Retrieval-Augmented Generation in LLMs. In NeurIPS . http://papers.nips.cc/paper_files/paper/2024/hash/
db93ccb6cf392f352570dd5af0a223d3-Abstract-Conference.html
[84] Hamed Zamani and Michael Bendersky. 2024. Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization.
InProceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR
’24). Association for Computing Machinery, New York, NY, USA, 2641–2646. doi:10.1145/3626772.3657923
[85] Shenglai Zeng, Jiankun Zhang, Pengfei He, Yiding Liu, Yue Xing, Han Xu, Jie Ren, Yi Chang, Shuaiqiang Wang, Dawei Yin, and Jiliang Tang. 2024.
The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG). In Findings of the Association for Computational
Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand,
4505–4524. doi:10.18653/v1/2024.findings-acl.267
[86] Zihan Zhang, Meng Fang, and Ling Chen. 2024. RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain
Question Answering. In Findings of the Association for Computational Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).
Association for Computational Linguistics, Bangkok, Thailand, 6963–6975. doi:10.18653/v1/2024.findings-acl.415
[87] Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, and Min Zhang. 2024. SEER: Self-Aligned Evidence Extraction for
Retrieval-Augmented Generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , Yaser Al-Onaizan, Mohit
Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 3027–3041. doi:10.18653/v1/2024.emnlp-main.178
[88] Yuanhang Zheng, Peng Li, Wei Liu, Yang Liu, Jian Luan, and Bin Wang. 2024. ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool
Retrieval. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING
2024) , Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, Torino, Italia,
16263–16273. https://aclanthology.org/2024.lrec-main.1413/
[89] Kun Zhu, Xiaocheng Feng, Xiyuan Du, Yuxuan Gu, Weijiang Yu, Haotian Wang, Qianglong Chen, Zheng Chu, Jingchang Chen, and Bing Qin. 2024.
An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation. In Proceedings of the 62nd Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for
Computational Linguistics, Bangkok, Thailand, 1044–1069. doi:10.18653/v1/2024.acl-long.59
[90] Yun Zhu, Jia-Chen Gu, Caitlin Sikora, Ho Ko, Yinxiao Liu, Chu-Cheng Lin, Lei Shu, Liangchen Luo, Lei Meng, Bang Liu, and Jindong Chen. 2025.
Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection. In The Thirteenth International Conference on Learning
Representations . https://openreview.net/forum?id=HE6pJoNnFp
A Appendix
To support transparency and reproducibility, we include in the Appendix the original benchmark scores reported in the
primary publications of each RAG framework. These tables serve as the empirical source for the relative improvement
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 29
analyses presented in Sections 5. All values are cited from original papers, preserving reported metrics such as F1, EM,
Accuracy, and FactScore. Where applicable, dataset splits, backbone models, and evaluation metrics are clearly labeled
to ensure traceability.
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 30
Table 5. Reported Performance Scores for Short-Form QA Frameworks. Accuracy and Exact Match (EM) scores as reported in
the original publications of short-form RAG frameworks. These values were used to compute the normalized improvements presented
in Section 5.
Taxonomy Framework Backbone Dataset Metric Raw LLM LLM+Retrieval Framework Score
Retriever-Based RAGRQ-RAG LLaMA2-7B PopQA Acc 14.7 39.8 57.1
RQ-RAG LLaMA2-7B ARC-Challenge Acc 21.8 28.7 68.3
SimRAG LLaMA3-8B ARC-Challenge Acc – 71.08 81.4
SimRAG LLaMA3-8B SciQ EM – 20.8 57.5
SimRAG Gemma2-27B ARC-Challenge Acc – 85.75 88.65
SimRAG Gemma2-27B SciQ EM – 44.8 58.1
Re2G BART Large NQ Acc 45.22 – 51.73
Re2G BART Large TriviaQA Acc 60.99 – 76.27
FILCO LLaMA2-7B (Top-5) NQ EM – 47.6 61.8
FILCO LLaMA2-7B (Top-5) TriviaQA EM – 67.3 71.1
Generator-Based RAGSELF-RAG LLaMA2-7B PopQA Acc 14.7 38.2 54.9
SELF-RAG LLaMA2-7B TriviaQA Acc 30.5 42.5 66.4
SELF-RAG LLaMA2-7B ARC-Challenge Acc 21.8 48.0 67.4
SELF-RAG LLaMA2-13B PopQA Acc 14.7 45.7 55.8
SELF-RAG LLaMA2-13B TriviaQA Acc 38.5 47.0 69.3
xRAG Mistral-7B NQ EM 30.25 42.71 39.1
xRAG Mistral-7B TriviaQA EM 57.08 65.88 65.77
xRAG Mistral-7B WebQA EM 34.89 37.84 39.4
xRAG Mixtral-8x7B NQ EM 41.99 45.15 47.28
xRAG Mixtral-8x7B TriviaQA EM 71.1 70.34 74.14
xRAG Mixtral-8x7B WebQA EM 40.31 41.26 44.5
FiD-Light FiD+DPR TriviaQA EM 48.6 – 57.6
FiD-Light FiD+DPR NQ EM 41.9 – 53.2
R2AG LLaMA2-7B NQ Acc 0.38 – 0.693
SELF-RAG LLaMA2-7B NQ Acc 0.38 – 0.188
Hybrid RAGStochastic RAG FiD-Light (T5-Base) NQ EM – 45.6 46.2
Stochastic RAG FiD-Light (T5-Base) TriviaQA EM – 57.6 59.7
Stochastic RAG FiD-Light (T5-XL) NQ EM – 51.1 53.0
Stochastic RAG FiD-Light (T5-XL) TriviaQA EM – 63.7 64.7
CRAG LLaMA2-7B NQ Acc 0.38 – 0.397
CRAG LLaMA2-7B PopQA Acc 14.7 40.3 59.3
CRAG LLaMA2-7B ARC-Challenge Acc 21.8 46.7 54.8
Self-CRAG LLaMA2-7B PopQA Acc 14.7 40.3 61.8
Self-CRAG LLaMA2-7B ARC-Challenge Acc 21.8 46.7 67.2
TA-ARE GPT-3.5 RetrievalQA Acc 1.2 38.2 35.8
TA-ARE GPT-4 RetrievalQA Acc 2.4 46.0 46.4
TA-ARE LLaMA2-7B RetrievalQA Acc 2.0 36.0 30.7
Robustness-Based RAG RAAT LLaMA2-7B RAG-Bench (TQA/NQ/WebQ) EM 38.37 65.4 83.07
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 31
Table 6. Reported Performance Scores for Multi-Hop QA Frameworks. Raw F1 and EM scores extracted from the original
papers of multi-hop RAG systems, across datasets such as HotpotQA, 2Wiki, and MuSiQue. These scores form the basis of the
comparative analysis in Section 5.
Taxonomy Framework Backbone Dataset Metric Raw LLM LLM + Retrieval Framework Score
Retriever-BasedRQ-RAG LLaMA2-7B HotpotQA F1 6.6 16.7 62.6
RQ-RAG LLaMA2-7B 2Wiki F1 16 18.7 44.8
RQ-RAG LLaMA2-7B MuSiQue F1 3 7.4 41.7
RankRAG LLaMA3-8B HotpotQA F1 – 43.3 46.7
RankRAG LLaMA3-8B 2Wiki F1 – 27.9 36.9
RankRAG LLaMA3-70B HotpotQA F1 – 44.6 55.4
RankRAG LLaMA3-70B 2Wiki F1 – 31.9 43.9
LQR LLaMA3-8B MuSiQue F1 10.7 22.8 41.97
LQR LLaMA3-8B HotpotQA F1 22.71 46.15 69.96
LQR LLaMA3-8B 2Wiki F1 32.04 47.9 54.65
LongRAG GPT-4o HotpotQA EM 42.4 – 64.3
LongRAG Gemini-1.5-Pro HotpotQA EM 33.9 – 57.5
SEER LLaMA2-7B-Chat HotpotQA F1 0.5471 0.5826 0.604
FILCO LLaMA2-7B HotpotQA EM – 61.5 65
Generator-BasedR2AG LLaMA2-7B HotpotQA F1 8.52 – 36.05
R2AG LLaMA2-7B MuSiQue F1 2.41 – 16.87
R2AG LLaMA2-7B 2Wiki F1 6.34 – 34.52
xRAG Mistral-7B HotpotQA EM 27.02 38.79 34.05
xRAG Mixtral-8x7B HotpotQA EM 32.87 43.46 39.66
INFO-RAG LLaMA2-7B HotpotQA EM 39.4 – 46.56
INFO-RAG LLaMA2-13B HotpotQA EM 42.12 – 51.48
INFO-RAG LLaMA2-13B-chat HotpotQA EM 61.23 – 61.91
INFO-RAG LLaMA2-7B MuSiQue EM 25.95 – 30.19
INFO-RAG LLaMA2-13B MuSiQue EM 25.78 – 35.02
INFO-RAG LLaMA2-13B-chat MuSiQue EM 47.06 – 47.93
HybridDRAGIN LLaMA2-13B-chat HotpotQA F1 30.97 37.06 42.38
DRAGIN LLaMA2-13B-chat 2Wiki F1 27.21 33.64 39.31
DRAGIN LLaMA2-7B-chat HotpotQA F1 27.45 24.99 33.44
DRAGIN LLaMA2-7B-chat 2Wiki F1 22.32 25.49 29.26
DRAGIN Vicuna-13B-v1.5 HotpotQA F1 32.56 35.31 41.64
DRAGIN Vicuna-13B-v1.5 2Wiki F1 22.32 25.64 35.16
FLAREdirect GPT-3.5 2Wiki F1 36.8 48.8 59.7
FLAREinstruct GPT-3.5 2Wiki F1 36.8 48.8 49.8
GenGround GPT-3.5 HotpotQA F1 42.28 47.8 52.26
GenGround GPT-3.5 MuSiQue F1 20.13 20.11 27.36
GenGround GPT-3.5 2Wiki F1 41.19 44.77 50.21
GenGround GPT-3.5 StrategyQA F1 68.13 71.78 77.12
Stochastic RAG FiD-Light (T5-Base) HotpotQA EM 25.6 – 27.3
Stochastic RAG FiD-Light (T5-XL) HotpotQA EM 29.2 – 31.1
Manuscript submitted to ACM
Retrieval-Augmented Generation: A Survey 32
Table 7. Reported Robustness Scores for RAG Frameworks. Precision, recall, and FactScore values extracted from original
publications, across multiple datasets. These scores serve as the empirical basis for the comparative robustness analysis in Section 5.
Taxonomy Framework Backbone Metric (Dataset) LLM + Retrieval LLM + Retrieval + Framework
Retriever-Based RAGFILCO RAG Precision (FEVER) 1.2 5.1
Re2G KGI0 Precision (NQ) 64.65 70.92
Re2G KGI0 Recall (NQ) 69.6 74.79
Re2G KGI0 R-Precision (NQ) 61.13 72.01
Re2G KGI0 Recall (TriviaQA) 63.12 73.16
Re2G KGI0 R-Precision (FEVER) 80.34 90.06
Re2G KGI0 Recall (FEVER) 86.53 92.91
Generator-Based RAGSELF-RAG LLaMA2-7B Precision (ASQA) 2.9 66.9
SELF-RAG LLaMA2-7B Recall (ASQA) 4 67.8
SELF-RAG LLaMA2-7B FactScore (ASQA) 78 81.2
SELF-RAG LLaMA2-13B Precision (ASQA) 2.3 70.3
SELF-RAG LLaMA2-13B Recall (ASQA) 3.6 71.3
SELF-RAG LLaMA2-13B FactScore (ASQA) 77.5 80.2
FiD-Light T5-Base FactScore (FEVER) – 80.6
FiD-Light T5-XL FactScore (FEVER) – 84.5
RAG w/ Rich Ans. Encoding RAG Recall (MSMARCO) 25.3 27.5
RAG w/ Rich Ans. Encoding RAG Recall (KILT WoW) 61.98 68.63
GenRT RAG Recall (NQ) 59.4 60.78
GenRT RAG Recall (TriviaQA) 68.22 70.01
Hybrid RAGCRAG LLaMA2-7B FactScore (Biography) 59.2 74.1
Self-RAG LLaMA2-7B FactScore (Biography) 59.2 81.2
Self-CRAG LLaMA2-7B FactScore (Biography) 59.2 86.2
Flare Instruct GPT-3.5 Precision (2Wiki) 48.6 49.1
Flare Instruct GPT-3.5 Recall (2Wiki) 51.5 52.5
Flare Direct GPT-3.5 Precision (2Wiki) 48.6 59.1
Flare Direct GPT-3.5 Recall (2Wiki) 51.5 62.6
Stochastic RAG FiD-Light (T5-Base) FactScore (FEVER) 80.6 81.3
Stochastic RAG FiD-Light (T5-XL) FactScore (FEVER) 84.5 84.8
DRAGIN LLaMA2-13B Precision (HotPotQA) 0.3711 0.4401
DRAGIN LLaMA2-13B Recall (HotPotQA) 0.374 0.411
DRAGIN VICUNA-13B Precision (HotPotQA) 0.3457 0.4226
DRAGIN VICUNA-13B Recall (HotPotQA) 0.352 0.389
Manuscript submitted to ACM