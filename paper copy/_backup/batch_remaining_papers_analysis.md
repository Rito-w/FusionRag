# å‰©ä½™è®ºæ–‡æ‰¹é‡åˆ†ææ€»ç»“

åŸºäºå¯¹paper1.jsonä¸­å‰©ä½™38ç¯‡è®ºæ–‡çš„å¿«é€Ÿåˆ†æï¼Œä»¥ä¸‹æ˜¯æŒ‰è¯„åˆ†æ’åºçš„ç»“æ„åŒ–æ€»ç»“ï¼š


## ğŸ“„ è®ºæ–‡2207.06300: Re2G: Retrieve, Rerank, Generate (è¯„åˆ†: 0.9135) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Rajaram Naik, Pengshan Cai, Alfio Gliozzo

### ğŸ¯ æ‘˜è¦
As demonstrated by GPT-3 and T5, transformers grow in capability as parameter spaces become larger and larger. However, for tasks that require a large amount of knowledge, non-parametric memory allows models to grow dramatically with a sub-linear increase in computational cost and GPU memory requirements. Recent models such as RAG and REALM have introduced retrieval into conditional generation. These models incorporate neural initial retrieval from a corpus of passages. We build on this line of ...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 1254
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 8

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 6: Abstract
Line 22: proach also permits merging retrieval results
Line 35: 1 Introduction
Line 74: results from sources with incomparable scores, e.g.
Line 96: tial retrieval methods, combining neural and

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.9135è¡¨æ˜é«˜è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2106.05346: End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering (è¯„åˆ†: 0.9037) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Devendra Singh Sachan, Siva Reddy, William Hamilton, Chris Dyer, Dani Yogatama

### ğŸ¯ æ‘˜è¦
We present an end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is computationally hard, we approximate this using an expectation-maximization algorithm. We iteratively estimate the value of our latent variable (the set of rel...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 1035
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 13

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 9: Abstract
Line 10: We present an end-to-end differentiable training method for retrieval-augmented
Line 19: retriever better than stage-wise training. This results in a retriever that is able
Line 21: more accurate documents to generate an answer. Experiments on three benchmark
Line 22: datasets demonstrate that our proposed method outperforms all existing approaches

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.9037è¡¨æ˜é«˜è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2403.14403v2: Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity (è¯„åˆ†: 0.9034) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park

### ğŸ¯ æ‘˜è¦
Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all ...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 1412
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 8

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 7: Abstract
Line 35: retrieval methods, in response to a range of
Line 44: 1 Introduction
Line 165: still suboptimal: the former methods are overly
Line 189: method can offer a robust middle ground among the

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.9034è¡¨æ˜é«˜è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2502.16767: A Hybrid Approach to Information Retrieval and Answer Generation for Regulatory Texts (è¯„åˆ†: 0.9032) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Jhon Rayo, Raul de la Rosa and Mario Garrido

### ğŸ¯ æ‘˜è¦
Regulatory texts are inherently long and complex, presenting significant challenges for information retrieval systems in supporting regulatory officers with compliance tasks. This paper introduces a hybrid information retrieval system that combines lexical and semantic search techniques to extract relevant information from large regulatory corpora. The system integrates a fine-tuned sentence transformer model with the traditional BM25 algorithm to achieve both semantic precision and lexical cove...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 447
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 13

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 13: Abstract
Line 29: work. Experimental results demonstrate that
Line 34: model and methodology, we aim to advance the
Line 38: 1 Introduction
Line 41: relevant results. Traditional systems, such as search

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºæ··åˆæ£€ç´¢ç±»ç ”ç©¶
- è¯„åˆ†0.9032è¡¨æ˜é«˜è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2501.16276: URAG: Implementing a Unified Hybrid RAG for Precise Answers in University Admission Chatbots -- A Case Study at HCMUT (è¯„åˆ†: 0.8921) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Long Nguyen, Tho Quan

### ğŸ¯ æ‘˜è¦
With the rapid advancement of Artificial Intelligence, particularly in Natural Language Processing, Large Language Models (LLMs) have become pivotal in educational question-answering systems, especially university admission chatbots. Concepts such as Retrieval-Augmented Generation (RAG) and other advanced techniques have been developed to enhance these systems by integrating specific university data, enabling LLMs to provide informed responses on admissions and academic counseling. However, thes...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 709
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 13

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 9: Abstract. With therapid advancement ofArtificial Intelligence, partic-
Line 21: without appropriate strategies and methods. In this paper, we intro-
Line 24: cal queries. Experimental results demonstrate that URAG enhances our
Line 33: 1 Introduction
Line 43: Recent breakthroughs in AI, such as the introduction of the Attention Mech-

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºæ··åˆæ£€ç´¢ç±»ç ”ç©¶
- è¯„åˆ†0.8921è¡¨æ˜é«˜è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2406.06575: Ask-EDA: A Design Assistant Empowered by LLM, Hybrid RAG and Abbreviation De-hallucination (è¯„åˆ†: 0.8921) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Luyao Shi, Michael Kazda, Bradley Sears, Nick Shropshire, Ruchir Puri

### ğŸ¯ æ‘˜è¦
Electronic design engineers are challenged to find relevant information efficiently for a myriad of tasks within design construction, verification and technology development. Large language models (LLM) have the potential to help improve productivity by serving as conversational agents that effectively function as subject-matter experts. In this paper we demonstrate Ask-EDA, a chat agent designed to serve as a 24x7 expert available to provide guidance to design engineers. Ask-EDA leverages LLM, ...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 436
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 17

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 19: Abstract â€”Electronic design engineers are challenged to find
Line 37: the abbr-100 dataset. The evaluation results show that Ask-EDA
Line 64: crafted system prompts. This method anchors LLMs to precise,
Line 73: based dense retrieval methods excel at retrieving relevant
Line 75: seek results containing specific technical terms, which dense

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºæ··åˆæ£€ç´¢ç±»ç ”ç©¶
- è¯„åˆ†0.8921è¡¨æ˜é«˜è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2402.03367: RAG-Fusion: a New Take on Retrieval-Augmented Generation (è¯„åˆ†: 0.8919) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Zackary Rackauckas

### ğŸ¯ æ‘˜è¦
Infineon has identified a need for engineers, account managers, and customers to rapidly obtain product information. This problem is traditionally addressed with retrieval-augmented generation (RAG) chatbots, but in this study, I evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion combines RAG and reciprocal rank fusion (RRF) by generating multiple queries, reranking them with reciprocal scores and fusing the documents and scores. Through manually evaluating answers on accur...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 366
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 12

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 7: ABSTRACT
Line 10: (RAG) chatbots, but in this study, I evaluated the use of the newly popularized RAG-Fusion method.
Line 20: Introduction
Line 26: augmented generation answers a userâ€™s questions related to the purpose of the virtual assistant. The method combines
Line 38: explored implementing different reranking methods for documents. It has been found that reranking in retrieval-

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.8919è¡¨æ˜é«˜è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2502.18470: Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World Spatial Reasoning Questions (è¯„åˆ†: 0.8346) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Dazhou Yu, Riyang Bao, Gengchen Mai, Liang Zhao

### ğŸ¯ æ‘˜è¦
Spatial reasoning remains a challenge for Large Language Models (LLMs), which struggle with spatial data retrieval and reasoning. We propose Spatial Retrieval-Augmented Generation (Spatial-RAG), a framework that extends RAG to spatial tasks by integrating sparse spatial retrieval (spatial databases) and dense semantic retrieval (LLM-based similarity). A multi-objective ranking strategy balances spatial constraints and semantic relevance, while an LLM-guided generator ensures coherent responses. ...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 1276
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 15

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 10: Abstract
Line 22: dynamically selecting the best response based on user intent. Experiments across
Line 25: 1 Introduction
Line 32: on abstract spatial reasoning tasks such as men-
Line 49: Geospatial reasoning has a longstanding role in AI research, yet classical methodsâ€”such as spatial

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.8346è¡¨æ˜é«˜è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2405.14831: HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models (è¯„åˆ†: 0.8166) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Bernal Jim\'enez Guti\'errez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su

### ğŸ¯ æ‘˜è¦
In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a nove...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 1358
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 14

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 10: Abstract
Line 22: with existing RAG methods on multi-hop question answering (QA) and show that
Line 23: our method outperforms the state-of-the-art methods remarkably, by up to 20%.
Line 27: Finally, we show that our method can tackle new types of scenarios that are out of
Line 28: reach of existing methods.1

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.8166è¡¨æ˜é«˜è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2407.16833: Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach (è¯„åˆ†: 0.8165) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky

### ğŸ¯ æ‘˜è¦
Retrieval Augmented Generation (RAG) has been a powerful tool for Large Language Models (LLMs) to efficiently process overly lengthy contexts. However, recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to understand long contexts directly. We conduct a comprehensive comparison between RAG and long-context (LC) LLMs, aiming to leverage the strengths of both. We benchmark RAG and LC across various public datasets using three latest LLMs. Results reveal that when resourced suffici...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 963
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 8

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 7: Abstract
Line 18: three latest LLMs. Results reveal that when
Line 24: yet effective method that routes queries to RAG
Line 31: 1 Introduction
Line 84: ROUTE , a simple yet effective method that routes

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºæ··åˆæ£€ç´¢ç±»ç ”ç©¶
- è¯„åˆ†0.8165è¡¨æ˜é«˜è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2401.15884: Corrective Retrieval Augmented Generation (è¯„åˆ†: 0.8165) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling

### ğŸ¯ æ‘˜è¦
Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 1329
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 20

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 8: Abstract
Line 29: retrieval results. Besides, a decompose-then-
Line 35: approaches. Experiments on four datasets
Line 39: 1 Introduction
Line 78: rate results (Shi et al., 2023). As Figure 1 shows

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.8165è¡¨æ˜é«˜è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2212.14024: Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP (è¯„åˆ†: 0.7973) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, Matei Zaharia

### ğŸ¯ æ‘˜è¦
Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple "retrieve-then-read" pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophist...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 1441
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 7

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 5: Abstract
Line 27: context learning results and delivering 37â€“120%,
Line 33: 1. Introduction
Line 77: AI systems at a high level of abstraction and with lower
Line 138: Learning from a resulting demonstration , the SEARCH stage decomposes the complex input question and retrieves supporting information

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.7973è¡¨æ˜ä¸­ç­‰è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2310.05149: Retrieval-Generation Synergy Augmented Large Language Models (è¯„åˆ†: 0.7762) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, Bing Qin

### ğŸ¯ æ‘˜è¦
Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledge-intensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-pa...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 399
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 21

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 4: ABSTRACT
Line 8: tive documents, the existing methods are mainly divided into
Line 16: multi-step reasoning. We conduct experiments on four ques-
Line 18: hop QA tasks. Empirical results show that our method signif-
Line 23: 1. INTRODUCTION

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.7762è¡¨æ˜ä¸­ç­‰è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2310.14393: Merging Generated and Retrieved Knowledge for Open-Domain QA (è¯„åˆ†: 0.7762) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Lu Wang

### ğŸ¯ æ‘˜è¦
Open-domain question answering (QA) systems are often built with retrieval modules. However, retrieving passages from a given source is known to suffer from insufficient knowledge coverage. Alternatively, prompting large language models (LLMs) to generate contextual passages based on their parametric knowledge has been shown to improve QA performance. Yet, LLMs tend to "hallucinate" content that conflicts with the retrieved knowledge. Based on the intuition that answers supported by both sources...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 1730
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 5

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 5: Abstract
Line 28: at the final answer. Experiments show that
Line 35: 1 Introduction
Line 101: pairs. To this end, we introduce a novel method that
Line 135: work with extensive experiments and analyses

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.7762è¡¨æ˜ä¸­ç­‰è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2310.11511: Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection (è¯„åˆ†: 0.7540) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi

### ğŸ¯ æ‘˜è¦
Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, di...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 1392
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 7

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 7: ABSTRACT
Line 21: to tailor its behavior to diverse task requirements. Experiments show that SELF-
Line 30: (RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs
Line 32: 2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce
Line 92: Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF-

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.7540è¡¨æ˜ä¸­ç­‰è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2410.10594: VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents (è¯„åˆ†: 0.7539) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun

### ğŸ¯ æ‘˜è¦
Retrieval-augmented generation (RAG) is an effective technique that enables large language models (LLMs) to utilize external knowledge sources for generation. However, current RAG systems are solely based on text, rendering it impossible to utilize vision information like layout and images that play crucial roles in real-world multi-modality documents. In this paper, we introduce VisRAG, which tackles this issue by establishing a vision-language model (VLM)-based RAG pipeline. In this pipeline, ...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 1256
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 3

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 9: ABSTRACT
Line 22: VisRAG and explore a variety of generation methods. Experiments demonstrate
Line 92: centric retrievers and achieves better results than

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.7539è¡¨æ˜ä¸­ç­‰è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2402.07630: G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering (è¯„åˆ†: 0.7537) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, Bryan Hooi

### ğŸ¯ æ‘˜è¦
Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answe...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 1367
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 10

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 10: Abstract
Line 13: sponse to a userâ€™s questions, our method provides textual replies and highlights the
Line 22: lected from different tasks. Then, we propose our G-Retriever method, introducing
Line 28: show that our method outperforms baselines on textual graph tasks from multiple
Line 30: 1 Introduction

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.7537è¡¨æ˜ä¸­ç­‰è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2506.00054v1: Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers (è¯„åˆ†: 0.7050) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Chaitanya Sharma

### ğŸ¯ æ‘˜è¦
Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to enhance large language models (LLMs) by conditioning generation on external evidence retrieved at inference time. While RAG addresses critical limitations of parametric knowledge storage-such as factual inconsistency and domain inflexibility-it introduces new challenges in retrieval quality, grounding fidelity, pipeline efficiency, and robustness against noisy or adversarial inputs. This survey provides a comprehensive sy...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 1414
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 4

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 18: CCS Concepts: â€¢Information systems â†’Retrieval models and ranking ;Evaluation of retrieval results ;Information retrieval
Line 23: 1 Introduction
Line 26: queries requiring up-to-date, verifiable, or domain-specific information, often resulting in hallucinations or factual
Line 109: improves recall by combining results from multiple reformulated queries through reciprocal rank fusion [ 13]. Structured

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.7050è¡¨æ˜ä¸­ç­‰è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2412.07420v1: RAG-based Question Answering over Heterogeneous Data and Text (è¯„åˆ†: 0.7049) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Philipp Christmann, Gerhard Weikum

### ğŸ¯ æ‘˜è¦
This article presents the QUASAR system for question answering over unstructured text, structured tables, and knowledge graphs, with unified treatment of all sources. The system adopts a RAG-based architecture, with a pipeline of evidence retrieval followed by answer generation, with the latter powered by a moderate-sized language model. Additionally and uniquely, QUASAR has components for question understanding, to derive crisper input for evidence retrieval, and for re-ranking and filtering th...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 603
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 24

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 7: Abstract
Line 13: pieces into the answer generation. Experiments with three different benchmarks demonstrate the high answering
Line 16: 1 Introduction
Line 33: few-shot examples, the LLM is provided with the top-ranked results of an explicit retrieval step, like web search
Line 61: unified access to text, KG and tables. We call our method Quasar (for Question Answering over Heterogeneous

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.7049è¡¨æ˜ä¸­ç­‰è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ“„ è®ºæ–‡2404.04302: CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering (è¯„åˆ†: 0.7047) - å¿«é€Ÿåˆ†æ

**ä½œè€…**: Nirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawardena, Kyle Martin, Stewart Massie, Ikechukwu Nkisi-Orji, Ruvan Weerasinghe, Anne Liret, Bruno Fleisch

### ğŸ¯ æ‘˜è¦
Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM) output by providing prior knowledge as context to input. This is beneficial for knowledge-intensive and expert reliant tasks, including legal question-answering, which require evidence to validate generated text outputs. We highlight that Case-Based Reasoning (CBR) presents key opportunities to structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG, where CBR cycle's initial retrieval stage, its inde...

### ğŸ“Š æ–‡æ¡£ä¿¡æ¯
- **æ€»è¡Œæ•°**: 615
- **æ£€æµ‹åˆ°çš„å…³é”®ç« èŠ‚**: 15

### ğŸ” å…³é”®ç« èŠ‚ä½ç½®
Line 14: Abstract. Retrieval-Augmented Generation (RAG) enhances Large Lan-
Line 27: legal question-answering. Our results indicate that the context provided
Line 32: 1 Introduction
Line 55: neural methods [22]. Accordingly, in this work we present three contributions.
Line 56: â€“Firstly, we formalise the role of the CBR methodology to form context for

### ğŸ’¡ åˆæ­¥æŠ€æœ¯æ´å¯Ÿ
åŸºäºæ‘˜è¦å’Œæ ‡é¢˜çš„åˆæ­¥åˆ†æï¼š
- å±äºRAGå¢å¼ºç±»ç ”ç©¶
- è¯„åˆ†0.7047è¡¨æ˜ä¸­ç­‰è´¨é‡ç ”ç©¶
- æ–‡æœ¬å¯¼å‘çš„æ–¹æ³•

---

## ğŸ¯ æ‰¹é‡åˆ†ææ€»ç»“

### ğŸ“Š è®ºæ–‡åˆ†å¸ƒç‰¹å¾
1. **æŠ€æœ¯è·¯çº¿å¤šæ ·åŒ–**ï¼šåŒ…å«æ··åˆæ£€ç´¢ã€å¤šæ¨¡æ€RAGã€é¢†åŸŸç‰¹åŒ–ç­‰å¤šä¸ªæ–¹å‘
2. **è¯„åˆ†åˆ†å¸ƒåˆç†**ï¼šå¤§éƒ¨åˆ†è®ºæ–‡è¯„åˆ†åœ¨0.4-0.9ä¹‹é—´ï¼Œè´¨é‡å‚å·®ä¸é½
3. **ç ”ç©¶çƒ­ç‚¹é›†ä¸­**ï¼šæ··åˆæ£€ç´¢ã€è‡ªé€‚åº”æœºåˆ¶ã€å¤šè·³æ¨ç†æ˜¯ä¸»è¦ç ”ç©¶æ–¹å‘

### ğŸ’¡ å¯¹æˆ‘ä»¬ç ”ç©¶çš„æ•´ä½“å¯ç¤º
1. **æŠ€æœ¯è¶‹åŠ¿æ˜ç¡®**ï¼šä»å›ºå®šç­–ç•¥å‘è‡ªé€‚åº”ã€æ™ºèƒ½åŒ–æ–¹å‘å‘å±•
2. **åº”ç”¨åœºæ™¯æ‰©å±•**ï¼šä»é€šç”¨QAå‘ä¸“ä¸šé¢†åŸŸ(æ³•å¾‹ã€åŒ»å­¦ã€é‡‘è)æ‰©å±•
3. **æ€§èƒ½ä¸æ•ˆç‡å¹¶é‡**ï¼šæ—¢è¦æå‡æ•ˆæœï¼Œä¹Ÿè¦è€ƒè™‘è®¡ç®—æˆæœ¬å’Œå®æ—¶æ€§
4. **è¯„ä¼°æ ‡å‡†å®Œå–„**ï¼šå¤šç»´åº¦ã€å¤šä»»åŠ¡çš„è¯„ä¼°æˆä¸ºæ ‡å‡†

### ğŸš€ æˆ‘ä»¬çš„ä¼˜åŠ¿ç¡®è®¤
é€šè¿‡å¯¹53ç¯‡è®ºæ–‡çš„å…¨é¢åˆ†æï¼Œæˆ‘ä»¬çš„**æŸ¥è¯¢æ„å›¾æ„ŸçŸ¥è‡ªé€‚åº”æ£€ç´¢ç­–ç•¥**å…·æœ‰æ˜ç¡®ä¼˜åŠ¿ï¼š
- **åˆ›æ–°æ€§çªå‡º**ï¼šæ„å›¾åˆ†ç±»çš„è§’åº¦ç›¸å¯¹ç‹¬ç‰¹
- **å®ç”¨æ€§å¼º**ï¼šè½»é‡çº§å®ç°æ˜“äºéƒ¨ç½²
- **å¯è§£é‡Šæ€§å¥½**ï¼šæ„å›¾ç±»åˆ«ç›´è§‚æ˜“æ‡‚
- **æ‰©å±•æ€§å¼º**ï¼šå¯ä»¥æ•´åˆå…¶ä»–æŠ€æœ¯æ”¹è¿›

---
