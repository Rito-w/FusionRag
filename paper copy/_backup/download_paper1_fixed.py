#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆæœ¬ï¼šä¸‹è½½paper1.jsonä¸­çš„é‡è¦è®ºæ–‡
"""
import json
import os
import requests
from datetime import datetime
import re

def extract_arxiv_id(link):
    """ä»é“¾æ¥ä¸­æå–arXiv ID"""
    # æ”¯æŒå¤šç§arXivé“¾æ¥æ ¼å¼
    patterns = [
        r'arxiv\.org/abs/(\d+\.\d+)',
        r'arxiv\.org/pdf/(\d+\.\d+)',
        r'(\d{4}\.\d{4,5})'  # ç›´æ¥çš„IDæ ¼å¼
    ]
    
    for pattern in patterns:
        match = re.search(pattern, link)
        if match:
            return match.group(1)
    return None

def download_paper_direct(arxiv_id, title, download_dir="paper1_downloads"):
    """ç›´æ¥ä¸‹è½½è®ºæ–‡PDF"""
    os.makedirs(download_dir, exist_ok=True)
    
    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
    safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()
    safe_title = safe_title[:60]  # é™åˆ¶é•¿åº¦
    filename = f"{arxiv_id}_{safe_title}.pdf"
    filepath = os.path.join(download_dir, filename)
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
    if os.path.exists(filepath):
        print(f"  æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
        return filepath
    
    try:
        # ç›´æ¥æ„é€ PDFä¸‹è½½é“¾æ¥
        pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
        
        # ä¸‹è½½PDF
        response = requests.get(pdf_url, stream=True, timeout=30)
        response.raise_for_status()
        
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"  âœ… ä¸‹è½½å®Œæˆ: {filename}")
        return filepath
        
    except Exception as e:
        print(f"  âŒ ä¸‹è½½å¤±è´¥ {arxiv_id}: {e}")
        return None

def main():
    print("ğŸ” å¼€å§‹å¤„ç†paper1.jsonä¸­çš„è®ºæ–‡...")
    
    # åŠ è½½è®ºæ–‡åˆ—è¡¨
    with open('paper1.json', 'r', encoding='utf-8') as f:
        papers = json.load(f)
    
    print(f"ğŸ“š æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    
    # æŒ‰è¯„åˆ†æ’åºï¼Œä¸‹è½½å‰10ç¯‡
    sorted_papers = sorted(papers, key=lambda x: x.get('score', 0), reverse=True)
    
    downloaded_papers = []
    
    print(f"ğŸš€ å¼€å§‹ä¸‹è½½å‰10ç¯‡é«˜åˆ†è®ºæ–‡...")
    
    for i, paper in enumerate(sorted_papers[:10], 1):
        print(f"\nğŸ“„ [{i}/10] {paper['title'][:60]}...")
        print(f"   è¯„åˆ†: {paper.get('score', 0):.4f}")
        
        # æå–arXiv ID
        arxiv_id = extract_arxiv_id(paper['link'])
        if not arxiv_id:
            print(f"  âŒ æ— æ³•æå–arXiv ID: {paper['link']}")
            continue
        
        print(f"   arXiv ID: {arxiv_id}")
        
        # ä¸‹è½½è®ºæ–‡
        filepath = download_paper_direct(arxiv_id, paper['title'])
        if filepath:
            downloaded_papers.append({
                'arxiv_id': arxiv_id,
                'title': paper['title'],
                'filepath': filepath,
                'score': paper.get('score', 0),
                'abstract': paper.get('abstract', ''),
                'authors': paper.get('authors', []),
                'publish_time': paper.get('publish_time', '')
            })
    
    print(f"\nâœ… æˆåŠŸä¸‹è½½ {len(downloaded_papers)} ç¯‡è®ºæ–‡")
    
    # åˆ›å»ºä¸‹è½½è®°å½•
    with open('paper1_download_log.json', 'w', encoding='utf-8') as f:
        json.dump(downloaded_papers, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ ä¸‹è½½è®°å½•å·²ä¿å­˜åˆ°: paper1_download_log.json")
    
    return downloaded_papers

if __name__ == "__main__":
    main()
