## **TableRAG: A Retrieval Augmented Generation Framework for** **Heterogeneous Document Reasoning**


**Xiaohan Yu** _[∗]_

Huawei Cloud BU, Beijing
yuxiaohan5@huawei.com


**Pu Jian** _[∗]_

Huawei Cloud BU, Beijing
jianpu2@huawei.com


**Chong Chen** _[†]_

Huawei Cloud BU, Beijing
chenchong55@huawei.com


**Abstract**

Retrieval-Augmented Generation (RAG) has
demonstrated considerable effectiveness in

open-domain question answering. However,
when applied to heterogeneous documents,
comprising both textual and tabular components, existing RAG approaches exhibit critical
limitations. The prevailing practice of flattening tables and chunking strategies disrupts the
intrinsic tabular structure, leads to information
loss, and undermines the reasoning capabilities
of LLMs in multi-hop, global queries. To address these challenges, we propose TableRAG,
an hybrid framework that unifies textual understanding and complex manipulations over
tabular data. TableRAG iteratively operates in
four steps: context-sensitive query decomposition, text retrieval, SQL programming and
execution, and compositional intermediate answer generation. We also develop HeteQA,
a novel benchmark designed to evaluate the
multi-hop heterogeneous reasoning capabilities. Experimental results demonstrate that
TableRAG consistently outperforms existing
baselines on both public datasets and our HeteQA, establishing a new state-of-the-art for heterogeneous document question answering. We
release TableRAG at [https://github.com/](https://github.com/yxh-y/TableRAG/tree/main)
[yxh-y/TableRAG/tree/main.](https://github.com/yxh-y/TableRAG/tree/main)

**1** **Introduction**

Heterogeneous document-based question answering (Chen et al., 2020), which necessitates reasoning over both unstructured text and structured tabular data, presents substantial challenges. Tables are
characterized by interdependent rows and columns,
while natural language texts are sequential. Bridging this divergence within a unified QA system
remains a non-trivial task.

The prevailing approach extends the retrievalaugmented generation (RAG) paradigm, in which

1 _∗_ These authors contributed equally to this work.
2 _†_ Corresponding author.

|2008_in_video_gaming: 2008 has seen many sequels and prequels in video games . New intellectual properties<br>include Army of Two, Dead Space, Left 4 Dead, LittleBigPlanet, Mirror 's Edge, Race Driver : Grid, Grand<br>Theft Auto IV and Spore……<br>Title Release date … Publisher LIVE Games on<br>Demand<br>007 : Quantum 2008-11-04 ... Activision Y N<br>of Solace<br>(Abbreviated Table content…..)<br>Lego Batman : 2008-09-23 Warner Bros. Interactive N Y<br>The Videogame Entertainment|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|2008_in_video_gaming: 2008 has seen many sequels and prequels in video games . New intellectual properties<br>include Army of Two, Dead Space, Left 4 Dead, LittleBigPlanet, Mirror 's Edge, Race Driver : Grid, Grand<br>Theft Auto IV and Spore……<br>Title Release date … Publisher LIVE Games on<br>Demand<br>007 : Quantum 2008-11-04 ... Activision Y N<br>of Solace<br>(Abbreviated Table content…..)<br>Lego Batman : 2008-09-23 Warner Bros. Interactive N Y<br>The Videogame Entertainment|Title Release date …|Publisher||LIVE|Games on<br>Demand|
|2008_in_video_gaming: 2008 has seen many sequels and prequels in video games . New intellectual properties<br>include Army of Two, Dead Space, Left 4 Dead, LittleBigPlanet, Mirror 's Edge, Race Driver : Grid, Grand<br>Theft Auto IV and Spore……<br>Title Release date … Publisher LIVE Games on<br>Demand<br>007 : Quantum 2008-11-04 ... Activision Y N<br>of Solace<br>(Abbreviated Table content…..)<br>Lego Batman : 2008-09-23 Warner Bros. Interactive N Y<br>The Videogame Entertainment|007 : Quantum 2008-11-04 ...<br>of Solace<br>(Abbreviated<br>Lego Batman : 2008-09-23<br>The Videogame|Activision<br>Table content…..)<br>Warner Bros. Interactive<br>Entertainment||Y<br>N|N<br>Y|


Figure 1: An example of the heterogeneous document
based question answering task.

the tables are linearized into textual representations
(e.g., Markdown) (Gao et al., 2023; Jin and Lu,
2023; Ye et al., 2023b). Typically, chunking strategies are employed (Finardi et al., 2024), wherein
flattened tables are segmented and merged with adjacent text spans. During inference, the LLMs generate answers based on the top-N retrieved chunks.
However, these methodologies are predominantly
tailored to scenarios that require only surface-level
comprehension of tables, such as direct answer
extraction (Pasupat and Liang, 2015a; Zhu et al.,
2021). When applied to extensive documents that
interleave textual and tabular elements, existing
RAG methodologies exhibit critical limitations:

- Structural Information Loss: The tabular struc
ture integrity is compromised, leading to information loss or irrelevant context that impedes
downstream LLMs performance.

- Lack of Global View: Due to document fragmentation, the RAG system struggles with multi-hop
global queries (Edge et al., 2024), such as aggregation, mathematical computations, and other


What percentage of games published by Activision in 2008 are still live
(have ‘Y’ in the live column) compared to all live games in the List of
Games for Windows titles?








1

reasoning tasks that require a holistic understanding across entire tables.

As illustrated in Figure 1, the RAG approach computes percentage over the top-N most relevant
chunks rather than the full table, and thus results in

an incorrect answer.

To address these limitations of existing RAG systems, we propose TableRAG, an hybrid (SQL execution and text retrieval) framework that dynamic
transitions between textual understanding and complex manipulations over tabular data. TableRAG
interacts with tables by leveraging SQL as an interface. Concretely, the framework operates via a
two-stage process: an offline database construction
phase and an online inference phase of iterative
reasoning. The iterative reasoning procedure comprises four core operations: (i) context-sensitive
query decomposition, (ii) text retrieval, (iii) SQL
programming and execution, and (iv) intermediate
answer generation. The utilization of SQL enables
precise symbolic execution by treating table-related
queries as indivisible reasoning units, thereby enhancing both computational efficiency and reasoning fidelity. To facilitate rigorous evaluation
of multi-hop reasoning over heterogeneous documents, we introduce HeteQA, a novel benchmark
consisting of 304 examples across nine diverse
domains. Each example contains a composition
across five distinct tabular operations. We evaluate TableRAG on both established public benchmarks and our HeteQA dataset against strong baselines, including generic RAG and program-aided
approaches. Experimental results demonstrate that
TableRAG consistently achieves state-of-the-art
performance. Overall, our contributions are summarized as follows:

- We identify two key limitations of existing RAG
approaches in the context of heterogeneous document question answering: structural information
loss and lack of global view.

- We propose TableRAG, an hybrid framework that
unifies textual understanding and complex manipulations over tabular data. TableRAG comprises
an offline database construction phase and a fourstep online iterative reasoning process.

- We develop HeteQA, a benchmark for evaluating
multi-hop heterogeneous reasoning capabilities.
Experimental results show that TableRAG outperforms RAG and programmatic approaches on


HeteQA and public benchmarks, establishing a
state-of-the-art solution.

**2** **Task Formulation**

In the context of the heterogeneous document question answering task, we define the task input as
extensive documents, denoted as ( _T, D_ ) where _T_
denotes the textual contents and _D_ refers to the

tabular components. Given a user question _q_, the
objective of this task is to optimize a function _F_
that, given the combined textual and tabular context, can produce the correct answer _A_ :

_F_ ( _D, T, q_ ) _→A._ (1)

**3** **TableRAG Framework**

**3.1** **Overview and Design Principles**

We propose TableRAG, an hybrid (SQL execution and text retrieval) framework designed to preserve table structural integrity and facilitate heterogeneous reasoning. As depicted in Figure 2,
TableRAG consists of offline and online workflows.
The offline phase is tasked with database construction, while the online phase facilitates iterative reasoning. The reasoning procedure unfolds in a fourstage process: (i) Context-sensitive query decomposition, which identifies the respective roles of
textual and tabular modalities within the query. (ii)
Text retrieval. (iii) SQL programming and execution, which is selectively invoked for subqueries
requiring tabular data reasoning. (iv) Compositional intermediate answer generation. The preferential use of SQL is motivated by its capacity to
leverage the expressive strength of symbolic execution over structured data, thereby enabling tabular
components within user queries to be treated as
monolithic reasoning units. In contrast, other languages like Python incur substantial computational
overhead when dealing with large-scale data or
complex workloads (Shahrokhi et al., 2024).

**3.2** **Database Construction**

In the offline stage, we first extract structured components from heterogeneous documents, yielding a
set of tables _D_ = _{D_ 1 _, . . ., D_ _M_ _}_ . To enable information retrieval, we construct two parallel corpora:
a textual knowledge base and a tabular schema
database. The textual knowledge base comprises
both the raw texts _T_ and the Markdown-rendered
form of each table, denoted as _D_ [ˆ] . Both _D_ [ˆ] and _T_


2

|Offline Phase: Database construction<br>Table Content<br>Table Parsing<br>Table Schema<br>Extraction<br>Table Component<br>Ingestion<br>Document Table Schema<br>chunking {<br>Heterogeneous "table_name": "<Table Name>",<br>Documents (word, "columns": [<br>pdf, csv, execl ["<ColName>", "<Type>", "<Examples>"],<br>.etc) ...<br>]<br>Text Component Document-oriented Mc ah pu pn ink } R De al ta at bio an sa el<br>g<br>Database<br>Table Schema|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
|Online Phase: Interative reasoning SQL programing and execution<br>User Sub Query Extract SQL<br>Original + NL2SQL and<br>Text Retrival<br>Query Table Schema Get excution result<br>Yes<br>Trigger the workflow<br>of fetching results from DB Summarize<br>Query reference materials<br>Decompision Any table chunks in from both source<br>TopN results?<br>Please answer the question {question}<br>based on the following materials：<br>## Final Answer<br>Document {SQL excution results} Generation<br>Sub Query<br>chunk Retrival Retrival …<br>Results {Doc retrieval results}|SQL programing and execution<br>Sub Query Extract SQL<br>+ NL2SQL and<br>Table Schema Get excution result<br>Yes|SQL programing and execution<br>Sub Query Extract SQL<br>+ NL2SQL and<br>Table Schema Get excution result<br>Yes|SQL programing and execution<br>Sub Query Extract SQL<br>+ NL2SQL and<br>Table Schema Get excution result<br>Yes|SQL programing and execution<br>Sub Query Extract SQL<br>+ NL2SQL and<br>Table Schema Get excution result<br>Yes|


Figure 2: The overall architecture of TableRAG.


are segmented into chunks, which are then embedded into dense vector representations using a pretrained language model (Chen et al., 2024a). For
tabular schema database construction, we represent
each table _D_ _i_ by a standardized schema description
_S_ ( _D_ _i_ ), derived via a template as follows:



Then, we define a mapping from each flattened
table chunk to its originating table schema:

_f_ : _D_ [ˆ] _i,j_ _→_ _S_ ( _D_ _i_ ) (2)

where _D_ [ˆ] _i,j_ denotes the _j_ -th chunk derived from
table _D_ [ˆ] _i_ . This mapping ensures that local segments
remain contextually anchored to the table structure
from which they are derived.
The tables are also ingested in a relational


database (e.g., MySQL [1] ), supporting symbolic
query execution in the subsequent online reasoning.

**3.3** **Iterative Reasoning**

To address multi-hop, global queries that require
compositional reasoning over texts and tables, we
introduce an iterative inference process aligned
with _F_ in Equation 1. This process comprises
four core operations: (i) context-sensitive query
decomposition, (ii) text retrieval, (iii) program and
execute SQL, and (iv) compositional intermediate
answer generation. Through repeated cycles of decomposition and resolution, a solution to the query
is progressively constructed. Detailed prompt templates are provided in Appendix F.

**Context-Sensitive Query Decomposition** We
explicitly delineate the respective roles of textual
and tabular modalities during the reasoning process. While a table-related query may involve
multiple semantic reasoning steps, its tabular resolution can collapse to a single executable operation. Consequently, an effective decomposition
of global queries demands more than mere syntac
1 [https://github.com/mysql](https://github.com/mysql)


3

tic segmentation, but also structural awareness of
the underlying data sources. To this end, we first
retrieve the most relevant table content from the

textual database and link it to its corresponding
table schema description _S_ ( _D_ _[t]_ ) via the mapping
function _f_ . Based on this, we formulate a subquery
_q_ _t_ at the _t_ -th iteration.

**Text Retrieval** We deploy a retrieval module that
operates in two successive stages: vector-based
recall followed by semantic reranking. Given an
incoming query _q_ _t_, it is encoded into a shared dense
embedding space alongside document chunks. We
then select the top- _N_ candidates with the highest
cosine similarity to the query embedding:


�


_T_ ˆ _recall_ _[q]_ _[t]_ [=][ top-] _[N]_


�


arg _T_ ˆ _i_ _∈{_ max ˆ _D,T }_ _cos_ ( **v** _T_ ˆ _i_ _,_ **v** _q_ _t_ )


_,_


(3)
In the subsequent reranking stage, the recalled candidate chunks are re-evaluated by a more expressive
relevance model, yielding the final top- _k_ selections,
denoted by _T_ [ˆ] _rerank_ _[q]_ _[t]_ [.]

**SQL Programming and Execution** To support
accurate reasoning over tabular data, we incorporate a "program-and-execute" mechanism that is
selectively invoked only when subquery reasoning
involves tables. Specifically, we inspect whether
any content originates from tabular sources in the
retrieved results. For each chunk in the top-ranked
set _T_ [ˆ] _rerank_ _[q]_ _[t]_ [, we apply the mapping function (in]
Equation 2) to extract its associated schema, yielding a table schema set:

_S_ _[t]_ = _{f_ ( _T_ [ˆ] _i_ ) _|_ _T_ [ˆ] _i_ _∈_ _T_ [ˆ] _rerank_ _[q]_ _[t]_ _[}][.]_ (4)

If the set _S_ _[t]_ is empty, this module is passed. Otherwise, we derive an accurate answer with the current
subquery _q_ _t_ and the corresponding schema context as inputs. To achieve this, we leverage structured query execution over relational data and use
SQL as the intermediate formal language. A dedicated tool _f_ _SQL_ with LLM as backend generates
executable SQL programs and applies them to the
pre-constructed MySQL database, formalized as
follows:

_e_ _t_ = _f_ _SQL_ ( _S_ _[t]_ _, q_ _t_ ) _._ (5)

**Intermediate Answer Generation** For the sub
query _q_ _t_, TableRAG can benefit from two heterogeneous information sources: the execution result _e_ _t_
over SQL database and text retrieval result _T_ [ˆ] _rerank_ _[q]_ _[t]_


from the document database. Both of the data

sources provide partial or complete evidence. They
introduce distinct failure modes: SQL execution
may produce incorrect results or execution errors,
while text retrieval may yield incomplete or misleading context. Consequently, the results from
these sources may either reinforce each other or
present contradictions. To address this, we adopt
a compositional reasoning mechanism. The execution result _e_ _t_ and the retrieved textual chunks
_T_ ˆ _rerank_ _[q]_ _[t]_ [are cross-examined to validate consistency]
and guide answer selection. The final answer to
each subquery is derived by adaptively weighting
the reliability of each source based on its evidential
utility, _a_ _t_ = _F_ ( _e_ _t_ _,_ _T_ [ˆ] _rerank_ _[q]_ _[t]_ [)][.]
Once the query decomposition module determines that no further subqueries are necessary,
TableRAG terminates the iterative reasoning process, yielding the final answer _A_ = _a_ _T_, where _T_
denotes the total number of iterations performed.

**4** **Benchmark Construction**

In this section, we present HeteQA, a novel benchmark for assessing multi-hop reasoning across heterogeneous documents.

**4.1** **Data Collection**

HeteQA necessitates advanced operations, such as
arithmetic computation, nested logic, etc. To balance annotation fidelity with scalability, we adopt
a human-in-the-loop collaborative strategy that integrates LLMs with human verification. The construction pipeline proceeds in three stages:

**Query Generation** We curate tabular sources
from the Wikipedia dataset (Chen et al., 2020).
To facilitate analytical depth, we restrict our selection to tables with a minimum of 20 rows and

7 columns, and apply structural deduplication to
eliminate redundancy across similar schemas. For
each retained table, we define a suite of advanced
operations, e.g., conditional filtering, and statistical
aggregation. These operations serve as primitives
for constructing complex queries. Leveraging the
Claude-3.7-sonnet [2], we prompt for query synthesis
as compositions over these primitives. Each generated query is paired with executable code in both
SQL and Python. We execute the associated code
and obtain the answer. A final deduplication pass is
applied over both queries and answers, promoting

2 [https://www.anthropic.com/claude/sonnet](https://www.anthropic.com/claude/sonnet)


4

Figure 3: Domain distribution and tabular operation
distribution of HeteQA.

diversity in the dataset. Full implementation details
are provided in Appendix A.

**Answer Verification** To ensure correctness and
reliability, each instance is subjected to manual
inspection by human annotators. Their task is to
verify that the execution outcome is accurate for the
corresponding query. In cases where discrepancies
are found, they are responsible for correcting both
the underlying code and the resulting answer.

**Document Reference** To support queries that integrate both tabular and textual information, we
augment the instance by leveraging the associated
Wikipedia document. Specifically, certain entities
within the query are replaced with reference-based
formulations by the human annotators. For exam
_"_
ple, the query _"Which driver_ _. . ._ can be rephrased

_"_
as _"What is the nationality of the driver_ _. . ._ . This
entity substitution can either modify the subject of
the question and its corresponding answer or alter
the query phrasing while preserving the original
answer. The annotation guidelines and annotator
profiles are detailed in Appendix B.

**4.2** **Discussion**

Each data instance in HeteQA is composed of a
query, its corresponding answer, the executable
SQL sentence, and the execution-derived answer.
Through our data collection pipeline, we construct
304 high-quality examples whose answers are
grounded in both single-source (82%) and multisource (18%). The resulting benchmark spans 136
distinct tables and 5314 wiki knowledge entities.
To characterize the dataset, we analyze its semantic
domains and the types of tabular reasoning operations. As illustrated in Figure 3, HeteQA covers 9
semantically diverse domains and encompasses 5
principal categories of tabular operations. Together,
HeteQA constitutes a structurally diverse and semantically broad resource for advancing question
answering over heterogeneous documents.


**5** **Experiments**

**5.1** **Experimental Settings**

**5.1.1** **Datasets.**

We assess the performance of TableRAG on our
curated HeteQA, as well as multiple established
benchmarks spanning two settings:

**HybridQA** (Chen et al., 2020) A multi-hop QA
dataset involving both tabular and textual information. For our evaluation, we only retain data cases
with tables containing more than 100 cells.

**WikiTableQuestion** (Pasupat and Liang, 2015b)
A TableQA dataset spanning diverse domains. The
queries necessitate a range of data manipulation
operations, including comparison, aggregation, etc.

**5.1.2** **Implementation Details**

In the text retrieval process, we employ the BGEM3 series models (Chen et al., 2024a,b). During
recall, we retain the top 30 candidates, from which
the top 3 are subsequently selected via reranking.
To manage large inputs, the text is chunked into
segments of 1000 tokens, with a 200-token overlap
between consecutive chunks. The iterative loop is
bounded by a maximum of 5 iterations. For backbone LLMs, we utilize Claude-3.5-Sonnet as a representative closed-source LLM, while DeepseekV3, Deepseek-R1 (Guo et al., 2025), and Qwen-2.572B (Yang et al., 2024) serve as the open-source
counterparts. A consistent backend is maintained
for all modules in the online iterative reasoning
process. We use accuracy as the evaluation metric,
assessed by Qwen-2.5-72B, which yields a binary
score of 0 or 1. The prompt is shown in Appendix

F.

**5.1.3** **Baselines**

We evaluate the performance of TableRAG by
benchmarking it against three distinct baseline
methodologies: (1) Direct answer generation with
LLMs. (2) NaiveRAG, which processes tabular
data as linearized Markdown formatted texts and

subsequently applies a standard RAG pipeline. (3)
React (Yao et al., 2023), a prompt based paradigm
to synergize reasoning and acting in LLMs with
external knowledge sources. (4) TableGPT2 (Su
et al., 2024) employs a Python-based execution
module to generate code (e.g., Pandas ) for answer
derivation within a simulated environment. The

detailed implementation of these baseline methods
is provided in Appendix C.


5

**HybridQA** **WikiTQ** **HeteQA**
**Method** **Backbone**

                        -                        - **Single-Source** **Multi-Source** **Overall**

Direct Claude-3.5 9.84 6.21 10.68 8.65 10.00

DeepSeek-R1 24.42 12.20 3.40 13.46 6.77
DeepSeek-V3 14.75 10.39 6.80 28.85 14.19
Qwen-2.5-72b 11.47 7.37 4.85 12.50 7.42

NaiveRAG Claude-3.5 20.28 82.60 33.20 40.35 34.54

DeepSeek-V3 26.56 75.40 33.60 45.61 35.85
Qwen-2.5-72b 22.62 66.33 23.07 36.84 25.66

ReAct Claude-3.5 43.38 69.81 26.40 44.44 29.60

DeepSeek-V3 38.36 63.40 21.14 47.39 26.07
Qwen-2.5-72b 37.38 53.80 16.94 35.71 20.47

TableGPT2 9.51 63.40 35.60 16.67 32.24

TableRAG Claude-3.5 47.87 84.62 44.94 40.74 44.19

DeepSeek-V3 47.87 80.40 43.32 51.85 44.85
Qwen-2.5-72b 48.52 78.00 37.65 43.96 38.82

Table 1: Performance of TableRAG compared to baseline models across multiple benchmarks, measured by accuracy.
"Multi-Source" indicates questions requiring both tabular and textual information, while “Single-Source” refers to
questions relying on only one source type.



**5.2** **Main Result**

The main results across different LLMs backbones

are presented in Table 1. Several key observations emerge: (1) The ReAct framework demonstrates advantages over naive RAG on multi-source
data, but exhibits degraded performance on singlesource data that requires tabular reasoning. This
can be attributed to context insensitivity during
multi-turn reasoning. Queries decomposed into
multiple sub-tasks, such as filtering or aggregation, suffering from information incompleteness or
error propagation. (2) TableGPT2 yields acceptable results solely on single-source queries, such
as WikiTQ, underscoring its limited capacity for
handling multi-source queries. This reflects a lack
of generalizability in heterogeneous information
environments. (3) TableRAG surpasses all baselines, achieving at least a 10% improvement over
the strongest alternative. Notably, it performs robustly across both single-source and multi-source
data. This performance gain is attributed to the incorporation of symbolic reasoning, which enables
effective adaptation to heterogeneous documents.
Moreover, the consistency in performance across
different LLM backbones underscores TableRAG’s

architectural generality and compatibility with a
broad range of backbones.


Figure 4: Ablation study on HybridQA and HeteQA
benchmarks based on DeepSeek-V3 backbone.

**5.3** **Ablation Study**

To elucidate the relative importance of each component within the TableRAG framework, we evaluate
the full architecture against three ablated variants:
(1) w/o Context-Sensitive Query Decomposition,
where query decomposition is performed without
conditioning on retrieved table schema. (2) w/o
SQL Execution, which replaces the SQL programming and execution module with the markdown
table format. (3) w/o Textual Retrieval, which
operates solely through table-based SQL execution, without leveraging textual resources such as
Wikipedia documents. The results are summarized
in Figure 4. All the modules contribute to the overall performance of TableRAG, though their relative







6

|Error Distribution|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>ableRAG (Qwen-72b)<br>eRAG (DeepSeek-V3)|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>ableRAG (Qwen-72b)<br>eRAG (DeepSeek-V3)|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>ableRAG (Qwen-72b)<br>eRAG (DeepSeek-V3)|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>ableRAG (Qwen-72b)<br>eRAG (DeepSeek-V3)|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>ableRAG (Qwen-72b)<br>eRAG (DeepSeek-V3)|
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>ableRAG (Qwen-72b)<br>eRAG (DeepSeek-V3)|||||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>ableRAG (Qwen-72b)<br>eRAG (DeepSeek-V3)|||||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>ableRAG (Qwen-72b)<br>eRAG (DeepSeek-V3)|||||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>ableRAG (Qwen-72b)<br>eRAG (DeepSeek-V3)|||||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>ableRAG (Qwen-72b)<br>eRAG (DeepSeek-V3)|||||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>ableRAG (Qwen-72b)<br>eRAG (DeepSeek-V3)|||||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>ableRAG (Qwen-72b)<br>eRAG (DeepSeek-V3)|||||


Figure 5: Comparison of the execution iterations on
HeteQA between TableRAG, ReAct and TableGPT2.

impact varies across benchmarks. On HybridQA,
document retrieval proves particularly critical, due
to its emphasis on the extraction of entity-centric
or numerical cues. Conversely, for HeteQA, SQL
execution proves more influential, as the queries
involve nested operations that benefit from SQLbased symbolic reasoning. These findings highlight
the complementary design of TableRAG’s textual
retrieval and program-executed reasoning compo
nents.

**6** **Efficiency**

We evaluate the efficiency of TableRAG by examining the distribution of its execution iterations,
as illustrated in Figure 5. Execution lengths are
grouped into four categories: fewer than 3 steps,
3–5 steps, exactly 5 steps, and more than 5 steps.
Among the evaluated methods, TableGPT2 demonstrates the highest average number of execution
steps, with a modal value centered around five.
In contrast, TableRAG consistently requires fewer
steps, resolving approximately 63.55% of instances
in fewer than five steps and an additional 30.00%
precisely within five, with only a marginal proportion of cases remaining unsolved under the given
iteration constraints. While ReAct exhibits a com
parable distribution in execution steps, its overall
performance remains markedly inferior to that of
TableRAG. These results suggest that TableRAG
achieves both superior efficiency in execution and
outstanding reasoning accuracy. It is attributed to
the incorporation of SQL-based tabular reasoning.

**7** **Analysis**

We provide a comprehensive analysis of TableRAG
in this section, with additional results presented in
Appendix D.


Figure 6: Error analysis of TableRAG, TableGPT2 and
ReAct with DeepSeek-V3 and Qwen-2.5-72b as backbones on HeteQA.

**7.1** **Error Analysis**

In addition to evaluating the overall performance
of TableRAG against established baselines, we performed a detailed error analysis to characterize the
nature of prediction failures. Broadly, the incorrect
outputs fall into two primary categories: (1) reasoning failures, attributable to errors in SQL execution
or flawed intermediate query decomposition, and
(2) task incompletion, typically manifesting as refusals to answer or termination upon exceeding the
maximum iteration limit. The prediction distribution is shown in Figure 6. Notably, TableGPT2 exhibits the highest frequency of such failures, largely
due to its limited capacity to integrate contextual
cues from the wiki documents. This constraint

frequently results in the model either explicitly refusing to respond or acknowledging its inability
to do so. In contrast, ReAct, which lacks mechanisms for context-aware query decomposition and
code execution simulation, often engages in unnecessarily elaborate reasoning steps for problems
that could be addressed via a single structured inquiry. TableRAG demonstrates the lowest failure
rate among the methodologies assessed. Its consistent ability to yield valid responses within five
iterations highlights the efficacy of its design —
particularly its use of context-aware query decomposition and selective SQL-based execution planning.

**7.2** **Prediction across Domains**

Figure 7 presents a comparative evaluation of
TableRAG, instantiated with various backbone
LLMs, against the ReAct framework across various domains. The results reveal that TableRAG

consistently outperforms ReAct in the majority of


7

|Military<br>Geography<br>Entertainment<br>S<br>TableRAG DeepSeek<br>ReAct DeepSeek V3|Col2|Transportation<br>10 20 30 40 50 60 70 Sports<br>Technology<br>ociety<br>-V3 TableRAG Qwen<br>ReAct Qwen|
|---|---|---|
|Military<br>Geography<br>Entertainment<br>S<br>TableRAG DeepSeek<br>ReAct DeepSeek V3|TableRAG DeepSeek<br>ReAct DeepSeek V3|-V3 TableRAG<br>ReAct Qw|


Figure 7: Performance distribution of TableRAG and
ReAct across different domains.

domains, demonstrating its effectiveness in heterogeneous document question answering. Only
certain domains, such as Culture, exhibit comparatively weaker performance on TableRAG with
Qwen backbone. A closer inspection of the data
distribution suggests that this degradation may stem
from the sparsity of domain-specific instances.

**8** **Related Work**

**8.1** **Retrieval Augmented Generation**

Retrieval-Augmented Generation (RAG) has
emerged as a robust paradigm for mitigating hallucination (Zhang et al., 2023a) and enhancing the
reliability of Large Language Models (LLMs) generated responses (Lewis et al., 2020; Guu et al.,
2020). The RAG approaches retrieve from the
knowledge base, and the most relevant document
chunks are subsequently incorporated into the generation process (Gao et al., 2023; Zhu et al., 2024;
Borgeaud et al., 2022). However, this straightforward retrieval process often yields noisy chunks
that may lack critical details, thereby diminishing
the quality of the subsequent generation. Recent
advancements have thus focused on task-adaptive
retrieval mechanisms. Notable frameworks in this

regard include Self-RAG (Asai et al., 2023), RQRAG (Chan et al., 2024), etc. Despite these innovations, RAG still faces challenges when dealing
with heterogeneous contexts (Satpute et al., 2024).

**8.2** **Table Reasoning via Large Language**

**Models**

Table reasoning refers to the development of a system that provides responses to user queries based
on tabular data (Lu et al., 2025). The mainstream
approaches to table reasoning can be broadly classified into two categories. The first category revolves around leveraging LLMs through prompt


engineering. For instance, Tab-CoT (Jin and Lu,
2023) applies chain-of-thought (CoT) reasoning
to establish a tabular structured reasoning process.
Similarly, Chain-of-Table (Wang et al., 2024) extends the CoT methodology to the tabular setting,
enabling a multi-step reasoning process for more
complex table-based queries. The second category
involves utilizing programs to process tabular data.
Tabsqlify (Nahid and Rafiei, 2024) employs a textto-SQL approach to decompose tables into smaller,
contextually relevant sub-tables. DATER (Ye et al.,
2023a) adopts a few-shot prompting strategy to
reduce large tables into more manageable subtables, using a parsing-execution-filling technique
that generates intermediate SQL queries. BINDER
(Cheng et al., 2022; Zhang et al., 2023b) integrates
both Python and SQL code to derive answers from
tables. InfiAgent-DABench (Hu et al., 2024) utilizes an LLM-based agent that plans, writes code,
interacts with a Python sandbox, and synthesizes
results to solve table-based questions.

**9** **Conclusion**

We address the limitations of existing RAG approaches in handling heterogeneous documents
that combine textual and tabular data. Current approaches compromise the structural integrity of tables, resulting in information loss and degraded performance in global, multi-hop reasoning tasks. To
overcome these issues, we introduce TableRAG, an
SQL-driven framework that integrates textual understanding with precise tabular manipulation. To
rigorously assess the capabilities of our approach,
we also present a new benchmark HeteQA. Experimental evaluations across public datasets and
HeteQA reveal that TableRAG significantly outperforms existing baseline approaches.

**Limitations**

While TableRAG demonstrates strong performance,
several limitations merit consideration: 1. The ef
fectiveness of TableRAG is closely tied to the capabilities of the underlying LLMs. Our implementation leverages high-capacity models such as Claude,
DeepSeek-v3, and Qwen-72B-Instruct, which possess strong generalization abilities. Smaller models
that lack specialized instruction tuning may exhibit a marked degradation in performance. This
suggests that achieving competitive results may necessitate substantial computational resources. 2.
The HeteQA benchmark is restricted to English.


8

This limitation arises from the difficulty in curating
high-quality heterogeneous sources across multiple
languages. As a result, cross-lingual generalization
remains unexplored. In future work, we aim to
extend HeteQA to a multilingual setting, thereby
broadening the applicability and robustness of our
evaluation framework.

**References**

Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2023. Self-rag: Learning to
retrieve, generate, and critique through self-reflection.
_arXiv preprint arXiv:2310.11511_ .

Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.
Improving language models by retrieving from trillions of tokens. In _International conference on ma-_
_chine learning_, pages 2206–2240. PMLR.

Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo,
Wei Xue, Yike Guo, and Jie Fu. 2024. Rq-rag: Learning to refine queries for retrieval augmented generation. _arXiv preprint arXiv:2404.00610_ .

Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu
Lian, and Zheng Liu. 2024a. Bge m3-embedding:
Multi-lingual, multi-functionality, multi-granularity
text embeddings through self-knowledge distillation.
_arXiv preprint arXiv:2402.03216_ .

Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu
[Lian, and Zheng Liu. 2024b. Bge m3-embedding:](https://arxiv.org/abs/2402.03216)
[Multi-lingual, multi-functionality, multi-granularity](https://arxiv.org/abs/2402.03216)
[text embeddings through self-knowledge distillation.](https://arxiv.org/abs/2402.03216)
_Preprint_, arXiv:2402.03216.

Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong,
[Hong Wang, and William Yang Wang. 2020. Hy-](https://doi.org/10.18653/v1/2020.findings-emnlp.91)
[bridQA: A dataset of multi-hop question answering](https://doi.org/10.18653/v1/2020.findings-emnlp.91)
[over tabular and textual data. In](https://doi.org/10.18653/v1/2020.findings-emnlp.91) _Findings of the Asso-_
_ciation for Computational Linguistics: EMNLP 2020_,
pages 1026–1036, Online. Association for Computational Linguistics.

Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu
Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong,
Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer,
et al. 2022. Binding language models in symbolic
languages. _arXiv preprint arXiv:2210.02875_ .

Darren Edge, Ha Trinh, Newman Cheng, Joshua
Bradley, Alex Chao, Apurva Mody, Steven Truitt,
Dasha Metropolitansky, Robert Osazuwa Ness, and
Jonathan Larson. 2024. From local to global: A
graph rag approach to query-focused summarization.
_arXiv preprint arXiv:2404.16130_ .

Paulo Finardi, Leonardo Avila, Rodrigo Castaldoni, Pedro Gengo, Celio Larcher, Marcos Piau, Pablo Costa,


and Vinicius Caridá. 2024. The chronicles of rag:
The retriever, the chunk and the generator. _arXiv_
_preprint arXiv:2401.07883_ .

Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen
Wang. 2023. Retrieval-augmented generation for
large language models: A survey. _arXiv preprint_
_arXiv:2312.10997_ .

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,
Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,
Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. _arXiv preprint arXiv:2501.12948_ .

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented
language model pre-training. In _International confer-_
_ence on machine learning_, pages 3929–3938. PMLR.

Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli
Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing
Xu, Ming Zhu, et al. 2024. Infiagent-dabench: Evaluating agents on data analysis tasks. _arXiv preprint_
_arXiv:2401.05507_ .

Ziqi Jin and Wei Lu. 2023. Tab-cot: Zero-shot tabular
chain of thought. _arXiv preprint arXiv:2305.17812_ .

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. _Advances in Neu-_
_ral Information Processing Systems_, 33:9459–9474.

Weizheng Lu, Jing Zhang, Ju Fan, Zihao Fu, Yueguo
Chen, and Xiaoyong Du. 2025. Large language
model for table processing: A survey. _Frontiers of_
_Computer Science_, 19(2):192350.

Md Mahadi Hasan Nahid and Davood Rafiei. 2024.
Tabsqlify: Enhancing reasoning capabilities of
llms through table decomposition. _arXiv preprint_
_arXiv:2404.10150_ .

Panupong Pasupat and Percy Liang. 2015a. Compositional semantic parsing on semi-structured tables.
_arXiv preprint arXiv:1508.00305_ .

Panupong Pasupat and Percy Liang. 2015b. Compositional semantic parsing on semi-structured tables.
_arXiv preprint arXiv:1508.00305_ .

Ankit Satpute, Noah Gießing, André Greiner-Petter,
Moritz Schubotz, Olaf Teschke, Akiko Aizawa, and
Bela Gipp. 2024. Can llms master math? investigating large language models on math stack exchange.
In _Proceedings of the 47th international ACM SIGIR_
_conference on research and development in informa-_
_tion retrieval_, pages 2316–2320.

Hesam Shahrokhi, Amirali Kaboli, Mahdi Ghorbani,
and Amir Shaikhha. 2024. Pytond: Efficient python
data science on the shoulders of databases. In _2024_


9

_IEEE 40th International Conference on Data Engi-_
_neering (ICDE)_, pages 423–435. IEEE.

Aofeng Su, Aowen Wang, Chao Ye, Chen Zhou,
Ga Zhang, Gang Chen, Guangcheng Zhu, Haobo
Wang, Haokai Xu, Hao Chen, et al. 2024. Tablegpt2:
A large multimodal model with tabular data integration. _arXiv preprint arXiv:2411.02059_ .

Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly
Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu
Lee, et al. 2024. Chain-of-table: Evolving tables in
the reasoning chain for table understanding. _arXiv_
_preprint arXiv:2401.04398_ .

An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,
Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,
Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. _arXiv preprint arXiv:2412.15115_ .

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2023.
React: Synergizing reasoning and acting in language
models. In _International Conference on Learning_
_Representations (ICLR)_ .

Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei
Huang, and Yongbin Li. 2023a. Large language
models are versatile decomposers: Decompose evidence and questions for table-based reasoning. _arXiv_
_preprint arXiv:2301.13808_ .

Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei
Huang, and Yongbin Li. 2023b. [Large language](https://doi.org/10.1145/3539618.3591708)
[models are versatile decomposers: Decomposing ev-](https://doi.org/10.1145/3539618.3591708)
[idence and questions for table-based reasoning. In](https://doi.org/10.1145/3539618.3591708)
_Proceedings of the 46th International ACM SIGIR_
_Conference on Research and Development in Infor-_
_mation Retrieval_, SIGIR ’23, page 174–184, New
York, NY, USA. Association for Computing Machin
ery.

Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,
Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,
Yulong Chen, et al. 2023a. Siren’s song in the ai
ocean: a survey on hallucination in large language
models. _arXiv preprint arXiv:2309.01219_ .

Yunjia Zhang, Jordan Henkel, Avrilia Floratou, Joyce
Cahoon, Shaleen Deep, and Jignesh M Patel. 2023b.
Reactable: Enhancing react for table question answering. _arXiv preprint arXiv:2310.00815_ .

Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao
Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and
Tat-Seng Chua. 2021. Tat-qa: A question answering
benchmark on a hybrid of tabular and textual content
in finance. _arXiv preprint arXiv:2105.07624_ .

Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu,
Hangyuan Ji, Zixiang Wang, Tao Sun, Long He,
Zhoujun Li, Xi Zhu, et al. 2024. Realm: Rag-driven
enhancement of multimodal electronic health records
analysis via large language models. _arXiv preprint_
_arXiv:2402.07016_ .


Figure 8: The dataset construction pipeline of HeteQA.

**A** **HeteQA**

**A.1** **Prompt Design**

The templates employed for HeteQA benchmark
construction are presented in Table **??** .

**A.2** **Table Collection**

To enable complex reasoning over tabular structures, we curate a subset of extensive tables from
the Wikipedia-based corpus. Specifically, we retain
only tables containing more than 20 rows and at
least 7 columns. This filtering process reduced the
initial collection from 15,314 tables to 1,345 candi
dates that meet the criteria for structural richness.

To further enhance query diversity and reduce redundancy in the dataset, we apply a deduplication
step based on schema similarity. In cases where
multiple tables share identical column structures —
for example, the entries for 1947 BAA draft and
1949 BAA draft — only a single representative
instance is preserved. This procedure yielded a
final dataset comprising 155 unique tables.

**A.3** **Data Collection**

As illustrated in Figure 8, each data instance is constructed through a three-stage pipeline, facilitated
via the Claude 3.7 Sonnet API [3] .

To construct the dataset, we prompt the LLM
to generate SQL or pandas code associated with
queries conditioned on a provided table schema.
The generated code is then executed against the
table to obtain the corresponding answer. Instances
that fail to produce an executable result are discarded. To promote diversity within the dataset
and avoid redundancy, we eliminate any instances
exhibiting duplicate queries or identical answers.
This filtering is performed using regular expressions to detect lexical or semantic repetition. Additionally, we discard cases where the execution
result is ambiguous, such as queries seeking the topranked entity when multiple candidates are tied.
Following automated filtering, all remaining instances undergo a manual verification process. Two
human annotators independently assess each query,

3 [https://www.anthropic.com/claude/sonnet](https://www.anthropic.com/claude/sonnet)


10

the corresponding code, and the resulting answer.
In cases of inconsistency or error, annotators revise
the SQL or Python code and correct the associated
answer to ensure accuracy and coherence.

**B** **Guidelines for HeteQA Annotations**

**B.1** **Annotator Profiles**

The two annotators are volunteers whose native lan
guage is Chinese and who possess fluent English
proficiency. Both annotators are professional software engineers with substantial experience in SQL
and Python programming.

**B.2** **Guidelines for Answer Verification**

**Objective**
Your task is to verify whether the provided answer
correctly and completely addresses the natural
language query, based on the associated code and
table. You will also identify and handle ambiguous
or non-unique cases.

**Annotation Steps**
1. Verify the Answer
Read the natural language query and examine the
accompanying Excel table, SQL statements, and
Python (pandas) code. Verify whether the output
aligns with the semantics of the original query and
the underlying data. To this end, you are permitted to execute multiple operations on the provided
Excel files.
2. Correction Tasks (if necessary)
If the answer is incorrect, modify both the code and
the answer so that they correctly fulfill the query.
Ensure the modified code is minimal, clean, and
logically sound.
3. Ambiguity and Tie Cases
In instances where the query yields no unique resolution - whether due to tied outcomes, under specified conditions, or semantic ambiguity in the formulation - you can employ principled strategies to
ensure meaningful processing:

- Option A: Modify the query to resolve the ambiguity (e.g., make clarification).

- Option B: If the ambiguity is irreparable or the answer depends on arbitrary choices, discard the case.

**Additional Guidelines**

Maintain consistency between the query, code, and
answer. Ensure your corrections do not introduce
new ambiguities or assumptions not grounded in
the table or query.


**B.3** **Guidelines for Document Reference**

**Objective**
Your task is to add an additional reasoning hop to
the original query using the provided Wikipedia
entities and content. This will help transform the
original query into a more complex, multi-hop
question that requires deeper reasoning.

**Input Data**

Each data case consists of:

- Original Query

- Answer

- A set of Wikipedia entities relevant to the query
or the answer

- Corresponding Wikipedia content for each entity

**Task Overview**

For each data case, check if the original answer is
mentioned in the provided Wiki entities:

- If not mentioned or ambiguous (e.g., "Jordan"
for both basketball star and sports brand), do not
change the query and answer. Just mark the case
as “No modification needed”.

- If the answer entity exists in the Wikipedia content,
perform the following steps:
1. Identify key factual descriptions about the
answer entity from its Wiki content.
2. Add a reasoning hop to the original query that
leads to the answer via this key fact.
3. Generate two candidate (Query, Answer)
pairs by rewriting the query to incorporate this
extra reasoning step and updating the answer
accordingly.

**Example**
Original Query: Which album takes first place on
the Billboard leaderboard in 2013?

Original Answer: ArtPop
Wiki Entity: ArtPop (album)
A key description: It was released on November 6,
2013, by Streamline and Interscope Records.
Modified Query Candidates:
Who released the album that takes the first place
on the Billboard leaderboard in 2013? → Answer:

Streamline and Interscope Records

**C** **Implementation Details of Baselines**

This section provides a detailed account of the implementation procedures for the baseline methodologies to ensure a fair and reproducible evaluation.


11

**C.1** **ReAct**

For the ReAct framework, we preprocess tabular
data by converting it into markdown-formatted
plain text. To ensure consistency in experimental conditions, we adopt the same chunking and
retrieval configurations as those employed in our
TableRAG model. We build upon the publicly
available ReAct implementation [4] . The framework
addresses user queries through an iterative reasoning loop consisting of Thought, Action, and
Observation steps, culminating in a final answer
via the Finish operation. The max iteration is set
to 5, the same as TableRAG.

**C.2** **TableGPT2**

We evaluate TableGPT2 using its officially released
TableGPT Agent implementation [5] . As specified
in its API documentation, we provide both the document content and the tabular data as inputs to
the HumanMessage class, ensuring adherence to the
intended usage of the model:

1 from typing import TypedDict
2 from langchain_core.messages import

HumanMessage
3
4 class Attachment(TypedDict):
5 """ Contains at least one dictionary
with the key filename."""
6 filename: str

7

8 attachment_msg = HumanMessage(
9 content="",
10 # Please make sure your iPython
kernel can access your filename.
11 additional_kwargs ={ "attachments": [
Attachment(filename="titanic.csv")
]},
12 )

**D** **Analysis**

We present a series of auxiliary analyses conducted
on both our proposed benchmark, H ETE QA, and
the publicly available H YBRID QA dataset. These
analyses offer further insight into the behavior and
limitations of TableRAG beyond the primary evaluation metrics.

**D.1** **Analysis on HybridQA**

We extend our investigation to the H YBRID QA
dataset by examining the performance distribution
on data domains and conducting a detailed error
analysis, using D EEP S EEK  - V 3 as the backbone
model. The results, summarized in Figures 10

4 [https://github.com/ysymyth/ReAct](https://github.com/ysymyth/ReAct)
5 [https://github.com/tablegpt/tablegpt-agent](https://github.com/tablegpt/tablegpt-agent)


|Col1|Prediction Distribution|
|---|---|
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>bleRAG (Qwen-72b)<br>RAG (DeepSeek-V3)<br>0 50|Prediction Distribution|
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>bleRAG (Qwen-72b)<br>RAG (DeepSeek-V3)<br>0 50||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>bleRAG (Qwen-72b)<br>RAG (DeepSeek-V3)<br>0 50||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>bleRAG (Qwen-72b)<br>RAG (DeepSeek-V3)<br>0 50||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>bleRAG (Qwen-72b)<br>RAG (DeepSeek-V3)<br>0 50||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>bleRAG (Qwen-72b)<br>RAG (DeepSeek-V3)<br>0 50||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>bleRAG (Qwen-72b)<br>RAG (DeepSeek-V3)<br>0 50||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>bleRAG (Qwen-72b)<br>RAG (DeepSeek-V3)<br>0 50||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>bleRAG (Qwen-72b)<br>RAG (DeepSeek-V3)<br>0 50||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>bleRAG (Qwen-72b)<br>RAG (DeepSeek-V3)<br>0 50||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>bleRAG (Qwen-72b)<br>RAG (DeepSeek-V3)<br>0 50||
|TableGPT2<br>ReAct (Qwen-72b)<br>eAct (DeepSeek-V3)<br>bleRAG (Qwen-72b)<br>RAG (DeepSeek-V3)<br>0 50|100 150 200 250 3|


Figure 9: Prediction distribution of TableRAG,
TableGPT2 and ReAct on HybridQA.











Figure 10: Model performance of different domains on
HybridQA.

and 9, reveal patterns consistent with those observed on H ETE QA. This parallel further validates
the generality of our observations across HeteQA
settings.

**D.2** **HyperParameter Analysis**

To elucidate the influence of the top- _k_ retrieval
parameter on the performance of TableRAG, we
undertook a systematic sensitivity analysis. While
our main experimental setup fixed _k_ = 3, we expanded our investigation on the HeteQA dataset,
utilizing DeepSeek-V3 as the retrieval backbone,
and varied _k_ across the set 1 _,_ 3 _,_ 5 . The resulting




|HybridQA<br>HeteQA|Col2|
|---|---|
|||
|||
|||
|1 3 5|1 3 5|


Figure 11: Hyperparameter top- _k_ analysis of TableRAG
on HeteQA.


12

**table** List_of_Australian_films_of_2012_0

**query** Who wrote and starred the comedy film released in the second half of 2012 (July-December)
that had the highest number of cast members in the List of Australian films of 2012?

**sql_query** Which comedy film released in the second half of 2012 (July-December) had the highest
number of cast members in the List of Australian films of 2012?

**sql**

1 **SELECT**
2 title,
3 LENGTH(cast_subject_of_documentary) - LENGTH(
4 REPLACE(
5 cast_subject_of_documentary, ’,’,
6 ’’

7 )
8 ) + 1 **AS** cast_count
9 **FROM**

10 ‘list_of_australian_films_of_2012_0_sheet1 ‘

11 **WHERE**

12 genre **LIKE** ’%Comedy%’
13 **AND** (
14 release_date **LIKE** ’%July%’
15 **OR** release_date **LIKE** ’%August%’
16 **OR** release_date **LIKE** ’%September%’
17 **OR** release_date **LIKE** ’%October%’
18 **OR** release_date **LIKE** ’%November%’
19 **OR** release_date **LIKE** ’\ufffdcember%’
20 )
21 **ORDER BY**

22 cast_count **DESC**

23 **LIMIT**
24 1;

**sql_ans** Kath & Kimderella

**answer** Riley, Turner, and Magda Szubanski

Table 2: An example of HeteQA.

13

performance metrics are illustrated in Figure 11.

Our observations reveal distinct behaviors across

benchmarks with respect to the choice of _k_ . Notably, HybridQA exhibits superior performance
at higher _k_ values. This effect is plausibly attributable to the lack of deduplication within its
tables, whereby overlapping or similar information
from multiple tables or Wikipedia documents contributes constructively during retrieval. In aggregate, setting _k_ = 3 strikes an effective balance,
yielding robust performance across both benchmarks while maintaining computational efficiency.
This efficiency gain arises from the direct relationship between the top- _k_ retrieval size and the subsequent LLM context length, underscoring the practical importance of this hyperparameter in optimizing the trade-off between accuracy and resource
consumption.

**E** **Check List**

**Harmful information And Privacy** We propose
a new RAG solution to address multi-hop problems
related to heterogeneous data, without involving
any harmful information or privacy. In addition,
we provide a heterogeneous benchmark containing
tables and texts sourced from Wikipedia. All of
the data is publicly available, contains no personal
information, and involves no harmful content.

**License and Intend** We provide the license we
used here:

Claude 3.5 Sonnet ( [https://www.anthropic.](https://www.anthropic.com/legal/aup)
[com/legal/aup)](https://www.anthropic.com/legal/aup)

Qwen2.5-72B-Instruct ( [https://huggingface.](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE)
[co/Qwen/Qwen2.5-72B-Instruct/blob/](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE)
[main/LICENSE)](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE)

DeepSeek-V3 ( [https://huggingface.co/](https://huggingface.co/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL)
[deepseek-ai/DeepSeek-V3/blob/main/](https://huggingface.co/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL)
[LICENSE-MODEL)](https://huggingface.co/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL)

TableGPT Agent (Apache License 2.0) and
TableGPT2-7B ( [https://huggingface.](https://huggingface.co/tablegpt/TableGPT2-7B/blob/main/LICENSE)
[co/tablegpt/TableGPT2-7B/blob/main/](https://huggingface.co/tablegpt/TableGPT2-7B/blob/main/LICENSE)
[LICENSE)](https://huggingface.co/tablegpt/TableGPT2-7B/blob/main/LICENSE)

Our use of these existing artifacts was consistent
with their intended use.

**Documentation of the artifacts** We propose
a novel Retrieval-Augmented Generation (RAG)

14


framework that integrates traditional document retrieval with structured data querying via SQL, aiming to enhance performance in table-related question answering tasks. The framework comprises
two fundamental stages: offline database construction and online interactive reasoning. Compared to
conventional document-centric RAG approaches,
our architecture offers additional reliable SQL execution results—when table fragments are involved
in the retrieval process. This supplementary source
mitigates the limitations of generative models in
handling structured tabular data.
To facilitate a more comprehensive evaluation
of our framework, we further construct a heterogeneous benchmark named HeteQA. This benchmark
aims to evaluate the capability to handle multi-hop
reasoning tasks across heterogeneous documents.
The benchmark instances are initially generated
by LLMs, and then rigorously validated by experienced programmers and database administrators to
ensure correctness and realism.

**F** **Prompts**

The prompts for TableRAG are presented in this
section.


.



15

16

17

18

