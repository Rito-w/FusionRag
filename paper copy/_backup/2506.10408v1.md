## **Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic** **Retrieval-Augmented Generation for Industry Challenges**

**Jintao Liang** [1], **Gang Su** [2], **Huifeng Lin** [3], **You Wu** [3], **Rui Zhao** [5] _[,]_ [6] and **Ziyue Li** [4] _[∗]_

1 Beijing University of Posts and Telecommunications
2 University of Georgia
3 South China University of Technology
4 Technical University of Munich, University of Cologne
5 SenseTime Research
6 Qingyuan Research Institute, Shanghai Jiaotong University
ljt2021@bupt.edu.cn, _{_ gangsuedu,huifeng.work,wuyouscut _}_ @gmail.com, zhaorui@sensetime.com,
zlibn@wiso.uni-koeln.de


**Abstract**

Retrieval-Augmented Generation (RAG) has
emerged as a powerful framework to overcome the
knowledge limitations of Large Language Models
(LLMs) by integrating external retrieval with
language generation. While early RAG systems
based on static pipelines have shown effectiveness
in well-structured tasks, they struggle in real-world
scenarios requiring complex reasoning, dynamic
retrieval, and multi-modal integration. To address
these challenges, the field has shifted toward
_Reasoning Agentic RAG_, a paradigm that embeds
decision-making and adaptive tool use directly
into the retrieval process. In this paper, we present
a comprehensive review of Reasoning Agentic
RAG methods, categorizing them into two primary
systems: _predefined reasoning_, which follow
fixed modular pipelines to boost reasoning, and
_agentic reasoning_, where the model autonomously
orchestrates tool interaction during inference.
We analyze representative techniques under both
paradigms, covering architectural design, reasoning
strategies, and tool coordination. Finally, we
discuss key research challenges and propose future
directions to advance the flexibility, robustness, and
applicability of reasoning agentic RAG systems.
Our collection of the relevant researches has been
[organized into a Github Repository.](https://github.com/ByebyeMonica/Reasoning-Agentic-RAG)

**1** **Introduction**

Large Language Models (LLMs) [ Singh, 2023; Zhao _et_
_al._, 2023; Zhu _et al._, 2024 ] have demonstrated remarkable
capabilities in natural language understanding and generation, enabling a wide array of applications from opendomain question-answer (QA) to task-specific dialogue systems. However, LLMs rely on static training data, making them prone to hallucinations and limiting their abil
_∗_ Corresponding author.


ity to provide accurate, up-to-date information in dynamic
or knowledge-intensive tasks [ Rawte _et al._, 2023; Zhang
_et al._, 2023; Huang _et al._, 2025 ] . Retrieval-Augmented
Generation (RAG) [ Chen _et al._, 2024; Lewis _et al._, 2020;
Gao _et al._, 2023 ] has attracted significant attention as a
promising approach to overcome the knowledge limitations
of LLMs resulting from static pretraining. By integrating relevant information from external knowledge bases or search
engines, RAG enhances factual accuracy and broadens the
model’s temporal and domain coverage [ Zhao _et al._, 2024;
Li _et al._, 2024a ] . Traditional RAG methods have demonstrated strong performance when queries are well-formed and
the necessary information is readily available in the retrieved

context.

Despite the effectiveness of basic RAG methods, they often
struggle when applied to real-world, industrial-scale applications involving complex and heterogeneous data. For example,
in multi-document scenarios, relevant information is spread
across sources, requiring not just retrieval but also coherent
synthesis [ Wang _et al._, 2025; Wang _et al._, 2024b ] . Naively
concatenating retrieved passages can lead to fragmented or
contradictory responses, particularly in domains like legal or
biomedical QA where multi-hop reasoning is critical. Additionally, most RAG systems are limited to text-only processing
and cannot natively handle multi-modal inputs such as tables,
charts, or images [ Ma _et al._, 2024; Yu _et al._, 2025 ] . This limits
their ability to operate in data-rich environments like enterprise
intelligence, scientific reporting, or technical support, where
visual and structured data play a central role [ Lin _et al._, 2023a;
Yu _et al._, 2024].

To address these limitations of basic RAG in handling complex, real-world tasks, recent research has turned to _Agentic_
_RAG_ [ Ravuru _et al._, 2024 ], a paradigm that tightly integrates
retrieval with reasoning and decision-making. Unlike static
pipelines, Agentic RAG treats retrieval not as a one-off preprocessing step, but as a dynamic, context-sensitive operation guided by the model’s ongoing reasoning process. This
reasoning-centric perspective is crucial for applications that
demand multi-step problem solving, adaptive information acquisition, and tool-assisted synthesis. Within this paradigm, as

**Predefined Reasoning**

Design Follow

Human LLM

**Agentic Reasoning**

Tool Calling


Reasoning



Retrieved Information

Figure 1: Overview of two major types of reasoning Agentic Systems.

shown in Figure 1, two major types of reasoning agentic systems have emerged based on how control and decision-making
are handled: _predefined reasoning_, which follow structured,
rule-based plans with fixed pipelines to boost reasoning for
retrieval and generation; and _agentic reasoning_, where the
model actively monitors its reasoning process and determines
when and how to retrieve or interact with external tools. These
two workflows form the basis of _Reasoning Agentic RAG_,
which unifies structured and autonomous approaches for more
intelligent, context-aware retrieval-augmented reasoning.
**Predefined reasoning** adopts structured and modular RAG
pipelines where the retrieval and reasoning steps are explicitly
designed, following fixed control pipeline. These workflows
typically decompose tasks into discrete components such as
query reformulation, document retrieval, re-ranking, and answer synthesis, executed in a linear or orchestrated fashion.
In general, predefined reasoning spans several architectural
variants: _route-based_ methods selectively trigger retrieval
based on context or model uncertainty, such as low confidence scores or ambiguous intermediate outputs [ Wang _et al._,
2024a ] ; _loop-based_ methods enable limited iteration through
retrieval-feedback cycles, supporting multiple rounds of refinement [ Asai _et al._, 2023; Yang _et al._, 2024b ] ; _tree-based_
methods organize information hierarchically to support structured exploration [ Sarthi _et al._, 2024; Hu _et al._, 2025 ] ; and
_hybrid-modular_ frameworks compose specialized modules
into a flexible but still rule-driven workflow [ Jeong _et al._, 2024;
Gao _et al._, 2024 ] . These workflows prioritize control and
modularity, making them suitable for tasks requiring efficient
computation and customization. However, their reasoning
remains constrained by predesigned execution paths, limiting
flexibility in evolving and open-ended tasks.
**Agentic reasoning** repositions the LLM as an active decision maker, that autonomously orchestrates retrieval and tool


use throughout the reasoning process. Instead of executing
a fixed plan, the model identifies knowledge gaps, formulates queries, retrieves external information via tools such
as search engines or APIs, and integrates the retrieved contents into an evolving solution. This dynamic interplay of
reasoning and tool use enables the system to tackle complex,
multi-turn tasks that require iterative refinement and adaptive
information synthesis. There are two primary methods for implementing agentic reasoning. The first is _prompt-based_ methods, which leverages the in-context reasoning and instruction~~f~~ o ~~ll~~ ow ~~i~~ ng capa ~~biliti~~ es o ~~f~~ pretrained LLMs [ Yao _et al._, 2023;
Press _et al._, 2023; Li _et al._, 2025a ] . In this setting, the model
is guided by carefully crafted prompts or embedded control
tokens that instruct it when to retrieve, what actions to take,
and how to integrate external information. These methods require no additional training, making them lightweight and
adaptable across tasks. The second paradigm is _training-_
_based_ methods, where models are explicitly optimized through
reinforcement learning, to determine when and how to invoke external tools [ Jiang _et al._, 2025; Jin _et al._, 2025;
Zheng _et al._, 2025 ] . This paradigm enables more fine-grained
and strategic tool usage, enabling models to learn long-term
planning and develop retrieval policies tailored to complex
tasks. Owing to its autonomy and adaptability, agentic reasoning has shown strong performance in open-domain QA,
scientific reasoning, and multi-stage decision-making scenarios.
**Perspective of Cognitive Science - System 1 and System**
**2:** To further contextualize predefined and agentic reasoning within the dual-process theory of cognition—commonly
referred to as System 1 and System 2 thinking [ Yang _et al._,
2024a; Li _et al._, 2025b ] — we can draw an analogy between
these RAG paradigms and human cognitive modes.

  - Predefined reasoning resembles System 1 thinking: fast,
structured, and efficient, relying on predefined heuristics
and modular workflows that mirror habitual or rule-based
cognition. While this enables rapid execution and predictable behavior, it often lacks the flexibility to adapt
beyond its design.

  - In contrast, agentic reasoning aligns more closely with
System 2 thinking: slow, deliberative, and adaptive. Here,
the LLM actively engages in reasoning, planning, and
decision-making, dynamically leveraging external tools
and retrieved knowledge to address complex, novel tasks.
This reflective mode allows the model to identify gaps,
reassess strategies, and adjust its behavior—traits characteristic of conscious, analytical human reasoning.

By framing these paradigms through the lens of cognitive
systems, we highlight the trade-off between efficiency and
adaptability, and the growing capacity of agentic RAG to emulate more sophisticated, human-like problem solving. Table 1
aligns predefined and agentic reasoning with the dual-system
theory from cognitive science, illustrating their respective control structures and behavioral characteristics.
The paper systematically reviews and analyzes the current
research approaches and future development paths of Reasoning Agentic RAG, summarizing them into two primary
technical paradigms. The remainder of the paper is organized

**System Type** **Reasoning Workflow** **Description**

System 1 Predefined Reasoning Structured, modular, rule-based execution.
System 2 Agentic Reasoning Autonomous, adaptive, model-driven decision-making.

Table 1: Cognitive system alignment of reasoning workflows.


as follows: Section 2 introduces related work; Section 3 and
Section 4 dive into the two types of reasoning workflows
within Agentic RAG, predefined reasoning and agentic reasoning, respectively. Section 5 outlines future research directions,
and Section 6 concludes the paper.

Figure 2: Distributed Works of Reasoning Agentic RAG.

**2** **Related Work**

**2.1** **Basic RAG**

_Retrieval-Augmented Generation_ (RAG) was introduced to
overcome the static knowledge limitations of LLMs by integrating external retrieval mechanisms during inference [ Chen
_et al._, 2024; Gao _et al._, 2023 ] . Naive RAG methods represent
the earliest implementations, typically using sparse retrieval
techniques like BM25 [ Robertson _et al._, 2009 ] to fetch documents based on keyword overlap [ Ma _et al._, 2023 ] . While
efficient for simple factoid queries, these approaches offered
limited semantic understanding, thus often retrieving noisy
or redundant content and failing to reason across multiple

sources.
The emergence of Advanced RAG and Modular RAG was
aimed at addressing key limitations of the Naive RAG, particularly in terms of retrieval precision, information integration, and system flexibility [ Gao _et al._, 2023 ] . Advanced
RAG improves retrieval quality through techniques such as
dense semantic matching, re-ranking, and multi-hop querying, while also introducing refined indexing strategies like
fine-grained chunking and metadata-aware retrieval. Modular
RAG rethinks the Naive RAG by breaking down the end-toend process of indexing, retrieval, and generation into discrete,
configurable modules. This design allows for greater architectural flexibility and enables system developers to incorporate
diverse techniques into specific stages, such as enhancing retrieval with fine-tuned search modules [ Lin _et al._, 2023b ] . In
response to specific task demands, various restructured and
iterative module designs have also emerged. As a result, modular RAG has increasingly become a dominant paradigm in
the field, supporting both serialized pipeline execution and
end-to-end learning across modular components.


Despite their effectiveness, basic RAG workflows are limited by static control logic and lack the ability to reflect, adapt,
or assess the sufficiency of retrieved information. These constraints reduce their suitability for tasks requiring iterative
reasoning, tool use, or multi-modal integration. Thus, Agentic
RAG has proposed to embed reasoning and decision-making
into the retrieval process. This work focuses on reasoning
Agentic RAG approaches that enable more autonomous and
context-aware information processing.

**2.2** **Reasoning Agentic RAG**

The year 2025 is marked as the year of agentic AI, with applications emerging such as agentic LLMs and so on [ Ruan _et_
_al._, 2023; Kong _et al._, 2024; Zhang _et al._, ]. Recent advances
in RAG have seen a shift from static, rule-driven retrieval
pipelines toward dynamic, reasoning-driven architectures, collectively referred to as _Reasoning Agentic RAG_ . These systems
embed decision-making into the retrieval process, enabling
models to actively determine when, what, and how to retrieve
based on their internal reasoning trajectory. As shown in Figure 3, Reasoning Agentic RAG approaches can be broadly
categorized into two paradigms: _predefined reasoning_ and
_agentic reasoning_ .
Predefined reasoning depends on structured, rule-based
pipelines where the retrieval and reasoning stages are modularized and fixed in advance. These workflows often include
components for query reformulation, document retrieval, reranking, and response generation, coordinated by static control
logic. RAGate [ Wang _et al._, 2024a ] exemplifies route-based
designs, where retrieval is conditionally triggered based on the
context or model confidence, enabling the system to skip unnecessary operations and focus on knowledge-intensive inputs.
Self-RAG [ Asai _et al._, 2023 ] introduces loop-based reasoning
by enabling the model to self-reflect and iteratively refine its
responses, while RAPTOR [ Sarthi _et al._, 2024 ] leverages a
recursive tree structure to hierarchically summarize and organize retrieved content, supporting multi-hop and abstractive
reasoning. Building on these foundations, more advanced
frameworks like Adaptive-RAG [ Jeong _et al._, 2024 ] combine
dynamic routing and retrieval adaptation, enabling models to
select optimal reasoning paths. Modular-RAG [ Gao _et al._,
2024 ] extends this idea by dividing the RAG pipeline into
interoperable modules like retrievers, rerankers and generators, which can be flexibly composed into hybrid workflows.
These designs enabling more flexible orchestration while still
operating under predefined execution paths.
Agentic reasoning empowers the LLM to act as an autonomous agent, dynamically deciding how to interact with
external tools based on its current reasoning state. These
workflows tightly couple reasoning with tool use, enabling
the model to issue retrieval queries, assess results, and itera
Figure 3: A taxonomy of Reasoning Agentic RAG.



|C|RAG|
|---|---|
|R|APTOR|








|Col1|Col2|Col3|Col4|Col5|Col6|C<br>R|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|Predefined<br>Reasoning||||||||||
|Predefined<br>Reasoning||||||||||
|Predefined<br>Reasoning||||||||||
|Predefined<br>Reasoning||||Self-<br>ASK|ReAct|A|daptive- Modular-<br>RAG RAG|||
|Agentic<br>Reasoning||||||||||
|Agentic<br>Reasoning||||||||||
|Agentic<br>Reasoning||||||||||
|Agentic<br>Reasoning|||||Function<br>Calling|||Searc|Searc|
|Agentic<br>Reasoning||||2023|2023|2024|2024|||
|Agentic<br>Reasoning||2022|2022|||||2025|2025|


Figure 4: Illustration of the evolution of Reasoning Agentic RAG.


tively adapt its actions. Two main implementation strategies
have emerged: _prompt-based_ and _training-based_ approaches.
Prompt-based methods leverage the instruction-following abilities of pretrained LLMs to drive agentic behavior without
additional training. For example, ReAct [ Yao _et al._, 2023 ]
interleaves reasoning steps with tool use to guide retrieval
based on emerging knowledge gaps. Other methods like
Self-Ask [ Press _et al._, 2023 ] and Search-o1 [ Li _et al._, 2025a ]
support decomposition into sub-questions or trigger retrieval
mid-generation. Additionally, function calling mechanisms

[ Eleti _et al._, 2023 ] built into commercial LLMs such as GPT
and Gemini offer structured interfaces for tool use, further
enabling prompt-based agentic control. In parallel, trainingbased approaches aim to explicitly teach LLMs to reason
and retrieve in a unified, goal-driven manner by leveraging
reinforcement learning (RL) to optimize tool-use behavior.
DeepRetrieval [ Jiang _et al._, 2025 ] trains models to reformulate
queries by maximizing retrieval metrics. Search-R1 [ Jin _et_
_al._, 2025 ] and R1-Searcher [ Song _et al._, 2025 ] both adopt a


two-stage, outcome-driven RL framework that enables LLMs
to learn when and what to search within a reasoning trajectory.
ReZero [ Dao and Le, 2025 ] incentivizes persistence, rewarding effective retry strategies. DeepResearcher [ Zheng _et al._,
2025 ] pushes further by training agents in open web environments, enabling robust search and synthesis across diverse,
unstructured sources.

**3** **Predefined Reasoning**

Agents and RAG are increasingly integrated in advanced AI
systems. By augmenting LLMs with external knowledge
retrieval, RAG enables agents to ground their reasoning in
relevant information. In turn, agent-based reasoning which
includes planning, tool use and self-reflection, enhances RAG
by guiding the model on what information to retrieve and
how to incorporate it into the reasoning process. This synergy
supports a predefined reasoning, where the agent iteratively
queries external sources (e.g., a local database or web search)
and refines its reasoning based on the retrieved evidence. We

categorize predefined RAG reasoning workflows into four
broad types based on their structural and reasoning characteristics as follows.

**Route-based Approaches:** RAG incorporates dynamic
routing mechanisms that direct queries along different retrieval
or reasoning paths based on predefined conditions—such
as query type, model uncertainty, or confidence estimation—while still operating within a fixed architecture. RAGate

[ Wang _et al._, 2024a ] uses the conversation context and model
confidence to route only those dialogue turns that truly require external knowledge to a RAG process. This ensures
the system can bypass retrieval for straightforward prompts
while invoking it for kno ~~wledge~~ - ~~intensive~~ ~~queries~~, ~~exemplify~~ ing conditional RAG in dialogue. Self-Route [ Li _et al._, 2024b ]
introduced dynamically routes queries to either RAG or LongContext (LC) models based on the model’s confidence-based
routing. This method significantly reduces computation cost
while maintaining performance comparable to LC models.
**Loop-based Approaches:** RAG operates within a feedback
loop that supports multiple rounds of refinement. The system
can self-reflect, critique intermediate outputs, and iteratively
update retrieval inputs to improve generation quality. SelfRAG [ Asai _et al._, 2023 ] is a foundational example of this
controlled reasoning loop. In the Self-RAG workflow, a single
LLM agent engages in self-reflection during generation to
improve its output. Instead of relying on a fixed retrieved context, the model can decide mid-generation to fetch additional
information or to critique its own draft answer. CRAG [ Yan
_et al._, 2024 ] introduced loop-based corrective feedback mechanism into the retrieval process. In the CRAG workflow, a
lightweight retrieval evaluator assigning the confidence scores
about the quality of the retrieved chunks/documents — categorized as correct, incorrect, or ambiguous. When retrieval
quality is deemed suboptimal, the system activates corrective
strategies such as query rewriting or external web search to
gather better evidence. The system refines the retrieved content into a focused context and iteratively improves retrieval
until a satisfactory output is generated.
**Tree-based Approaches:** RAG organizes the retrieval process hierarchically, often using recursive structures such as
trees to support multi-hop reasoning or document summarization. RAPTOR [ Sarthi _et al._, 2024 ] introduces a recursive
tree structure from documents, allowing for more efficient and
context-aware information retrieval. This approach enhances
RAG by creating a summary tree from text chunks, providing
deeper insights and overcoming limitations of short, contiguous text retrieval. MCTS-RAG [ Hu _et al._, 2025 ] integrates
a Monte Carlo Tree Search loop into the RAG process for
complex reasoning tasks. MCTS-RAG dynamically integrates
retrieval and reasoning through an iterative decision-making
process. Unlike standard RAG methods, which typically retrieve information independently from reasoning and thus
integrate knowledge suboptimally, or conventional MCTS reasoning, which depends solely on internal model knowledge
without external facts, MCTS-RAG combines structured reasoning with adaptive retrieval. **Hybrid-modular Approaches:**
RAG in its most flexible form combines routing, looping, reflection, and modular orchestration. Tasks are divided among
specialized components, coordinated by an agent that can dy

Figure 5: A demonstration of Predefined Reasoning.

namically reconfigure the workflow according to the query or
reasoning context. Adaptive-RAG [ Jeong _et al._, 2024 ] extends
the Self-RAG framework by introducing routing mechanisms
that enable dynamic path selection. In addition to allowing the
model to interleave retrieval and generation steps, it equips the
agent with a decision-making router that selects appropriate
retrieval strategies or reasoning pathways based on the query
characteristics or the agent’s own uncertainty. Rather than
simply determining whether to retrieve more information, the
agent can choose which retrieval method to apply, what type
of information to prioritize, or which downstream modules to
engage. Modular-RAG [ Gao _et al._, 2024 ] is the most advanced
incarnation that transform RAG into a LEGO-like modular
framework, breaking the RAG process into an orchestrated
pipeline of specialized modules. Rather than a single agent
handling everything, a Modular-RAG architecture compartmentalizes tasks, e.g., one module for query reformulation,
one for document retrieval, another for ranking or filtering
results, and another for answer synthesis – all chained together
in a composable workflow. The pipeline is composed by an
agent that coordinates modular components, each of which
can be optimized or swapped independently.
This progression of predefine reasoning workflows reflects a
broader shift from static retrieval pipelines to dynamic, agentdriven reasoning systems. Modern predefined reasoning increasingly integrates planning, tool use, and decision-making
components that allow flexible orchestration of retrieval and
reasoning strategies. Rather than predefining rigid retrieval
steps, these systems empower agents to determine what information to seek, how to use it, and when to adapt their
approach—marking a move toward more autonomous and


**Predefined Reasoning**









intelligent knowledge integration. A summary of the representative research works and open-source industrial/enterprise
implementations across these predefined RAG workflow types
is provided in Table 2.

**4** **Agentic Reasoning**

Beyond the predefined reasoning mentioned above, a more
dynamic paradigm has emerged: the _Agentic Reasoning_ . In
this setting, the LLM serves as an autonomous agent that not
only generates text, but also actively manages retrieval. With
advances in reasoning and instruction-following capabilities,
the model can identify knowledge gaps, determine when and
what to retrieve, and interact with external tools such as search
engines or APIs. This tight integration of reasoning and tool
use enables iterative decision-making, enabling the system to
refine its responses based on newly retrieved information. As
a result, agentic reasoning supports more flexible and adaptive
problem-solving, extending RAG beyond basic QA to complex tasks such as scientific inquiry, multi-step reasoning, and
strategic decision-making. Agentic reasoning approaches can
be broadly categorized by how the LLM learns to use tools:

  - **Prompt-Based Approaches:** These methods leverage
the instruction-following, in-context learning and reasoning capabilities of pretrained LLMs, guiding tool use
through carefully crafted prompts or built-in functionalities without additional training.

  - **Training-Based Approaches:** These methods involve
explicitly training LLMs, typically via reinforcement
learning, to learn when and how to interact with external
tools effectively.

A summary of representative agentic reasoning apporaches
and their characteristics is provided in Table 2. The following
sections examine representative frameworks and techniques
within each approach.

**4.1** **Prompt-Based Approaches**

Prompt-based approaches harness the remarkable capabilities
already present in pre-trained LLMs to enable agentic behavior.
Instead of modifying the model’s weights through training,
these methods rely on sophisticated prompting techniques,
few-shot examples or built-in tool interfaces, to guide the
LLM in its interaction with external tools like search engines.
**Function-Calling-Based** : A foundational prompt-based
method for agentic behavior, and one way to implement function calling, is ReAct (Reason+Act) [ Yao _et al._, 2023 ] . ReAct
aims to create a synergy between the reasoning processes
and action-taking capabilities within an LLM. Its core mechanism involves prompting the LLM to generate outputs in an
interleaved sequence of Thought, Action, and Observation.
ReAct typically employs few-shot prompting, providing the
LLM with examples that demonstrate this Thought-ActionObservation trajectory for solving similar tasks. These examples guide the frozen LLM on how to structure its reasoning,
utilize available tools, and progress towards the goal. The
framework demonstrated significant advantages, particularly
in grounding the LLM’s reasoning. By allowing the model to
actively seek and incorporate external information via actions,


ReAct can mitigate the hallucination and error propagation issues sometimes observed in purely internal reasoning methods
like Chain-of-Thought (CoT) [ Wei _et al._, 2023 ] . The explicit
reasoning traces (“Thoughts”) in ReAct enhance the interpretability and transparency of the model’s decision-making.
Within RAG, ReAct offers a natural agentic reasoning pipeline:
the LLM’s ”Thought” process can identify a knowledge gap,
leading to a search ”Action,” with the retrieved results forming the ”Observation” that informs subsequent reasoning. A
related method, Self-Ask [ Press _et al._, 2023 ], encourages stepby-step problem decomposition by prompting the LLM to
generate and answer simpler follow-up questions. These intermediate steps often involve search actions, enabling the model
to gather relevant information before attempting to answer the
main question.
Another prominent prompt-based approach involves leveraging the function calling or tool use capabilities that have
been explicitly built into or fine-tuned into certain LLMs, such
as versions of GPT [ Eleti _et al._, 2023 ], Llama, and Gemini. This feature allows the LLM to interact reliably with
predefined external tools or APIs based on natural language
instructions. Function calling significantly expands the capabilities of LLMs beyond text generation, enabling them to
access real-time, dynamic information, interact with external
systems and databases, automate tasks, and reliably convert
natural language requests into structured API calls or database
queries. In contrast to the more open-ended ”thought-actionobservation” cycle of ReAct, function calling often bypasses
explicit intermediate reasoning steps. The LLM directly identifies the relevant tool and generates the necessary parameters
based on its training to recognize and format specific function
calls. This more direct approach relies on the model’s preexisting knowledge of available tools and their required inputs.
Furthermore, the format and capabilities of the tools accessible via function calling are typically predefined and have
been integrated into the model’s training or prompt design.
For Agentic RAG, function calling provides a straightforward
and structured way for the LLM agent to invoke a search API
when its internal analysis determines that external information
is required to answer a prompt accurately.
**Large Reasoning Model-based** : A growing trend in Agentic RAG workflow involves directly utilizing LLMs that possess inherently strong reasoning capabilities, often referred to
as Large Reasoning Models (LRMs). These models, sometimes developed through techniques like large-scale reinforcement learning (e.g., models analogous to OpenAI’s o1 [ OpenAI _et al._, 2024 ], DeepSeek-R1 [ DeepSeek-AI _et al._, 2025 ] ),
are designed to excel at complex, multi-step reasoning tasks.
The underlying premise is that an LLM with superior intrinsic reasoning abilities will be better equipped to manage the
complexities of an Agentic RAG workflow, including decomposing challenging queries, planning information-gathering
steps, assessing the relevance and utility of retrieved information, and synthesizing knowledge effectively. In essence,
leveraging LRMs within RAG represents a prompt-based agentic strategy where the model’s powerful inherent reasoning
capabilities drive the process, implicitly deciding when and
how to retrieve information to support its complex thought

processes.

**Predefined Reasoning**

**Approach** **Strategy** **Control Type** **Reasoning Complexity** **Code**

RAGate [Wang _et al._, 2024a] Route-based Adaptive Medium [Link](https://github.com/wangxieric/RAGate)
self-RAG [Asai _et al._, 2023] Loop-based Agentic Medium [Link](https://github.com/AkariAsai/self-rag)
CRAG [Yan _et al._, 2024] Loop-based Adaptive Medium [Link](https://github.com/HuskyInSalt/CRAG)
MCTS-RAG [Hu _et al._, 2025] Tree-based Agentic High [Link](https://github.com/yale-nlp/MCTS-RAG)
RAPTOR [Sarthi _et al._, 2024] Tree-based Fixed Medium [Link](https://github.com/parthsarthi03/raptor)
Adaptive-RAG [Jeong _et al._, 2024] Hybrid-modular Adaptive Medium [Link](https://github.com/starsuzi/Adaptive-RAG)
Modular-RAG [Gao _et al._, 2024] Hybrid-modular Fixed Low N/A
DeepSearcher Industry Adaptive Medium [Link](https://github.com/zilliztech/deep-searcher)
RAGFlow Industry Adaptive Medium [Link](https://github.com/infiniflow/ragflow)
Haystack Industry Adaptive Medium [Link](https://github.com/deepset-ai/haystack)
Langchain-Chatchat Industry Adaptive/Agentic Medium [Link](https://github.com/chatchat-space/Langchain-Chatchat)
LightRAG Industry Adaptive Medium [Link](https://github.com/HKUDS/LightRAG)
R2R Industry Agentic High [Link](https://github.com/SciPhi-AI/R2R)
FlashRAG Industry Adaptive Medium [Link](https://github.com/RUC-NLPIR/FlashRAG)

**Agentic Reasoning**

**Approach** **Strategy** **Training environment** **Reward design** **Code**

ReAct [Yao _et al._, 2023] Prompt-based N/A N/A [Link](https://github.com/ysymyth/ReAct)
Self-Ask [Press _et al._, 2023] Prompt-based N/A N/A [Link](https://github.com/ofirpress/self-ask)
Funciton calling [Eleti _et al._, 2023] Prompt-based N/A N/A N/A
Search-O1 [Li _et al._, 2025a] Prompt-based N/A N/A [Link](https://github.com/sunnynexus/Search-o1)
Search-R1 [Jin _et al._, 2025] Training-based Local retrieval system Answer reward [Link](https://github.com/PeterGriffinJin/Search-R1)
R1-Searcher [Song _et al._, 2025] Training-based Local retrieval system Retrieval reward, format reward, answer reward [Link](https://github.com/RUCAIBox/R1-Searcher)
ReZero [Dao and Le, 2025] Training-based Local retrieval system Retrieval reward, format reward, answer reward, retry reward [Link](https://github.com/menloresearch/ReZero)
DeepRetrieval [Jiang _et al._, 2025] Training-based Restricted real-world search engine Retrieval reward, format reward [Link](https://github.com/pat-jj/DeepRetrieval)
DeepResearcher [Zheng _et al._, 2025] Training-based Real-world search engine Format reward, answer reward [Link](https://github.com/GAIR-NLP/DeepResearcher)

Table 2: A summary of Reasoning agentic rag.


However, effectively managing the retrieved context is another significant challenge. LLMs with extremely long context windows can suffer from a ”lost-in-the-middle” problem,
where information presented in the middle of a long input
receives less attention. Furthermore, retrieved documents,
whether in long-context models or standard RAG, often contain verbose, noisy or contradictory content that can disrupt
the coherence of the LLM’s reasoning process. Mitigating
this challenge requires more precise retrieval strategies and
adaptive context management mechanisms. The Search-o1
framework [ Li _et al._, 2025a ] is specifically designed to enhance LRMs by tackling knowledge insufficiency during long,
step-by-step reasoning chains. It integrates two core components: an Agentic RAG Mechanism where the LRM dynamically triggers search queries based on self-assessed knowledge
gaps, and a Reason-in-Documents Module that processes retrieved content to dist ill relevant information into a refined
format, thereby minimizing noise and maintaining the LRM’s
reasoning integrity. Search-o1 exemplifies a sophisticated
prompt-based agentic approach focused on maintaining reasoning integrity in the face of external information retrieval.

**4.2** **Training-Based Approaches**

While prompt-based methods leverage the inherent capabilities of LLMs, their performance in complex tool-use scenarios
can be inconsistent. Achieving highly reliable and optimized
behavior, especially in deciding when and how to interact
with tools like search engines, often benefits from explicit
training. Training-based approaches, particularly those utilizing Reinforcement Learning (RL), enable the LLM agent
to learn sophisticated strategies through trial and error, directly optimizing its actions towards specific goals such as
maximizing retrieval effectiveness or overall task success. RL
enables agents to develop more robust and strategic interaction


patterns than prompting alone.

**Interacting with local retrieval systems** : Search-R1 [ Jin _et_
_al._, 2025 ] tackles a different aspect of agentic search: training
the LLM to autonomously decide when to search and what to
search for during a multi-step reasoning process. It extends
RL-based reasoning frameworks (like DeepSeek-R1) by integrating search engine interaction directly into the learning
loop. In the Search-R1 framework, the search engine is modeled as part of the RL environment. The LLM agent learns a
policy to generate a sequence of tokens that includes both internal reasoning steps (often enclosed in <think> tags) and
explicit triggers for search actions. These triggers are special
tokens, <search> and </search>, which encapsulate the
generated search query. This design allows for flexible, multiturn interactions where the LLM can interleave reasoning,
searching, processing retrieved information (presented within
<information> tags), and further reasoning or searching
as needed. The framework utilizes a simple outcome-based
reward function, typically based on the correctness of the final
answer generated by the LLM (within <answer> tags) compared to a ground truth, avoiding the complexity of designing
intermediate process rewards. A crucial technique employed
is retrieved token masking. During the calculation of the RL
loss (using algorithms like PPO or GRPO [ Shao _et al._, 2024 ] ),
the tokens corresponding to the content retrieved from the
search engine (i.e., within the <information> tags) are
ignored or masked out, which stabilizes the training process.
Search-R1 has shown significant performance improvements
over various RAG baselines on question-answering datasets.
Its core contribution is training the LLM to learn an optimal
policy for interacting with the search engine as an integrated
part of its reasoning flow, enabling dynamic, context-aware
search decisions. The related R1-Searcher [ Song _et al._, 2025 ]
framework also proposes a similar two-stage, outcome-based

**Agentic Reasoning**




Step n


Search Query



Retrieved

~~D~~ ocumen ~~t~~ s





**iterable**

Distilled

Information


Step n+2

Final Step


...



Figure 6: A demonstration of Agentic Reasoning.

RL approach for enhancing search capabilities.
ReZero (Retry-Zero) [ Dao and Le, 2025 ] introduces another
dimension to RL-based agentic search by specifically focusing
on incentivizing persistence. It addresses the common scenario where an initial search query might fail to retrieve the
necessary information, potentially causing the LLM agent to
halt prematurely or generate a suboptimal response. ReZero
aims to teach the agent the value of “trying one more time.”
The framework operates within a standard RL setup (using
GRPO is mentioned) where the LLM interacts with a search
environment. The novelty lies in its modified reward function, which includes a specific component termed reward retry.
This component provides a positive reward signal whenever
the LLM issues a <search> query after the initial search
query within the same reasoning trajectory. Crucially, this
reward for retrying is conditional upon the agent successfully
completing the task, indicated by generating a final answer
enclosed in <answer> tags. This conditionality prevents the
agent from accumulating rewards simply by retrying indefinitely without making progress. By directly rewarding the
act of persistence (when productive), ReZero encourages the
LLM to explore alternative queries or search strategies if the
first attempt proves insufficient. This contrasts with methods
that might only implicitly reward persistence through eventual
task success. ReZero positions itself as complementary to
frameworks like DeepRetrieval; while DeepRetrieval focuses
on optimizing a single refined query, ReZero emphasizes the
value of making multiple retrieval attempts when needed.
**Interacting with real-world search engines** : DeepRe

trieval [ Jiang _et al._, 2025 ] focuses specifically on improving the quality of the search queries generated by the LLM
agent. It frames the task of query generation or rewriting as
an RL problem, training the LLM to transform an initial user
query into a more effective query for downstream retrieval
systems. The core mechanism involves the LLM generating
an augmented or rewritten query based on the input query.
DeepRetrieval employs RL algorithms like Proximal Policy
Optimization (PPO) [ Schulman _et al._, 2017 ] to train this query
generation process. A key innovation lies in its reward signal:
instead of relying on supervised data (e.g., pairs of original and
”gold” rewritten queries), DeepRetrieval uses the performance
of the generated query in the actual retrieval system as the
~~reward~~ . ~~Metrics~~ ~~such~~ ~~as~~ ~~r~~ e ~~ca~~ ll@k, Normalized Discounted Cumulative Gain (NDCG), or evidence-seeking retrieval accuracy
(Hits@N) obtained from executing the generated query against
a restricted real search engine (like PubMed) or document collection are used to provide feedback to the LLM. The model
learns, through trial and error, to generate queries that maximize these retrieval metrics. To structure the generation, the
model often produces reasoning steps within <think> tags
before outputting the final query in an <answer> tag. This
approach offers significant advantages. By directly optimizing
for the end goal (retrieval performance), it bypasses the need
for expensive and potentially suboptimal supervised query
datasets. Compared to other RL methods, DeepRetrieval’s
primary focus is on optimizing the content and formulation of
the search query itself.
DeepResearcher [ Zheng _et al._, 2025 ] pushes the boundaries
of training-based Agentic RAG by moving beyond controlled
environments or static corpora to perform end-to-end RL training directly within real-world web environments. It aims
to equip LLM agents with the capabilities needed for complex, deep research tasks that require navigating the noisy,
unstructured, and dynamic nature of the open web. This addresses a key limitation of many existing agents, whether
prompt-engineered or trained in simulated/static RAG settings, which often struggle with the complexities of real-world
web interaction. The framework employs RL (specifically
GRPO with an F1 score-based reward for answer accuracy
) to train agents that interact with live web search APIs and
browse actual webpages. DeepResearcher utilizes a specialized multi-agent architecture to handle the complexities of
web interaction. This includes a reasoning module, a tool
for invoking web search, and dedicated “browsing agents”
responsible for extracting relevant information from the diverse structures of webpages encountered. Training in this
realistic setting was found to foster several emergent cognitive
behaviors not typically observed in agents trained under more
constrained conditions. These include the ability to formulate
initial plans and dynamically adjust them during the research
process, cross-validate information retrieved from multiple
web sources, engage in self-reflection when retrieved information seems contradictory or insufficient leading to refined
search strategies, and exhibit honesty by declining to provide
an answer when definitive information cannot be found. DeepResearcher demonstrated substantial performance improvements over prompt-engineering baselines and RAG-based RL
agents trained on static corpora, particularly on open-domain

research tasks. The results strongly suggest that end-to-end
training in realistic web environments is crucial for developing robust and capable research agents, moving closer to the
capabilities hinted at by proprietary systems like OpenAI’s
Deep Research [OpenAI, 2025] or Grok’s DeeperSearch.
The progression for the training-based methods, from optimizing the decision process of when and what to query
(Search-R1), to fostering persistence (ReZero), optimizing
query formulation (DeepRetrieval), and managing real-world
research workflows (DeepResearcher) reflects the growing
sophistication of RL in agentic search. It reflects a growing
appreciation that effective information seeking by an agent
involves a confluence of factors: query quality, strategic timing, resilience to failure, and adeptness in navigating realistic
information environments and so on. Future advancements
in RL-based Agentic RAG will likely need to integrate these
facets more holistically, perhaps through more complex reward structures, multi-objective optimization, or architectures
that explicitly model these different dimensions of the search
process, to achieve truly human-like research and problemsolving capabilities.

**5** **Future Research Directions**

**Enhancing tool interaction through advanced configura-**
**tion.** Current agentic reasoning often utilizes search tools with
relatively basic interfaces, primarily focused on generating
text queries. Future work should enable agents to exploit more
advanced configurations offered by external APIs and tools.
This could involve training agents to understand and utilize
options like result filtering (e.g., by date, source type), sorting
criteria, specifying search domains, or interacting with structured databases via complex queries. Granting finer control
would support more targeted, efficient, and strategic retrieval
aligned with task demands.
**Developing finer-Grained and process-oriented reward**
**functions.** Simple outcome-based rewards like exact match
may not offer adequate guidance for complex RAG tasks that
require multi-step reasoning or detailed responses. Future
research should develop fine-grained reward functions that
assess both final answer correctness and intermediate steps
such as document relevance, reasoning coherence, information
cross-validation, and effective problem decomposition. These
signals are vital for training agents to handle queries that
demand more than short factual answers.

**Improving Efficiency in Retrieval.** The approaches mentioned above primarily focus on the accuracy of the final answer, but enhancing the efficiency of the retrieval process itself
is also critical. Agents trained to interact with potentially vast
information sources, must learn to perform retrievals strategically. Future research should focus on techniques that help
agents avoid excessive or unnecessary search queries, select
the most promising sources, and know when sufficient information has been gathered. Developing strategies to prevent
agents from getting stuck in loops of unproductive searching
or performing redundant retrievals is vital for practical and
scalable Agentic RAG.
**Enhancing Generalization and Robustness in Dynamic**
**Environments.** Robust generalization to new queries, un

seen tools (e.g., sparse, dense, or web retrieval), and changing
environments remains a major challenge. While training in realistic conditions (as in DeepResearcher) improves resilience,
agents still struggle with tool failures or shifting knowledge
availability. Future work should explore adaptive training
methodologies and architectures that ensure robust performance in unfamiliar or dynamic settings.
By addressing key areas such as improving agent control
over tools, designing more sophisticated reward signals, increasing efficiency, and enhancing generalization, the field
can move toward building more capable, reliable, and widely
applicable Agentic RAG systems. These advancements are
essential for transitioning agentic AI from research prototypes
to practical systems that can effectively support humans in
complex information tasks.

**6** **Conclusions**

As language models are increasingly deployed in complex,
knowledge-intensive applications, the limitations of static
RAG pipelines have become apparent. Reasoning Agentic
RAG offers a promising path forward by integrating retrieval
with model-driven planning, self-reflection, and tool use. This
paper surveyed the landscape of reasoning workflows within
Agentic RAG, distinguishing between predefined reasoning
with fixed orchestration, and agentic reasoning that enables
dynamic, autonomous decision-making. We reviewed key
methods across both paradigms, highlighting their strengths,
limitations, and use-case applicability. To advance the field,
we identify several crucial directions for future research, including fine-grained reward design, enhanced tool control,
automated data synthesis, and robust training in dynamic environments. These innovations will be essential for realizing
intelligent, context-aware RAG systems capable of addressing
real-world challenges with greater adaptability, transparency,
and reliability.

**References**

[Asai _et al._, 2023] Akari Asai, Zeqiu Wu, et al. Self-rag:
Learning to retrieve, generate, and critique through selfreflection, 2023.

[Chen _et al._, 2024] Jiawei Chen, Hongyu Lin, et al. Benchmarking large language models in retrieval-augmented generation. In _Proceedings of the AAAI Conference on Artifi-_
_cial Intelligence_, volume 38, pages 17754–17762, 2024.

[Dao and Le, 2025] Alan Dao and Thinh Le. Rezero: Enhancing llm search ability by trying one-more-time, 2025.

[DeepSeek-AI _et al._, 2025] DeepSeek-AI, Daya Guo, et al.
Deepseek-r1: Incentivizing reasoning capability in llms via
reinforcement learning, 2025.

[Eleti _et al._, 2023] Atty Eleti, Jeff Harris, et al. Function calling and other api updates, June 2023.

[Gao _et al._, 2023] Yunfan Gao, Yun Xiong, et al. Retrievalaugmented generation for large language models: A survey.
_arXiv preprint arXiv:2312.10997_, 2:1, 2023.

[Gao _et al._, 2024] Yunfan Gao, Yun Xiong, et al. Modular
rag: Transforming rag systems into lego-like reconfigurable
frameworks, 2024.

[Hu _et al._, 2025] Yunhai Hu, Yilun Zhao, et al. Mcts-rag: Enhancing retrieval-augmented generation with monte carlo
tree search, 2025.

[Huang _et al._, 2025] Lei Huang, Weijiang Yu, et al. A survey
on hallucination in large language models: Principles, taxonomy, challenges, and open questions. _ACM Transactions_
_on Information Systems_, 43(2):1–55, 2025.

[Jeong _et al._, 2024] Soyeong Jeong, Jinheon Baek, et al.
Adaptive-rag: Learning to adapt retrieval-augmented large
language models through question complexity, 2024.

[Jiang _et al._, 2025] Pengcheng Jiang, Jiacheng Lin, et al.
Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning,
2025.

[Jin _et al._, 2025] Bowen Jin, Hansi Zeng, et al. Search-r1:
Training llms to reason and leverage search engines with
reinforcement learning, 2025.

[Kong _et al._, 2024] Yilun Kong, Jingqing Ruan, et al. Tptuv2: Boosting task planning and tool usage of large language
model-based agents in real-world industry systems. In
_Proceedings of the 2024 Conference on Empirical Methods_
_in Natural Language Processing: Industry Track_, pages
371–385, 2024.

[Lewis _et al._, 2020] Patrick Lewis, Ethan Perez, et al.
Retrieval-augmented generation for knowledge-intensive
nlp tasks. _Advances in neural information processing sys-_
_tems_, 33:9459–9474, 2020.

[Li _et al._, 2024a] Jiarui Li, Ye Yuan, et al. Enhancing llm
factual accuracy with rag to counter hallucinations: A case
study on domain-specific queries in private knowledgebases. _arXiv preprint arXiv:2403.10446_, 2024.

[Li _et al._, 2024b] Zhuowan Li, Cheng Li, et al. Retrieval augmented generation or long-context llms? a comprehensive
study and hybrid approach, 2024.

[Li _et al._, 2025a] Xiaoxi Li, Guanting Dong, et al. Search-o1:
Agentic search-enhanced large reasoning models, 2025.

[Li _et al._, 2025b] Zhong-Zhi Li, Duzhen Zhang, et al. From
system 1 to system 2: A survey of reasoning large language
models. _arXiv preprint arXiv:2502.17419_, 2025.

[Lin _et al._, 2023a] Weizhe Lin, Jinghong Chen, et al. Finegrained late-interaction multi-modal retrieval for retrieval
augmented visual question answering. _Advances in Neural_
_Information Processing Systems_, 36:22820–22840, 2023.

[Lin _et al._, 2023b] Xi Victoria Lin, Xilun Chen, et al. Radit: Retrieval-augmented dual instruction tuning. In _The_
_Twelfth International Conference on Learning Representa-_
_tions_, 2023.

[Ma _et al._, 2023] Xinbei Ma, Yeyun Gong, et al. Query rewriting in retrieval-augmented large language models. In _Pro-_
_ceedings of the 2023 Conference on Empirical Methods in_
_Natural Language Processing_, pages 5303–5315, 2023.

[Ma _et al._, 2024] Zi-Ao Ma, Tian Lan, et al. Multi-modal
retrieval augmented multi-modal generation: A benchmark, evaluate metrics and strong baselines. _arXiv preprint_
_arXiv:2411.16365_, 2024.



[OpenAI _et al._, 2024] OpenAI, :, et al. Openai o1 system
card, 2024.

[OpenAI, 2025] OpenAI. Deep research system card, February 2025.

[Press _et al._, 2023] Ofir Press, Muru Zhang, et al. Measuring
and narrowing the compositionality gap in language models,
2023.

[Ravuru _et al._, 2024] Chidaksh Ravuru, Sagar Srinivas Sakhinana, et al. Agentic retrieval-augmented generation for time
series analysis. _arXiv preprint arXiv:2408.14484_, 2024.

[Rawte _et al._, 2023] Vipula Rawte, Swagata Chakraborty,
et al. The troubling emergence of hallucination in large language models-an extensive definition, quantification, and
prescriptive remediations. Association for Computational
Linguistics, 2023.

[Robertson _et al._, 2009] Stephen Robertson, Hugo Zaragoza,
et al. The probabilistic relevance framework: Bm25 and
beyond. _Foundations and Trends® in Information Retrieval_,
3(4):333–389, 2009.

[Ruan _et al._, 2023] Jingqing Ruan, Yihong Chen, et al. Tptu:
Task planning and tool usage of large language modelbased ai agents. In _NeurIPS 2023 Foundation Models for_
_Decision Making Workshop_, 2023.

[Sarthi _et al._, 2024] Parth Sarthi, Salman Abdullah, et al. Raptor: Recursive abstractive processing for tree-organized
retrieval, 2024.

[Schulman _et al._, 2017] John Schulman, Filip Wolski, et al.
Proximal policy optimization algorithms, 2017.

[Shao _et al._, 2024] Zhihong Shao, Peiyi Wang, et al.
Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024.

[Singh, 2023] Aditi Singh. Exploring language models: A
comprehensive survey and analysis. In _2023 International_
_Conference on Research Methodologies in Knowledge Man-_
_agement, Artificial Intelligence and Telecommunication En-_
_gineering (RMKMATE)_, pages 1–4. IEEE, 2023.

[Song _et al._, 2025] Huatong Song, Jinhao Jiang, et al. R1searcher: Incentivizing the search capability in llms via
reinforcement learning, 2025.

[Wang _et al._, 2024a] Xi Wang, Procheta Sen, et al. Adaptive
retrieval-augmented generation for conversational systems,
2024.

[Wang _et al._, 2024b] Yu Wang, Nedim Lipka, et al. Knowledge graph prompting for multi-document question answering. In _Proceedings of the AAAI Conference on Artificial_
_Intelligence_, volume 38, pages 19206–19214, 2024.

[Wang _et al._, 2025] Han Wang, Archiki Prasad, et al.
Retrieval-augmented generation with conflicting evidence.
_arXiv preprint arXiv:2504.13079_, 2025.

[Wei _et al._, 2023] Jason Wei, Xuezhi Wang, et al. Chainof-thought prompting elicits reasoning in large language
models, 2023.

[Yan _et al._, 2024] Shi-Qi Yan, Jia-Chen Gu, et al. Corrective
retrieval augmented generation, 2024.

[Yang _et al._, 2024a] Cheng Yang, Chufan Shi, et al. Llm2:
Let large language models harness system 2 reasoning.
_arXiv preprint arXiv:2412.20372_, 2024.

[Yang _et al._, 2024b] Xiao Yang, Kai Sun, et al. Cragcomprehensive rag benchmark. _Advances in Neural In-_
_formation Processing Systems_, 37:10470–10490, 2024.

[Yao _et al._, 2023] Shunyu Yao, Jeffrey Zhao, et al. React:
Synergizing reasoning and acting in language models,
2023.

[Yu _et al._, 2024] Shi Yu, Chaoyue Tang, et al. Visrag: Visionbased retrieval-augmented generation on multi-modality
documents. _arXiv preprint arXiv:2410.10594_, 2024.

[Yu _et al._, 2025] Qinhan Yu, Zhiyou Xiao, et al. Mramgbench: A beyondtext benchmark for multimodal retrievalaugmented multimodal generation. _arXiv preprint_
_arXiv:2502.04176_, 2025.

[Zhang _et al._, ] Bin Zhang, Hangyu Mao, et al. Controlling
large language model-based agents for large-scale decisionmaking: An actor-critic approach. In _ICLR 2024 Workshop_
_on Large Language Model (LLM) Agents_ .

[Zhang _et al._, 2023] Yue Zhang, Yafu Li, et al. Siren’s song
in the ai ocean: a survey on hallucination in large language
models. _arXiv preprint arXiv:2309.01219_, 2023.

[Zhao _et al._, 2023] Wayne Xin Zhao, Kun Zhou, et al.
A survey of large language models. _arXiv preprint_
_arXiv:2303.18223_, 1(2), 2023.

[Zhao _et al._, 2024] Penghao Zhao, Hailin Zhang, et al.
Retrieval-augmented generation for ai-generated content:
A survey. _arXiv preprint arXiv:2402.19473_, 2024.

[Zheng _et al._, 2025] Yuxiang Zheng, Dayuan Fu, et al. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments, 2025.

[Zhu _et al._, 2024] Yizhang Zhu, Shiyin Du, et al. Are large
language models good statisticians? _arXiv preprint_
_arXiv:2406.07815_, 2024.

