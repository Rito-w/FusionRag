## **Leveraging LLM-Assisted Query Understanding** **for Live Retrieval-Augmented Generation**


Guanting Dong
Renmin University of China
Beijing, China
dongguanting@ruc.edu.cn

Yuyao Zhang
Renmin University of China
Beijing, China
2020201710@ruc.edu.cn

**ABSTRACT**

Real-world live retrieval-augmented generation (RAG) systems face
significant challenges when processing user queries that are often noisy, ambiguous, and contain multiple intents. While RAG
enhances large language models (LLMs) with external knowledge,
current systems typically struggle with such complex inputs, as
they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the
robustness and effectiveness of RAG systems in live, open-domain
settings. Omni-RAG employs LLM-assisted query understanding
to preprocess user inputs through three key modules: (1) Deep
Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries ( _e.g._, correcting spelling
errors) and decompose multi-intent queries into structured subqueries; (2) Intent-Aware Knowledge Retrieval, which performs
retrieval for each sub-query from a corpus ( _i.e._, FineWeb using
OpenSearch) and aggregates the results; and (3) Reranking and
Generation, where a reranker ( _i.e._, BGE) refines document selection
before a final response is generated by an LLM ( _i.e._, Falcon-10B)
using a chain-of-thought prompt. Omni-RAG aims to bridge the gap
between current RAG capabilities and the demands of real-world
applications, such as those highlighted by the SIGIR 2025 LiveRAG
Challenge, by robustly handling complex and noisy queries.

**CCS CONCEPTS**

- **Information systems** → **Retrieval models and ranking** .

**KEYWORDS**

Retrieval-Augmented Generation, Query Understanding, Denoising,
Document Ranking

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
_Conference’17, July 2017, Washington, DC, USA_
© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM
[https://doi.org/10.1145/nnnnnnn.nnnnnnn](https://doi.org/10.1145/nnnnnnn.nnnnnnn)


Xiaoxi Li

Renmin University of China
Beijing, China
xiaoxi_li@ruc.edu.cn

Mengjie Deng
Renmin University of China
Beijing, China
dengmengjie_777@163.com

**ACM Reference Format:**

Guanting Dong, Xiaoxi Li, Yuyao Zhang, and Mengjie Deng. 2025. Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented
Generation. In _Proceedings of ACM Conference (Conference’17)._ ACM, New
[York, NY, USA, 6 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn](https://doi.org/10.1145/nnnnnnn.nnnnnnn)

**1** **INTRODUCTION**

The rapid advancement of large language models (LLMs) [ 10, 38, 50 ]
has led to transformative progress across a wide range of natural
language processing tasks [ 25, 52 ]. Nevertheless, when tackling
knowledge-intensive tasks, LLMs still rely solely on their internal
knowledge, which often fail short in factual inconsistency and hallucinations [ 15 ]. To address these issues, researchers have proposed
Retrieval-Augmented Generation (RAG) [ 13 ], which incorporates
external knowledge sources to assist LLMs in content generation,
significantly improving the accuracy and reliability of the outputs.
However, in real-world live RAG applications, user queries are
rarely atomic or single-intent. Instead, they often involve multiple intents, complex structures, and various types of noise [ 6, 21 ].
While existing RAG methods perform well on standard benchmarks,
they typically select simple and noise-free dataset for fine-tuning or
alignment. Consequently, these systems struggle to accurately interpret intent and generate reliable responses when faced with noisy,
ambiguous, and multi-intent queries in open-domain settings.
To advance research in this direction, the SIGIR 2025 LiveRAG
Challenge introduces the first competition specifically designed
to evaluate the real-time problem-solving capabilities of online
RAG systems. The challenge provides all teams with a fixed knowledge corpus (FineWeb) [ 31 ] and a pre-trained language model
(Falcon3-10B-Instruct) [32], while dynamically generating diverse
user queries using a configurable synthetic DataMorgana [ 11 ] simulator to mimic live human query interactions. Participating RAG systems must complete the task within a two-hour time limit, requiring
efficient handling of complex, noisy, and multi-intent queries. Thus,
the core challenge lies in robustly and efficiently understanding
the underlying intents and semantic noise in user queries, posing a
significant hurdle for building practical live RAG systems.
To bridge this gap, we design Omni-RAG, a framework that
leverages the understanding capabilities of LLMs to preprocess user
queries—through denoising and intent decomposition—enhancing
the robustness of RAG systems in real-world online environments.
The framework comprises three key modules:

Conference’17, July 2017, Washington, DC, USA Guanting Dong, Xiaoxi Li, Yuyao Zhang, and Mengjie Deng



- **Deep Query Understanding and Decomposition:** Based on
LLMs’ strengths in language understanding, we apply tailored
prompts to guide the model in preprocessing user queries. This includes rewriting noisy inputs and decomposing complex queries
with multiple intents into clearer, structured sub-queries.

- **Intent-Aware Knowledge Retrieval:** To retrieve comprehensive and relevant supporting information, we use an OpenSearch
system to perform retrieval over the FineWeb corpus for each
rewritten and decomposed sub-query. The retrieved documents
are then aggregated into a unified corpus that captures a broader
semantic context for generation.

- **Reranking and Generation:** Before generation, we apply a
BGE reranker to reorder candidate documents for all sub-queries,
selecting the top-10 most relevant ones. These are then integrated
with the rewritten main query into a chain-of-thought prompt,
which is fed into the Falcon-10B model to generate the final

response.

Experimental results show that Omni-RAG framework achieved a
**Rank-2 overall performance in Session 1 of the SIGIR Liv-**
**eRAG Challenge** . Additionally, we conduct pseudo-labeling experiments on the dry-test set following the official evaluation metrics,
further demonstrating the framework’s effectiveness in terms of
both generation efficiency and factual consistency.

**2** **RELATED WORK**

**Retrieval-Augmented Generation.** Retrieval-Augmented Generation (RAG) [ 13 ] has emerged as a powerful paradigm that incorporates external information or knowledge to enhance the quality,
factuality, and relevance of generated text. Recent efforts [ 5, 7,
23, 24 ] have leveraged RAG to address the challenge of hallucination and improve the performance of LLMs across a range of
tasks. To furthur improve retrieval quality, several post-retrieval
strategies [ 18, 36, 46 ] have been introduced to fill the gap between
retriever and generator, involving re-ranking, refinement and compression. Reranker [ 17, 19, 44 ] reorder the retrieved results from
retriever, enabling better alignment with the information needs of
the LLM. Additionally, some studies [ 2, 43 ] introduce techniques
to mitigate noise in retrieved knowledge documents. To tackle the
problem of long-context limitation, various methods [ 14, 45 ] focus
on compressing retrieved references to fit input length limits and
removing irrelevant content to enhance robustness.
**Query Understanding.** Query Understanding [ 3, 37 ] encompasses a range of techniques aimed at improving the efficiency
and accuracy of retrieval-augmented generation systems in the
pre-retrieval stage, including query rewriting, disambiguation, decomposition and expansion. Recent advancements [ 1, 6 ] have highlighted the pivotal role of LLMs in query understanding in to enhance retrieval quality. Query rewriting [ 26, 29 ] involves reformulating the original query into a version more closely aligned with
the information required for effective retrieval, thereby addressing
the common mismatch between human intent and model interpretation. Query disambiguation [ 20, 27, 28, 33 ] focuses on clarifying
user intent in ambiguous or multi-turn queries by transforming
them into more specific and context-aware search inputs. Query decomposition [ 34, 49, 51 ] breaks down complex queries into simpler


sub-queries to improve retrieval effectiveness and enable comprehensive answer generation. Reasoning-based query decomposition
methods [ 8, 39, 40, 47 ] focus on generating reasoning traces or
plans for solving complex tasks. Query expansion improves retrieval performance by enriching the original query with additional
information derived from internal or external knowledge sources.
Internal expansion methods [ 12, 16, 41, 48 ] focus on enhancing the
original query using parametric knowledge within LLMs, while
external expansion methods [ 30, 35 ] incorporate supplementary
information from external sources such as knowledge bases.

**3** **METHOD**

**Overview.** To ensure robust and high-quality RAG responses, we
introduce the Omini-RAG pipeline, powered by LLMs, as shown in
Figure 1. Given a query, an LLM first performs deep understanding,
including rewriting and decomposition. Retrieval and reranking
are then applied to obtain candidate documents for each sub-intent,
followed by response generation via Falcon-10B. The subsequent
subsections detail each stage of the pipeline.

**3.1** **Problem Definition**

Compared to standard text generation, RAG often follows a _retrieve-_
_then-read_ paradigm [ 9, 22 ], where an additional retriever is introduced to collect external knowledge and enhance the generation
process. Given the input user query be denoted by _𝑞_ . The primary
objective of the RAG system is to generate a comprehensive and
relevant response _𝑅_ . Formally, the system aims to find an optimal
response _𝑅_ [∗] such that:

_𝑅_ [∗] = arg max _𝑃_ ( _𝑅_ | _𝑞,_ K) (1)
_𝑅_

where K represents the available knowledge base or corpus from
which information can be retrieved. The pipeline described below
outlines the steps to approximate this optimal response.

**3.2** **Query Understanding and Decomposition**

Real-world user queries often contain noise, such as spelling errors
or ambiguous phrasing. Directly using such queries for searching
can lead to inaccurate or irrelevant retrieval results. Therefore, we
first employ an LLM for query understanding, rewriting. Let the
original query be _𝑞_ . The rewriting process can be represented as:

_𝑞_ [′] = Rewrite( _𝑞,𝜃_ rewrite ) (2)

where _𝑞_ [′] is the rewritten query, and _𝜃_ rewrite represents the parameters of the LLM fine-tuned for the rewriting task.
After rewriting, although the semantics of the query _𝑞_ [′] become
correct and intuitive, its intent may still be complex or multifaceted.
To further achieve more precise retrieval targets and improve the
recall of relevant documents, we perform query decomposition. In
detail, the rewritten query _𝑞_ [′] is input to an LLM, which outputs
a set of _𝑀_ sub-queries { _𝑞_ 1 [′] _[,𝑞]_ 2 [′] _[, . . .,𝑞]_ _𝑀_ [′] [}] [. The above process can be]
formulated as:

{ _𝑞_ _𝑠_ [′] [}] _𝑠_ _[𝑀]_ =1 [=][ Decompose][(] _[𝑞]_ [′] _[,𝜃]_ [decompose] [)] (3)

where each sub-query _𝑞_ _𝑠_ [′] is designed to target a specific aspect of
the original query. These sub-queries are typically generated in a

Omni-RAG: Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation Conference’17, July 2017, Washington, DC, USA















**Figure 1: The overall pipeline of our Omni-RAG.**


structured format, such as JSON, for easy extraction and processing. This lays a solid foundation for retrieving broader and more
accurate knowledge during the search process.

**3.3** **Intent-Aware Knowledge Retrieval**

To obtain more comprehensive and extensive information, an intuitive approach is to retrieve information for each sub-intent derived
from the decomposition of a complex query. For each decomposed
sub-query _𝑞_ _𝑠_ [′] (where _𝑠_ ∈{ 1 _, . . ., 𝑀_ } ), we utilize a search function
to retrieve the top-K relevant documents from the knowledge base
K. Let _𝐷_ _𝑠_ be the set of documents retrieved for sub-query _𝑞_ _𝑠_ [′] :

_𝐷_ _𝑠_ = Search( _𝑞_ _𝑠_ [′] _[,]_ [ K] _[, 𝐾]_ [)][ =][ {] _[𝑑]_ _[𝑠,]_ [1] _[,𝑑]_ _[𝑠,]_ [2] _[, . . .,𝑑]_ _𝑠,𝐾_ [}] (4)

where _𝑑_ _𝑠,𝑘_ is the _𝑘_ -th document retrieved for sub-query _𝑞_ _𝑠_ [′] . The
initial set of retrieved documents, _𝐷_ retrieved, is then formed by
taking the union of the documents retrieved for all sub-queries,
_𝐷_ retrieved = [�] _𝑠_ _[𝑀]_ =1 _[𝐷]_ _[𝑠]_ [. This ensures a broad coverage of information]
related to the different facets of the original query.

**3.4** **Reranking and Generation:**

**Reranking:** To balance the redundant retrieval results introduced
by sub-queries, reranking and filtering of the information are essential. After obtaining the initial set of retrieved documents _𝐷_ retrieved,
we employ a reranking model to refine the selection and order of
these documents. We use a sophisticated reranking model, such as
BGE-reranker-large, for this purpose. The reranker computes a relevance score between the original query _𝑞_ (or the rewritten query _𝑞_ [′] )
and each document _𝑑_ ∈ _𝐷_ retrieved . Let score( _𝑞,𝑑_ ) be the relevance
score assigned by the reranker. The documents in _𝐷_ retrieved are
then sorted based on these scores in descending order. We select
the top-N documents from this sorted list to form the final set of
context documents, _𝐷_ reranked :

_𝐷_ reranked = { _𝑑_ 1 [∗] _[,𝑑]_ 2 [∗] _[, . . .,𝑑]_ _𝑁_ [∗] [} ⊆] _[𝐷]_ [retrieved] (5)


such that score( _𝑞,𝑑_ _𝑖_ [∗] [) ≥] [score][(] _[𝑞,𝑑]_ _𝑖_ [∗] +1 [)] [ for all] _[ 𝑖]_ [∈{] [1] _[, . . ., 𝑁]_ [−] [1] [}] [, and]
_𝑁_ is a predefined number of documents to be used for generation.
After reranking, we obtained high-quality documents relevant to
the query, providing essential support for accurate generation.
**Generation:** Finally, with the original query _𝑞_ and the top-N
reranked documents _𝐷_ reranked, we use an LLM for generating the
final response _𝑅_ . The LLM is conditioned on both the query and
the contextual information provided by the selected documents:

_𝑅_ = Generate( _𝑞, 𝐷_ reranked _,𝜃_ generate ) (6)

where _𝜃_ generate represents the parameters of the LLM used for
generation. This step aims to synthesize the information from the
retrieved documents into a coherent, accurate, and contextually
appropriate answer to the user’s query.

**3.5** **Pseudo Labeling and Evaluation**

Since the reference data does not contain ground-truth answers,
we propose a pseudo-label generation and consistency evaluation
strategy based on LLMs to support performance assessment and
iterative optimization of RAG systems during development. It is
important to note that the use of pseudo-labels strictly follows
the competition guidelines: they are employed solely for system
evaluation and analysis of answerless samples in dry tests, and are
never used during the actual answer generation process.
Specifically, we first adopt Qwen2.5-7B-Instruct (fewer than 10B
parameters) [ 4 ] as the reference model to generate pseudo answers
by feeding it the input query along with its retrieved documents.
To enable multi-dimensional evaluation, we define two core metrics—“relevance” and “faithfulness”—in accordance with the official
evaluation criteria, and design dedicated prompts for each (below).
Taking the relevance evaluation prompt as an example, it begins
by defining the model’s role, followed by key evaluation points
and scoring guidelines. Additionally, four handcrafted examples
are included to represent different rating levels, which enhance the
model’s contextual understanding and improve scoring consistency.

Conference’17, July 2017, Washington, DC, USA Guanting Dong, Xiaoxi Li, Yuyao Zhang, and Mengjie Deng


**Table 1: Team Rankings of Session 1 of Live RAG Challenge**

**Rank** **Team Name** **Correctness** **Faithful**

1 RMIT-ADMS 1.1993 0.4774

**2** **RUC_DeepSearch (Ours)** **0.9693** **0.3878**
3 Ped100X 0.9289 0.0434

4 PRMAS-DRCA 0.9228 0.4106

5 Hybrid Search w. Graph 0.8751 0.3158
6 BagBag 0.6941 -0.9114
7 UniClustRAG 0.6851 0.4601

8 METURAG 0.6735 0.3253

9 DeepRAG 0.5661 0.0978

10 UiS-IAI 0.5523 0.4337

11 SNU-LDILab 0.5174 0.1030

12 Gravitational Lens 0.3766 -0.9881

The prompt for faithfulness evaluation follows the same structure as
that of relevance. Finally, we employ Falcon-10B to independently
execute both relevance and faithfulness evaluations, generating
pseudo scores for each candidate answer accordingly.


**Table 2: Performance comparison across different top-** _𝑘_ **set-**
**tings using OpenSearch. 5 (sc4) denotes generation using top-**
**5 docs and 4 sampled reasoning paths for self-consistency.**

**Method** **top-** _𝑘_ **Relevance** **Faithfulness**

Avg -1 0 1 2 Avg -1 0 1


Omni-RAG


1 140 4 2 44 50 62 4 30 66

2 136 6 2 42 50 66 2 30 68

3 146 6 0 36 58 70 0 30 70

4 154 2 0 40 58 76 2 20 78

5 156 4 0 32 64 80 0 20 80


**3.6** **Experiment**

_**Experiment Setup.**_ We strictly follow the requirements of the
LiveRAG competition, using OpenSearch to retrieve from the Falcon
corpus, BGE as the reranker, and Falcon-10B as the generator.
It is worth noting that in Table 2, we experiment with the selfconsistency(sc) strategy [ 42 ], and the metrics are based on in-house


5 (sc4) 170 2 0 24 74 72 0 28 72
Omni-RAG
5 (sc8) 148 4 0 40 56 80 0 20 80

pseudo-relevance and faithfulness scores generated by Qwen2.5
72B-Instruct.

_**Main Result.**_ The main results are presented in the primary
table, where our Team ( _𝑅𝑈𝐶_ _ _𝐷𝑒𝑒𝑝𝑆𝑒𝑎𝑟𝑐ℎ_ ) achieved an overall second place among the 12 participating teams in Session-1.
Notably, compared to the third-place team, Team Ped100X, our
system achieved over a 4% improvement in Correctness and approximately a 34% improvement in Faithfulness. Similarly, compared to
Team BagBag, which ranked fifth overall, our system outperformed
them by around 9% in Correctness and about 7% in Faithfulness.
These results clearly demonstrate the reliability and effectiveness
of our RobustRAG framework.

_**Dry Test Analysis.**_ Dry-Test serves as a representative evaluation setting in our study. To further analyze our RAG performance,
we select 50 samples from the Dry-Test set and use Qwen2.5-7Binstruct ’s answer based on the top-5 documents as references to
evaluate our model. The key findings are as follows.
1. **Performance scales with document count:** As the number

of retrieved documents increases, the Omni-RAG model exhibits
strong scalability in generation quality. Improvements are observed
in both faithfulness and relevance, indicating that the performance
benefits from richer document contexts.
2. **Trade-off in self-consistency path settings:** To enhance
inference stability, we introduced a self-consistency mechanism.
However, the 5 (sc8) configuration did not yield the expected performance gains, suggesting that more reasoning paths do not necessarily lead to better results. Interestingly, the 5 (sc4) setting improved
relevance but led to a moderate decline in faithfulness, highlighting
the need to balance path quantity with generation quality.

**4** **CONCLUSION**

In this paper, we introduces Omni-RAG, a robust and scalable framework that enhances RAG systems through LLM-assisted query
understanding. By integrating deep query decomposition, intentaware retrieval, and reranking-guided generation, Omni-RAG effectively addresses the challenges of complex and noisy queries in live,
open-domain environments. Our approach demonstrates strong
potential for real-world applications, achieving Rank-2 overall performance in Session 1 of the SIGIR LiveRAG Challenge and offering a practical step toward more reliable and intelligent retrievalaugmented systems.

Omni-RAG: Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation Conference’17, July 2017, Washington, DC, USA


**REFERENCES**

[1] Abhijit Anand, Venktesh V, Vinay Setty, and Avishek Anand. 2023. Context
Aware Query Rewriting for Text Rankers using LLM. _CoRR_ abs/2308.16753 (2023).
[https://doi.org/10.48550/ARXIV.2308.16753 arXiv:2308.16753](https://doi.org/10.48550/ARXIV.2308.16753)

[2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.
2023. Self-RAG: Learning to Retrieve, Generate, and Critique through SelfReflection. _CoRR_ [abs/2310.11511 (2023). https://doi.org/10.48550/ARXIV.2310.](https://doi.org/10.48550/ARXIV.2310.11511)
[11511 arXiv:2310.11511](https://doi.org/10.48550/ARXIV.2310.11511)

[3] Hiteshwar Kumar Azad and Akshay Deepak. 2019. Query expansion techniques
for information retrieval: A survey. _Inf. Process. Manag._ 56, 5 (2019), 1698–1735.
[https://doi.org/10.1016/J.IPM.2019.05.009](https://doi.org/10.1016/J.IPM.2019.05.009)

[4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,
Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji
Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng
Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,
Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng
Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang
Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical
Report. _CoRR_ [abs/2309.16609 (2023). https://doi.org/10.48550/ARXIV.2309.16609](https://doi.org/10.48550/ARXIV.2309.16609)
[arXiv:2309.16609](https://arxiv.org/abs/2309.16609)

[5] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin
Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon
Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving Language Models by Retrieving from Trillions of Tokens. In _Proceed-_
_ings of the 39th International Conference on Machine Learning (Proceedings of_
_Machine Learning Research, Vol. 162)_, Kamalika Chaudhuri, Stefanie Jegelka,
Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.). PMLR, 2206–2240.
[https://proceedings.mlr.press/v162/borgeaud22a.html](https://proceedings.mlr.press/v162/borgeaud22a.html)

[6] Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo,
and Jie Fu. 2024. RQ-RAG: Learning to Refine Queries for Retrieval Augmented
Generation. _CoRR_ [abs/2404.00610 (2024). https://doi.org/10.48550/ARXIV.2404.](https://doi.org/10.48550/ARXIV.2404.00610)
[00610 arXiv:2404.00610](https://doi.org/10.48550/ARXIV.2404.00610)

[7] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu,
Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. 2025. Tool-Star:
Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning.
[arXiv:2505.16410 [cs.CL] https://arxiv.org/abs/2505.16410](https://arxiv.org/abs/2505.16410)

[8] Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang
Zhou, and Jingren Zhou. 2025. Self-play with Execution Feedback: Improving
Instruction-following Capabilities of Large Language Models. In _The Thirteenth_
_International Conference on Learning Representations, ICLR 2025, Singapore, April_
_24-28, 2025_ [. OpenReview.net. https://openreview.net/forum?id=cRR0oDFEBC](https://openreview.net/forum?id=cRR0oDFEBC)

[9] Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and
Ji-Rong Wen. 2024. Understand What LLM Needs: Dual Preference Alignment
for Retrieval-Augmented Generation. _CoRR_ [abs/2406.18676 (2024). https://doi.](https://doi.org/10.48550/ARXIV.2406.18676)
[org/10.48550/ARXIV.2406.18676 arXiv:2406.18676](https://doi.org/10.48550/ARXIV.2406.18676)

[10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela
Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien
Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh
Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra,
Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne
Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song,
Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin,
Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson,
Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen,
Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra,
Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan
Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der
Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu
Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park,
Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone,
and et al. 2024. The Llama 3 Herd of Models. _CoRR_ abs/2407.21783 (2024).
[https://doi.org/10.48550/ARXIV.2407.21783 arXiv:2407.21783](https://doi.org/10.48550/ARXIV.2407.21783)

[11] Simone Filice, Guy Horowitz, David Carmel, Zohar Karnin, Liane Lewin-Eytan,
and Yoelle Maarek. 2025. Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana. _arXiv preprint arXiv:2501.12789_ (2025).

[12] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise Zero-Shot
Dense Retrieval without Relevance Labels. In _Proceedings of the 61st Annual_
_Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),_
_ACL 2023, Toronto, Canada, July 9-14, 2023_, Anna Rogers, Jordan L. Boyd-Graber,


and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 1762–1777.
[https://doi.org/10.18653/V1/2023.ACL-LONG.99](https://doi.org/10.18653/V1/2023.ACL-LONG.99)

[13] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,
Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023.
Retrieval-Augmented Generation for Large Language Models: A Survey.
_CoRR_ abs/2312.10997 (2023). [https://doi.org/10.48550/ARXIV.2312.10997](https://doi.org/10.48550/ARXIV.2312.10997)
[arXiv:2312.10997](https://arxiv.org/abs/2312.10997)

[14] Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023. Fidlight: Efficient and effective retrieval-augmented text generation. In _Proceedings_
_of the 46th International ACM SIGIR Conference on Research and Development in_
_Information Retrieval_ . 1437–1447.

[15] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian
Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting
Liu. 2023. A Survey on Hallucination in Large Language Models: Principles,
Taxonomy, Challenges, and Open Questions. _CoRR_ [abs/2311.05232 (2023). https:](https://doi.org/10.48550/ARXIV.2311.05232)
[//doi.org/10.48550/ARXIV.2311.05232 arXiv:2311.05232](https://doi.org/10.48550/ARXIV.2311.05232)

[16] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu,
Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval Augmented Generation. In _Proceedings of the 2023 Conference on Empirical Methods_
_in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023_ .
[Association for Computational Linguistics, 7969–7992. https://aclanthology.org/](https://aclanthology.org/2023.emnlp-main.495)
[2023.emnlp-main.495](https://aclanthology.org/2023.emnlp-main.495)

[17] Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan Ö. Arik. 2025. Long-Context
LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG. In _The Thir-_
_teenth International Conference on Learning Representations, ICLR 2025, Singa-_
_pore, April 24-28, 2025_ . OpenReview.net. [https://openreview.net/forum?id=](https://openreview.net/forum?id=oU3tpaR8fm)
[oU3tpaR8fm](https://openreview.net/forum?id=oU3tpaR8fm)

[18] Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yongkang Wu,
Zhonghua Li, Qi Ye, and Zhicheng Dou. 2025. Hierarchical Document Refinement
[for Long-context Retrieval-augmented Generation. arXiv:2505.10413 [cs.CL]](https://arxiv.org/abs/2505.10413)
[https://arxiv.org/abs/2505.10413](https://arxiv.org/abs/2505.10413)

[19] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael
Bendersky. 2024. Bridging the Preference Gap between Retrievers and LLMs.
[arXiv:2401.06954 [cs.CL]](https://arxiv.org/abs/2401.06954)

[20] Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang.
2023. Tree of Clarifications: Answering Ambiguous Questions with RetrievalAugmented Large Language Models. In _Proceedings of the 2023 Conference on_
_Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, Decem-_
_ber 6-10, 2023_, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for
[Computational Linguistics, 996–1009. https://doi.org/10.18653/V1/2023.EMNLP-](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.63)
[MAIN.63](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.63)

[21] Myeonghwa Lee, Seonho An, and Min-Soo Kim. 2024. PlanRAG: A Plan-thenRetrieval Augmented Generation for Generative Large Language Models as Decision Makers. In _Proceedings of the 2024 Conference of the North American Chapter of_
_the Association for Computational Linguistics: Human Language Technologies (Vol-_
_ume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024_, Kevin Duh,
Helena Gómez-Adorno, and Steven Bethard (Eds.). Association for Computational
[Linguistics, 6537–6555. https://doi.org/10.18653/V1/2024.NAACL-LONG.364](https://doi.org/10.18653/V1/2024.NAACL-LONG.364)

[22] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented
Generation for Knowledge-Intensive NLP Tasks. In _Advances in Neural In-_
_formation Processing Systems 33: Annual Conference on Neural Information_
_Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin (Eds.). [https://proceedings.neurips.cc/paper/2020/hash/](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html)
[6b493230205f780e1bc26945df7481e5-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html)

[23] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian
Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic Search-Enhanced Large
Reasoning Models. _CoRR_ [abs/2501.05366 (2025). https://doi.org/10.48550/ARXIV.](https://doi.org/10.48550/ARXIV.2501.05366)
[2501.05366 arXiv:2501.05366](https://doi.org/10.48550/ARXIV.2501.05366)

[24] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, JiRong Wen, and Zhicheng Dou. 2025. WebThinker: Empowering Large Reasoning
Models with Deep Research Capability. _CoRR_ abs/2504.21776 (2025). [https:](https://doi.org/10.48550/ARXIV.2504.21776)
[//doi.org/10.48550/ARXIV.2504.21776 arXiv:2504.21776](https://doi.org/10.48550/ARXIV.2504.21776)

[25] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and
Zhicheng Dou. 2025. From Matching to Generation: A Survey on Generative
Information Retrieval. _ACM Trans. Inf. Syst._ 43, 3, Article 83 (May 2025), 62 pages.
[https://doi.org/10.1145/3722552](https://doi.org/10.1145/3722552)

[26] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query
Rewriting for Retrieval-Augmented Large Language Models. _CoRR_ abs/2305.14283
[(2023). https://doi.org/10.48550/ARXIV.2305.14283 arXiv:2305.14283](https://doi.org/10.48550/ARXIV.2305.14283)

[27] Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun
Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. 2024. RaFe: Ranking Feedback
Improves Query Rewriting for RAG. _CoRR_ [abs/2405.14431 (2024). https://doi.](https://doi.org/10.48550/ARXIV.2405.14431)
[org/10.48550/ARXIV.2405.14431 arXiv:2405.14431](https://doi.org/10.48550/ARXIV.2405.14431)

[28] Raja Sekhar Reddy Mekala, Yasaman Razeghi, and Sameer Singh. 2024.
EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context

Conference’17, July 2017, Washington, DC, USA Guanting Dong, Xiaoxi Li, Yuyao Zhang, and Mengjie Deng


Learning. In _Proceedings of the 2024 Conference of the North American Chapter_
_of the Association for Computational Linguistics: Human Language Technologies:_
_Short Papers, NAACL 2024, Mexico City, Mexico, June 16-21, 2024_, Kevin Duh, Helena Gómez-Adorno, and Steven Bethard (Eds.). Association for Computational
[Linguistics, 399–432. https://doi.org/10.18653/V1/2024.NAACL-SHORT.35](https://doi.org/10.18653/V1/2024.NAACL-SHORT.35)

[29] Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019.
Multi-hop Reading Comprehension through Question Decomposition and Rescoring. In _Proceedings of the 57th Conference of the Association for Computational_
_Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long_
_Papers_, Anna Korhonen, David R. Traum, and Lluís Màrquez (Eds.). Association
[for Computational Linguistics, 6097–6109. https://doi.org/10.18653/V1/P19-1613](https://doi.org/10.18653/V1/P19-1613)

[30] Jeonghyun Park and Hwanhee Lee. 2024. Conversational Query Reformulation
with the Guidance of Retrieved Documents. _CoRR_ [abs/2407.12363 (2024). https:](https://doi.org/10.48550/ARXIV.2407.12363)
[//doi.org/10.48550/ARXIV.2407.12363 arXiv:2407.12363](https://doi.org/10.48550/ARXIV.2407.12363)

[31] Guilherme Penedo, Hynek Kydlíček, Anton Lozhkov, Margaret Mitchell, Colin A
Raffel, Leandro Von Werra, Thomas Wolf, et al . 2024. The fineweb datasets:
Decanting the web for the finest text data at scale. _Advances in Neural Information_
_Processing Systems_ 37 (2024), 30811–30849.

[32] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei,
and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data Only. In _Advances in Neu-_
_ral Information Processing Systems 36: Annual Conference on Neural Infor-_
_mation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, De-_
_cember 10 - 16, 2023_ . [http://papers.nips.cc/paper_files/paper/2023/hash/](http://papers.nips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html)
[fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html](http://papers.nips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html)

[33] Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong
Xu, Tong Xu, and Enhong Chen. 2024. Large Language Model based Long-tail
Query Rewriting in Taobao Search. In _Companion Proceedings of the ACM on Web_
_Conference 2024, WWW 2024, Singapore, Singapore, May 13-17, 2024_, Tat-Seng
Chua, Chong-Wah Ngo, Roy Ka-Wei Lee, Ravi Kumar, and Hady W. Lauw (Eds.).
[ACM, 20–28. https://doi.org/10.1145/3589335.3648298](https://doi.org/10.1145/3589335.3648298)

[34] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and
Mike Lewis. 2023. Measuring and Narrowing the Compositionality Gap in
Language Models. In _Findings of the Association for Computational Linguistics:_
_EMNLP 2023, Singapore, December 6-10, 2023_, Houda Bouamor, Juan Pino, and
[Kalika Bali (Eds.). Association for Computational Linguistics, 5687–5711. https:](https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.378)
[//doi.org/10.18653/V1/2023.FINDINGS-EMNLP.378](https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.378)

[35] Tao Shen, Guodong Long, Xiubo Geng, Chongyang Tao, Yibin Lei, Tianyi Zhou,
Michael Blumenstein, and Daxin Jiang. 2024. Retrieval-Augmented Retrieval:
Large Language Models are Strong Zero-Shot Retriever. In _Findings of the As-_
_sociation for Computational Linguistics, ACL 2024, Bangkok, Thailand and vir-_
_tual meeting, August 11-16, 2024_, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 15933–15946. [https:](https://doi.org/10.18653/V1/2024.FINDINGS-ACL.943)
[//doi.org/10.18653/V1/2024.FINDINGS-ACL.943](https://doi.org/10.18653/V1/2024.FINDINGS-ACL.943)

[36] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike
Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023. REPLUG: Retrieval-Augmented
Black-Box Language Models. _CoRR_ [abs/2301.12652 (2023). https://doi.org/10.](https://doi.org/10.48550/ARXIV.2301.12652)
[48550/ARXIV.2301.12652 arXiv:2301.12652](https://doi.org/10.48550/ARXIV.2301.12652)

[37] Mingyang Song and Mao Zheng. 2024. A Survey of Query Optimization in Large
Language Models. _CoRR_ [abs/2412.17558 (2024). https://doi.org/10.48550/ARXIV.](https://doi.org/10.48550/ARXIV.2412.17558)
[2412.17558 arXiv:2412.17558](https://doi.org/10.48550/ARXIV.2412.17558)

[38] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,
Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut
Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,
Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,
Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,
Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross
Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,
Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:
Open Foundation and Fine-Tuned Chat Models. _CoRR_ abs/2307.09288 (2023).


[https://doi.org/10.48550/ARXIV.2307.09288 arXiv:2307.09288](https://doi.org/10.48550/ARXIV.2307.09288)

[39] Venktesh V, Sourangshu Bhattacharya, and Avishek Anand. 2023. In-Context Ability Transfer for Question Decomposition in Complex QA. _CoRR_ abs/2310.18371
[(2023). https://doi.org/10.48550/ARXIV.2310.18371 arXiv:2310.18371](https://doi.org/10.48550/ARXIV.2310.18371)

[40] Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan
Natarajan, and Amit Sharma. 2024. Plan × RAG: Planning-guided Retrieval Aug[mented Generation. arXiv:2410.20753 [cs.CL] https://arxiv.org/abs/2410.20753](https://arxiv.org/abs/2410.20753)

[41] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion
with Large Language Models. In _Proceedings of the 2023 Conference on Empirical_
_Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-_
_10, 2023_, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for
[Computational Linguistics, 9414–9423. https://doi.org/10.18653/V1/2023.EMNLP-](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.585)
[MAIN.585](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.585)

[42] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel
Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Language
Models with Self-Generated Instructions. In _Proceedings of the 61st Annual Meeting_
_of the Association for Computational Linguistics (Volume 1: Long Papers), ACL_
_2023, Toronto, Canada, July 9-14, 2023_, Anna Rogers, Jordan L. Boyd-Graber, and
Naoaki Okazaki (Eds.). Association for Computational Linguistics, 13484–13508.
[https://doi.org/10.18653/V1/2023.ACL-LONG.754](https://doi.org/10.18653/V1/2023.ACL-LONG.754)

[43] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-Knowledge Guided
Retrieval Augmentation for Large Language Models. In _Findings of the Association_
_for Computational Linguistics: EMNLP 2023_, Houda Bouamor, Juan Pino, and
Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 10303–
[10315. https://doi.org/10.18653/v1/2023.findings-emnlp.691](https://doi.org/10.18653/v1/2023.findings-emnlp.691)

[44] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham
Neubig. 2023. Learning to Filter Context for Retrieval-Augmented Generation.
[arXiv:2311.08377 [cs.CL]](https://arxiv.org/abs/2311.08377)

[45] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2024. RECOMP: Improving RetrievalAugmented LMs with Context Compression and Selective Augmentation. In _The_
_Twelfth International Conference on Learning Representations, ICLR 2024, Vienna,_
_Austria, May 7-11, 2024_ . OpenReview.net. [https://openreview.net/forum?id=](https://openreview.net/forum?id=mlJLVigNHp)
[mlJLVigNHp](https://openreview.net/forum?id=mlJLVigNHp)

[46] Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li,
and Jing Xiao. 2023. PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter.
In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language_
_Processing, EMNLP 2023, Singapore, December 6-10, 2023_ . Association for Compu[tational Linguistics, 5364–5375. https://aclanthology.org/2023.emnlp-main.326](https://aclanthology.org/2023.emnlp-main.326)

[47] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan,
and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language
Models. In _The Eleventh International Conference on Learning Representations,_
_ICLR 2023, Kigali, Rwanda, May 1-5, 2023_ [. OpenReview.net. https://openreview.](https://openreview.net/forum?id=WE_vluYUL-X)
[net/forum?id=WE_vluYUL-X](https://openreview.net/forum?id=WE_vluYUL-X)

[48] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal,
Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than
Retrieve: Large Language Models are Strong Context Generators. In _The Eleventh_
_International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,_
_May 1-5, 2023_ [. OpenReview.net. https://openreview.net/forum?id=fB0hRu9GZUS](https://openreview.net/forum?id=fB0hRu9GZUS)

[49] Yuyao Zhang, Zhicheng Dou, Xiaoxi Li, Jiajie Jin, Yongkang Wu, Zhonghua
Li, Qi Ye, and Ji-Rong Wen. 2025. Neuro-Symbolic Query Compiler.
[arXiv:2505.11932 [cs.CL] https://arxiv.org/abs/2505.11932](https://arxiv.org/abs/2505.11932)

[50] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,
Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,
Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large
Language Models. _CoRR_ abs/2303.18223 (2023).

[51] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang,
Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. 2023.
Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.
In _The Eleventh International Conference on Learning Representations, ICLR 2023,_
_Kigali, Rwanda, May 1-5, 2023_ [. OpenReview.net. https://openreview.net/forum?](https://openreview.net/forum?id=WZH7099tgfM)
[id=WZH7099tgfM](https://openreview.net/forum?id=WZH7099tgfM)

[52] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large Language Models for Information Retrieval: A Survey. _CoRR_ abs/2308.07107 (2023). [https:](https://doi.org/10.48550/ARXIV.2308.07107)
[//doi.org/10.48550/ARXIV.2308.07107 arXiv:2308.07107](https://doi.org/10.48550/ARXIV.2308.07107)

