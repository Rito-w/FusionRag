## **GLAP: General contrastive audio-text pretraining across domains and** **languages**

_Heinrich Dinkel_ [1] _, Zhiyong Yan_ [1] _, Tianzi Wang_ [1] _, Yongqing Wang_ [1] _, Xingwei Sun_ [1] _, Yadong Niu_ [1] _,_
_Jizhong Liu_ [1] _, Gang Li_ [1] _, Junbo Zhang_ [1] _, Jian Luan_ [1]

1 MiLM Plus, Xiaomi Inc., China

dinkelheinrich@xiaomi.com, zhangjunbo5@xiaomi.com


**Abstract**

Contrastive Language Audio Pretraining (CLAP) is a
widely-used method to bridge the gap between audio and text
domains. Current CLAP methods enable sound and music re
trieval in English, ignoring multilingual spoken content. To
address this, we introduce general language audio pretraining
(GLAP), which expands CLAP with multilingual and multidomain abilities. GLAP demonstrates its versatility by achieving competitive performance on standard audio-text retrieval
benchmarks like Clotho and AudioCaps, while significantly
surpassing existing methods in speech retrieval and classification tasks. Additionally, GLAP achieves strong results on
widely used sound-event zero-shot benchmarks, while simultaneously outperforming previous methods on speech content
benchmarks. Further keyword spotting evaluations across 50
languages emphasize GLAP’s advanced multilingual capabilities. Finally, multilingual sound and music understanding is
evaluated across four languages.
**Index Terms** : contrastive language-audio pretraining, general
pretraining, general audio encoders, large-language models

**1. Introduction**

In the field of computer vision, Contrastive Language-Image
Pretraining (CLIP) [1] represents a significant breakthrough in
extracting efficient representations that can be applied across
various downstream tasks and domains. Similarly, Contrastive
Language-Audio Pretraining (CLAP) [2, 3, 4] bridges text and
audio domains, enabling zero-shot transfer learning i.e., testing the model on novel concepts that is has not seen during
training. Notably, [5] trained on 4.6 Million pairs of audio and
speech data, but has shown poor results for trivial speech classification tasks such as keyword spotting (see MSCLAP-2023 in
Figure 1). While multilingual extensions [6] improved retrieval
performance across eight languages, their approach still lacks
basic speech understanding. CLAP embeddings primarily target sound and music, missing comprehensive speech representation (i.e., spoken language) - a critical aspect of audio processing. While there has been previous work focusing on speechtext embeddings using contrastive learning [3], a general approach that can be used between sound, music and speech domains is still missing. This work proposes general language audio pretraining (GLAP), an extension of previous CLAP works,
aimed at aligning speech content with text, without compromising in sound and music performance. Our experiments demonstrate that GLAP achieves competitive performance in music
and sound retrieval tasks while significantly improving speech
understanding capabilities. GLAP also effectively generalizes
its speech and sound understanding capabilities beyond English.


Figure 1: _GLAP’s retrieval and zero-shot performance. A@T_
_and T@A represent retrieval tasks of Audio-to-Text and Text-to-_
_Audio, respectively, others are zero-shot (number of labels in_
_brackets). Missing baselines were evaluated by the authors._

**2. General language audio pretraining**

To enable speech understanding in CLAP models, which are
trained on sound and music data, one simple solution is to add
speech data to the training dataset. However, as our analysis in
Section 4.1 shows, this approach leads to compromised performance due to the absence of a unified audio encoder. The model
either performs well on sound/music or on speech, but struggles
to excel at both simultaneously. Thus, GLAP has two primary
goals:

1. Deliver a unified encoder framework that maintains high performance for sound, music and speech retrieval tasks to enable alignment across these audio modalities.

2. Enable multilingual search capabilities for sound, music and
speech content.

GLAP is trained with pairs of audio-text samples ( _a, t_ ) in
contrastive fashion. Features from these audio-text pairs are extracted through a pre-trained multi-lingual text encoder E _T_ and
a pre-trained general audio encoder E _A_ :

_e_ _a_ = MLP _A_ (E _A_ ( _a_ )) _,_ _e_ _t_ = MLP _T_ (E _T_ ( _t_ )) _,_

A trainable multi-layer perceptron (MLP) is added to align the
dimensions. Finally, the pair ( _e_ _a_ _, e_ _t_ ) is scored using cosine

distance _s_ = _e_ _a_ _·e_ _[T]_ _t_
_||e_ _a_ _||·||e_ _t_ _||_ [. Unlike previous works, we use the]



















Domain Dataset hours #Pairs #Lang


Speech

Sound





YODAS [9] 400 k 431 M 145
GigaSpeech [10] 10 k 8 M 1
LibriSpeech [11] 960 271 k 1
AISHELL-1 [12] 180 131 k 1

Sound-VECaps _A_ [13] 5200 1.6 M 1+7
Auto-ACD [14] 5200 1.8 M 1+7
AudiosetCaps [15] 5700 2.0 M 1+7
WavCaps [16] 7544 400 k 1+7
AudioCaps [17] 127 49 k 1+7
Clothov2 [18] 35 4884 1+7






~~1)~~





|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|Audio<br>Speech- Sound- Music-<br>Paired Paried Paired<br>Data Data Data<br>Novel in GLAP Dasheng<br>Speech content retrieval<br>Multilingual Speech<br>classification<br>Multilingual sound/music<br>classification<br>Text<br>s(1,1) s(2,1) s(3,1) s(4,<br>Speech<br>MultiLing. Speech s(1,2) s(2,2)s(3,2)s(4,<br>Sonar<br>Audio Caption s(1,3) s(2,3) s(3,3) s(4,<br>Multiling. Caption<br>s(1,4) s(2,4)s(3,4)s(4,|Audio<br>Speech- Sound- Music-<br>Paired Paried Paired<br>Data Data Data<br>Novel in GLAP Dasheng<br>Speech content retrieval<br>Multilingual Speech<br>classification<br>Multilingual sound/music<br>classification<br>Text<br>s(1,1) s(2,1) s(3,1) s(4,<br>Speech<br>MultiLing. Speech s(1,2) s(2,2)s(3,2)s(4,<br>Sonar<br>Audio Caption s(1,3) s(2,3) s(3,3) s(4,<br>Multiling. Caption<br>s(1,4) s(2,4)s(3,4)s(4,|Au|Au|dio|dio|
|Audio<br>Speech- Sound- Music-<br>Paired Paried Paired<br>Data Data Data<br>Novel in GLAP Dasheng<br>Speech content retrieval<br>Multilingual Speech<br>classification<br>Multilingual sound/music<br>classification<br>Text<br>s(1,1) s(2,1) s(3,1) s(4,<br>Speech<br>MultiLing. Speech s(1,2) s(2,2)s(3,2)s(4,<br>Sonar<br>Audio Caption s(1,3) s(2,3) s(3,3) s(4,<br>Multiling. Caption<br>s(1,4) s(2,4)s(3,4)s(4,|s(1,2)|s(1,2)|s(2,2)|s(3,2)|s(4,|
|Audio<br>Speech- Sound- Music-<br>Paired Paried Paired<br>Data Data Data<br>Novel in GLAP Dasheng<br>Speech content retrieval<br>Multilingual Speech<br>classification<br>Multilingual sound/music<br>classification<br>Text<br>s(1,1) s(2,1) s(3,1) s(4,<br>Speech<br>MultiLing. Speech s(1,2) s(2,2)s(3,2)s(4,<br>Sonar<br>Audio Caption s(1,3) s(2,3) s(3,3) s(4,<br>Multiling. Caption<br>s(1,4) s(2,4)s(3,4)s(4,|s(1,3)|s(1,3)|s(2,3)|s(3,3)|s(4,|
|Audio<br>Speech- Sound- Music-<br>Paired Paried Paired<br>Data Data Data<br>Novel in GLAP Dasheng<br>Speech content retrieval<br>Multilingual Speech<br>classification<br>Multilingual sound/music<br>classification<br>Text<br>s(1,1) s(2,1) s(3,1) s(4,<br>Speech<br>MultiLing. Speech s(1,2) s(2,2)s(3,2)s(4,<br>Sonar<br>Audio Caption s(1,3) s(2,3) s(3,3) s(4,<br>Multiling. Caption<br>s(1,4) s(2,4)s(3,4)s(4,|s(1,4)|s(1,4)|s(2,4)|s(3,4)|s(4,|


~~2)~~


~~3)~~


Figure 2: _The GLAP framework. GLAP is trained via con-_
_trastive learning where positive pairs (ψ_ [ _i, j_ ] = 1 _) are shown in_
_blue and negative pairs (ψ_ [ _i, j_ ] = _−_ 1 _) in white, with the added_
_benefits of enabling multilingual speech-content retrieval, on-_
_top of the standard sound/music capabilities._

sigmoid loss [7] as our main training objective _L_, computed as:


_B_
�

_i_


_B_
� log _σ_ � _s_ _[′]_ ( _i, j_ ) _· ψ_ [ _i, j_ ]� _,_ (1)

_j_


MusicCaps [19] 7.3 2640 1+7
Music
Songdescriber [20] 12 360 1+7

Table 1: _Training datasets with duration (hours), audio-text_
_pairs (# Pairs), and languages (# Lang)._ _Music and sound_
_data, labeled in English, are auto-translated into seven other_
_languages via Sonar._

**3.1. Datasets**

**Training** In this work, we integrate a wide range of existing
audio-text datasets, as outlined in Table 1. Our primary speech
training dataset is YODAS [9], a 400k-hour YouTube corpus
labeled mainly via automated speech-to-text pipelines. Due to
its noisy labeling, we supplement it with cleaner English (GigaSpeech, LibriSpeech) and Chinese (AISHELL-1) datasets.
While YODAS covers 145 languages, relying solely on multilingual speech data limits generalization in sound/music retrieval. To address this, we followed [6] and leveraged Sonar [8]
to translate the original English captions of all sound and music datasets into other seven widely spoken languages: German,
Chinese, Catalan, Spanish, Japanese, French, and Dutch. As
shown in Table 1, the training data is skewed towards speech,
leading to poor performance on non-speech tasks. To balance
this, we categorize the data into four groups: sound + music, English speech (GigaSpeech + LibriSpeech + YODAS English), Chinese speech (AISHELL-1 + YODAS Chinese), and
other languages in YODAS. During training, we sample equally
from each group, ensuring a balanced training process.

**Evaluation** We evaluate retrieval performance across seven
test sets, including sound datasets such as Auto-ACD
(ACD)[14], AudioCaps (AC)[17], Clothov2 (Clotho) [18],
music datasets such as MusicCaps (MC) [19] and the
speech datasets LibriSpeech (LS) [11], GigaSpeech [10] and
AISHELL-2 (AIS2) [23]. For zero-shot classification, we
primarily follow [5] and evaluate on ESC-50, FSD50K, UrbanSound8K (US8K), CREMA-D, GTZAN, NSynth instruments, Beijing-Opera, VocalSound, as well as Speech Commands V1/V2 (SCV1/2) [24] and Fluent Speech Commands
(FSC) [25]. All test datasets with the exception of AIS2 are
labeled in English.

**3.2. Evaluation metrics**

In audio-text retrieval tasks, performance is evaluated using recall at rank (R@k), where R@k is 1 if the target item appears
in the top k retrieved items, otherwise 0. We also use mean
average precision at rank 10 (mAP10) for a more comprehensive comparison. For zero-shot inference, accuracy is used for
single-class classification, and mean average precision (mAP)
is used for multi-label classification, as is standard practice [5].


_L_ = _−_ [1]

_B_


_L_ = _−_ [1]


_s_ _[′]_ ( _i, j_ ) = _[s]_ [(] _[i][,]_ _[j]_ [)][ +] _[β]_ _,_ (2)

_τ_


where _σ_ is the sigmoid function, _B_ is the batchsize, _β, τ_

_·_
are learnable parameters, “ ” is the element-wise product, and


_ψ_ [ _i, j_ ] =


1 if _i_ = _j,_

_−_ 1 otherwise _._
�


The primary reason for choosing sigmoid loss over standard cross-entropy is its superior performance with large batch
sizes and datasets, as we observed performance boosts of 1%
to 5% across all retrieval tasks. An overview of the proposed
framework can be seen in Figure 2.

**3. Experimental Setup**

The audio data is preprocessed by resampling all datasets to a
single channel at 16 kHz. The trainable loss parameters _τ, β_
(Equation (2)) are initialized as 0 _._ 07 and _−_ 10 respectively. For
text embeddings, we use the text encoder from Sonar [8] as the
default model, following [6]. We use a batch size of 128 per
GPU across eight A800 GPUs, resulting in an effective batch
size of _B_ = 1024. Embeddings are gathered across all GPUs
before calculating the loss. Each epoch is defined as processing 10,000 batches, with training running for a maximum of 20
epochs. Model training employs an 8-bit Adam optimizer with
a cosine decay scheduler. The learning rate starts at 0, warms
up to 10 _[−]_ [4] over the first two epochs, and decays to 10 _[−]_ [5] over
the remaining training period. Training takes approximately 1.5
days, with the best models typically achieved within the first 15
epochs. The source code and checkpoints are publicly available [1] .

1 github.com/xiaomi-research/dasheng-glap

|Col1|LibriSpeechOther<br>Text-to-Audio Audio-to-Text<br>R@1 R@10 R@1 R@10|MusicCaps<br>Text-to-Audio Audio-to-Text<br>R@5 R@10 R@5 R@10|AISHELL2-Test<br>Text-to-Audio Audio-to-Text<br>R@1 R@10 R@1 R@10|
|---|---|---|---|
|MSCLAP-2022† [2]<br>MSCLAP-2023† [5]<br>L-CLAP† [3]<br>L-CLAP† [3]<br>Speech-Music<br>COLLAP-Roberta [21]<br>COLLAP-GPT [21]<br>BLAT† [4]<br>M2D-Clap† [22]|0.1 0.6 0.0 0.4<br>0.1 0.4 0.1 0.2<br>0.1 0.8 0.1 0.5<br>0.1 0.9 0.1 0.9<br>- - - -<br>- - - -<br>0.0 0.8 0 0.4<br>0.1 0.6 0.0 0.4|4.3 7.2 5.2 7.4<br>14.4 21.7 17.7 25.9<br>17.2 25.5 22.0 31.1<br>16.8 25.4 16.8 25.2<br>15.2 - 9.5 -<br>17.4 - 10.3 -<br>3.2 5.1 3.9 5.8<br>4.3 7.2 5.2 7.4|0.0 0.2 0 0.18<br>0.1 0.2 0.0 0.2<br>0 0.2 0.0 0.2<br>0 0.2 0.0 0.2<br>- - - -<br>- - - -<br>0.0 0.2 0.0 0.2<br>0.1 0.2 0.1 0.3|
|GLAP|93.8 96.8 91.8 94.4|30.3 41.2 15.0 44.4|98.5 99.7 99.1 99.7|


Table 2: _Retrieval results for music and speech datasets._ _[†]_ _indicates evaluation from a public checkpoint. Best in bold; higher is better._


Task Prompt

Speech _{_ label _}_
Music The music in the style of _{_ label _}_ .
Sound The sound of _{_ label _}_ can be heard.

Table 3: _Prompts for zero-shot evaluation._

**3.3. Prompting**

GLAP supports zero-shot inference, allowing the model to generate outputs directly from text prompts without prior training
on specific tasks. In zero-shot scenarios, crafting an effective
text prompt is crucial for achieving optimal performance. This
is particularly important for GLAP, as it lacks a dedicated token
to distinguish between a spoken word (e.g., “cat”) and a sound
event (e.g., “the sound of a cat”). Prompts used in this work are
depicted in Table 3.

**4. Results**

**4.1. Audioencoder investigation**

Given the extensive range of previously explored CLAP methods, one might question why earlier approaches failed to
achieve strong performance in general audio-text pretraining.
In our view, a key limitation lies in the reliance on sound-event
audio encoders for CLAP, which we believe represents a significant bottleneck for performance. In this section, we examine the role of audio encoders in general contrastive audiotext learning. For this purpose, we compare the proposed
training framework using five different audio-encoders, being
Dasheng [26], CED-Base [27], Beats [28], Whisper-Base [29]
and WavLM [30]. Each encoder is selected for its approximately 90M parameters and support for variable-length inputs, a crucial requirement for processing speech recognition datasets—a capability lacking in many CLAP audio encoders [2, 5, 3]. Each encoder is initialized from a publicly
available checkpoint and trained on the same dataset described
in Section 3.1, using the previously described settings (Section 3), with Sonar serving as the default text encoder.
As it can be seen in our results in Table 4, the choice of
audio-encoder is vital to achieve a well-balanced performance
across tasks. Sound-event encoders, such as CED and Beats,
perform well in sound and music retrieval tasks but struggle
with speech-related tasks. Conversely, Whisper and WavLM
excel in speech-related retrieval but underperform in sound
event and music datasets. Dasheng, on the other hand, proves to


be the most versatile choice for general audio encoding, achieving competitive performance across sound, music, and speech
domains. Based on these findings, all subsequent experiments
utilize a _single_ GLAP model with Dasheng as its audio encoder,
without fine-tuning on target datasets.

|Encoder|Sound AC ACD|Music MC|Speech LS-other AIS2|
|---|---|---|---|
|CED-Base<br>Beats<br>Whisper-Base<br>WavLM|58.6 62.0<br>55.1 64.3<br>46.5 52.9<br>36.1 47.5|25.1<br>23.9<br>15.8<br>14.8|87.8 70.6<br>91.8 44.0<br>98.9 99.4<br>99.9 96.3|
|Dasheng|55.8 60.1|20.3|94.8 99.0|



Table 4: _Text-to-Audio retrieval performance across five_
_datasets, categorized by domain. LS-other refers to the test-_
_other subset of LibriSpeech, while AIS2 corresponds to the_
_AISHELL-2 test set. All experiments were conducted using the_
_proposed training dataset and configuration. Values indicate_
_mAP10, where higher scores represent better performance._

**4.2. Sound retrieval results**

The performance of GLAP on the widely used AudioCaps and
Clotho datasets for English sound-event retrieval is presented
in Table 5. GLAP demonstrates strong results on both benchmarks, surpassing other methods in Text-to-Audio retrieval
(R@1) on AudioCaps while maintaining competitive performance on Clotho.

**4.3. Music and speech retrieval results**

For speech retrieval, we select the test-other dataset from Librispeech as a representative in-domain English speech benchmark, while the AISHELL-2 test set serves as an unseen Chi



|Method|AudioCaps<br>Text-to-Audio Audio-to-Text<br>R@1 R@10 R@1 R@10|Clotho<br>Text-to-Audio Audio-to-Text<br>R@1 R@10 R@1 R@10|
|---|---|---|
|BLAT [4]<br>LClap-Large [3]<br>MSCLAP-2022 [2]<br>MSCLAP2023 [5]<br>Wavcaps-CNN14 [16]<br>Wavcaps-HTSAT [16]<br>Auto-ACD [14]<br>T-CLAP [31]<br>MLCLAP [6]<br>Cacophony [32]<br>SoundVECaps [13]|33.3 82.4 40.4 85.7<br>34.2 84.1 43.1 90.1<br>33.5 80.2 47.8 90.7<br>35.6 - 42.5 -<br>34.7 82.5 44.7 86.2<br>39.7 86.1 51.7 90.6<br>39.5 85.4 53.7 91.7<br>39.7 86.9 49.8 91.9<br>40.7 87.8 50.1 92.8<br>41.0 86.4 55.3 92.4<br>41.2 85.3 53.3 93.0|12.3 46.1 13.9 48.2<br>15.3 51.2 20.8 60.0<br>16.2 51.4 23.6 60.3<br>- 15.7 22.9 -<br>21.2 59.4 25.9 65.8<br>20.2 58.8 26.5 67.3<br>15.3 52.1 17.7 52.6<br>17.3 53.6 21.8 57.4<br>18.8 59.0 21.1 62.5<br>20.2 58.8 26.5 67.3<br>- - - -|
|GLAP|41.7 86.1 54.4 91.1|19.4 58.3 21.8 61.5|


Table 5: _Sound event retrieval results compared to baselines._

Sound Music Speech
Method ESC50 FSD50K US8K CD VS GTZAN NS BO SCV1 SCV2 FSC

|BLAT [4]<br>MS-CLAP-2023 [5]<br>L-CLAP [3]<br>Speech-Music<br>L-CLAP [3]<br>M2D-Clap [22]|80.6 31.3 77.3 17.6† 53.9†<br>88.2 40.3 75.0 29.7 69.2<br>89.3 20.2† 72.7† 20.7† 64.5†<br>91.0 21.5† 77.0 18.3† 79.3†<br>75.5 40.8 72.4 17.7 42.3†|10.0† 9.03† 31.4†<br>58.4 47.9 46.6<br>52.3† 29.7† 57.2†<br>47.4† 26.1† 40.2†<br>75.2 23.4 47.0†|3.9† 2.2† 0.<br>16.4⋆ 2.5† 0.<br>3.8† 3.8† 0.<br>3.8† 4.1† 0.<br>3.0† 2.1† 0.|
|---|---|---|---|
|GLAP|88.8 40.9 78.9 20.5 75.1|69.6 31.3 36.5|96.6 95.8 75|



Table 6: _Zero-shot evaluation performance. Results marked with_ _[†]_ _were obtained from a public checkpoint, while underlined entries_
_used the corresponding training dataset and are therefore not truly zero-shot. Entries with_ _[⋆]_ _used the 10-class variant instead of the_
_30-class version employed in this work. Best results are bolded and higher is better._



nese evaluation set. To assess music-related capabilities, we
use the widely adopted MusicCaps dataset [19]. Since some
baseline models operate at different sampling rates (16 kHz
vs. 44/48 kHz), we resample the test datasets accordingly. As
shown in Table 2, our approach significantly outperforms previous methods in both music and speech retrieval. GLAP demonstrates exceptional multilingual retrieval performance, achieving over 93% on the English LibriSpeech-other test set and
98% on the Chinese AISHELL-2 test set. While most com
pared CLAP models offer a competitive performance on the MC
dataset.

**4.4. Zero shot evaluation**

In this section, the zero-shot capabilities our approach are assessed using the prompts provided in Table 3. True zero-shot
evaluation is difficult to determine, as some studies are trained
on the respective datasets [3, 5] using its labels, meaning they
are not strictly zero-shot. As shown in Table 6, our approach
performs similarly to other baselines in sound-event and music classification. However, it excels in the keyword spotting
task, significantly outperforming the baselines with accuracies
of 95.8%, 96.6%, and 75.6% for the SCV1, V2, and FSC
datasets, respectively. Notably, the FSC dataset requires understanding entire sentences, not just individual words, highlighting our model’s ability to align well with spoken content,
as well as detecting single-word keywords (SCV1/2).

**4.5. Multilingual capabilities**

GLAP’s multilingual spoken content capabilities are assessed
through a zero-shot evaluation on the Multilingual Spoken
Words (MSW) Corpus [33]. Results seen in Figure 3 show a
strong zero-shot capability of GLAP on fifty languages. Oriya
and Guarani achieve the best performance at 70% and 65.9%
respectively. Notably, Chinese achieves 57.4% accuracy with
496 keywords, while Russian, containing 15,844 keywords, results in 39.6%. Additionally, we assess GLAP’s multilingual
sound and music understanding by conducting zero-shot evaluations on the ESC-50, US8K and GTZAN datasets. For this,
we use ChatGPT to translate the original (English) labels into
the target language and adjust the prompts (Table 3) accordingly. As shown in Table 7, while performance drops compared
to the English baseline, GLAP remains effective in multilingual sound-event classification. The model performs impressively in Russian (Ru) despite being trained only on Russian
speech-text pairs from YODAS, not music/speech pairs (see
Section 3.1). This suggests effective transfer of multilingual
text-based knowledge to the audio domain.




|roshot Accu<br>.0<br>.9<br>.3<br>.4<br>.3<br>.8<br>.2<br>.3<br>.4<br>.8<br>.6<br>.1<br>.7<br>.9<br>.4<br>.2<br>.8<br>.7<br>.7<br>.6<br>.7<br>.1<br>.6<br>.8<br>.9|raci|es (Top 25<br>or<br>gn<br>ia<br>zh-CN<br>cv<br>sk<br>rm-vallader<br>id<br>rm-sursilv<br>sah<br>ru<br>eo<br>uk<br>ta<br>pt<br>it<br>sl<br>ha<br>cnh<br>lt<br>tr<br>es<br>ro<br>fy-NL<br>br|
|---|---|---|
|10 20 30 40 50 60 70<br>Accuracy (%)|10 20 30 40 50 60 70<br>Accuracy (%)|10 20 30 40 50 60 70<br>Accuracy (%)|

|shot Acc<br>.7<br>.6<br>.5<br>.4<br>.6<br>.4<br>.2<br>.8<br>.1<br>.9<br>.2<br>.2<br>.0<br>.3<br>.6<br>.6<br>.4<br>.4<br>.8<br>.3<br>.6<br>.2<br>8<br>2<br>3|urac|ies (Bottom<br>eu<br>nl<br>mt<br>pl<br>lv<br>en<br>vi<br>ky<br>de<br>cs<br>ka<br>el<br>sv-SE<br>et<br>as<br>tt<br>ca<br>ga-IE<br>cy<br>dv<br>ar<br>fr<br>fa<br>mn<br>rw|
|---|---|---|
|5 10 15 20 25<br>Accuracy (%)|5 10 15 20 25<br>Accuracy (%)|5 10 15 20 25<br>Accuracy (%)|


Figure 3: _Multilingual zero-shot keyword spotting (KWS) per-_
_formance across 50 languages. Only the test set for each lan-_
_guage is used, and the accuracies are reported. The number of_
_keywords (num) for each language is shown on the right._

Language
Data
En De zh-CN Jp Ru

US8K 78.9 74.8 66.1 72.2 49.0

ESC-50 88.8 64.3 71.4 74.3 62.1

GTZAN 69.6 68.3 62.5 63.2 65.3

Table 7: _GLAP’s zero-shot evaluation for multilingual sound_
_and music. Original labels (in gray) are translated into the tar-_
_get language using ChatGPT._

**5. Conclusion**

We introduce GLAP, a versatile language-audio pretraining
framework that enables multilingual and multi-domain modeling of both audio and text. To the best of our knowledge, it is
the first _single_ system to integrate general audio and text embeddings into a unified contrastive framework. GLAP demonstrates
competitive performance on well-established benchmarks like
AudioCaps and Clotho, while surpassing previous methods in
music and speech retrieval tasks. Zero-shot evaluations show
strong results for English sound and music tasks, extending
effectively to other languages. Inference on the Multilingual
Spoken Words dataset highlights robust multilingual capabilities beyond English.

**6. References**

[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and
I. Sutskever, “Learning transferable visual models from natural
language supervision,” in _ICML_, 2021.

[2] B. Elizalde, S. Deshmukh, M. Al Ismail, and H. Wang, “Clap
learning audio concepts from natural language supervision,” in
_ICASSP 2023-2023 IEEE International Conference on Acoustics,_
_Speech and Signal Processing (ICASSP)_ . IEEE, 2023, pp. 1–5.

[3] Y. Wu*, K. Chen*, T. Zhang*, Y. Hui*, T. Berg-Kirkpatrick,
and S. Dubnov, “Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation,” in
_IEEE International Conference on Acoustics, Speech and Signal_
_Processing, ICASSP_, 2023.

[4] X. Xu, Z. Zhang, Z. Zhou, P. Zhang, Z. Xie, M. Wu, and K. Q.
Zhu, “Blat: Bootstrapping language-audio pre-training based on
audioset tag-guided synthetic data,” in _Proceedings of the 31st_
_ACM International Conference on Multimedia_, 2023, pp. 2756–
2764.

[5] B. Elizalde, S. Deshmukh, and H. Wang, “Natural language supervision for general-purpose audio representations,” in _ICASSP_
_2024-2024 IEEE International Conference on Acoustics, Speech_
_and Signal Processing (ICASSP)_ . IEEE, 2024, pp. 336–340.

[6] Z. Yan, H. Dinkel, Y. Wang, J. Liu, J. Zhang, Y. Wang, and
B. Wang, “Bridging language gaps in audio-text retrieval,” in _In-_
_terspeech 2024_, 2024, pp. 1675–1679.

[7] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, “Sigmoid
loss for language image pre-training,” in _Proceedings of the_
_IEEE/CVF International Conference on Computer Vision_, 2023,
pp. 11 975–11 986.

[8] P.-A. Duquenne, H. Schwenk, and B. Sagot, “Sentence-level multimodal and language-agnostic representations,” _arXiv preprint_
_arXiv:2308.11466_, 2023.

[9] X. Li, S. Takamichi, T. Saeki, W. Chen, S. Shiota, and S. Watanabe, “Yodas: Youtube-oriented dataset for audio and speech,”
in _2023 IEEE Automatic Speech Recognition and Understanding_
_Workshop (ASRU)_ . IEEE, 2023, pp. 1–8.

[10] G. Chen, S. Chai, G. Wang, J. Du, W.-Q. Zhang, C. Weng, D. Su,
D. Povey, J. Trmal, J. Zhang _et al._, “Gigaspeech: An evolving,
multi-domain asr corpus with 10,000 hours of transcribed audio,”
_arXiv preprint arXiv:2106.06909_, 2021.

[11] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on public domain audio books,”
in _2015 IEEE international conference on acoustics, speech and_
_signal processing (ICASSP)_ . IEEE, 2015, pp. 5206–5210.

[12] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “Aishell-1: An opensource mandarin speech corpus and a speech recognition baseline,” in _2017 20th conference of the oriental chapter of the inter-_
_national coordinating committee on speech databases and speech_
_I/O systems and assessment (O-COCOSDA)_ . IEEE, 2017, pp.
1–5.

[13] Y. Yuan, D. Jia, X. Zhuang, Y. Chen, Z. Liu, Z. Chen, Y. Wang,
Y. Wang, X. Liu, X. Kang _et al._, “Sound-vecaps: Improving
audio generation with visual enhanced captions,” _arXiv preprint_
_arXiv:2407.04416_, 2024.

[14] L. Sun, X. Xu, M. Wu, and W. Xie, “Auto-acd: A large-scale
dataset for audio-language representation learning,” in _Proceed-_
_ings of the 32nd ACM International Conference on Multimedia_,
2024, pp. 5025–5034.

[15] J. Bai, H. Liu, M. Wang, D. Shi, W. Wang, M. D. Plumbley, W.S. Gan, and J. Chen, “Audiosetcaps: An enriched audio-caption
dataset using automated generation pipeline with large audio and
language models,” _arXiv preprint arXiv:2411.18953_, 2024.

[16] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley, Y. Zou, and W. Wang, “Wavcaps: A chatgpt-assisted weaklylabelled audio captioning dataset for audio-language multimodal
research,” _arXiv preprint arXiv:2303.17395_, 2023.



[17] C. D. Kim, B. Kim, H. Lee, and G. Kim, “Audiocaps: Generating
captions for audios in the wild,” in _North American Chapter of_
_the Association for Computational Linguistics_, 2019. [Online].
Available: https://api.semanticscholar.org/CorpusID:174799768

[18] K. Drossos, S. Lipping, and T. Virtanen, “Clotho: An audio captioning dataset,” in _ICASSP 2020-2020 IEEE International Con-_
_ference on Acoustics, Speech and Signal Processing (ICASSP)_ .
IEEE, 2020, pp. 736–740.

[19] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti,
A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi
_et al._, “Musiclm: Generating music from text,” _arXiv preprint_
_arXiv:2301.11325_, 2023.

[20] I. Manco, B. Weck, S. Doh, M. Won, Y. Zhang, D. Bogdanov,
Y. Wu, K. Chen, P. Tovstogan, E. Benetos _et al._, “The song describer dataset: a corpus of audio captions for music-and-language
evaluation,” _arXiv preprint arXiv:2311.10057_, 2023.

[21] J. Wu, W. Li, Z. Novack, A. Namburi, C. Chen, and
J. McAuley, “Collap: Contrastive long-form language-audio pretraining with musical temporal structure augmentation,” _arXiv_
_preprint arXiv:2410.02271_, 2024.

[22] D. Niizumi, D. Takeuchi, Y. Ohishi, N. Harada, M. Yasuda,
S. Tsubaki, and K. Imoto, “M2d-clap: Masked modeling duo
meets clap for learning general-purpose audio-language representation,” in _Interspeech 2024_, 2024, pp. 57–61.

[23] J. Du, X. Na, X. Liu, and H. Bu, “Aishell-2: Transforming mandarin asr research into industrial scale,” _arXiv preprint_
_arXiv:1808.10583_, 2018.

[24] P. Warden, “Speech Commands: A Dataset for LimitedVocabulary Speech Recognition,” _ArXiv e-prints_, Apr. 2018.

[Online]. Available: https://arxiv.org/abs/1804.03209

[25] L. Lugosch, M. Ravanelli, P. Ignoto, V. S. Tomar, and Y. Bengio, “Speech model pre-training for end-to-end spoken language
understanding,” _arXiv preprint arXiv:1904.03670_, 2019.

[26] H. Dinkel, Z. Yan, Y. Wang, J. Zhang, Y. Wang, and B. Wang,
“Scaling up masked audio encoder learning for general audio classification,” in _Interspeech 2024_, 2024, pp. 547–551.

[27] H. Dinkel, Y. Wang, Z. Yan, J. Zhang, and Y. Wang, “Ced: Consistent ensemble distillation for audio tagging,” in _ICASSP 2024-_
_2024 IEEE International Conference on Acoustics, Speech and_
_Signal Processing (ICASSP)_ . IEEE, 2024, pp. 291–295.

[28] S. Chen, Y. Wu, C. Wang, S. Liu, D. Tompkins, Z. Chen, and
F. Wei, “Beats: Audio pre-training with acoustic tokenizers,”
_arXiv preprint arXiv:2212.09058_, 2022.

[29] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and
I. Sutskever, “Robust speech recognition via large-scale weak supervision,” in _International Conference on Machine Learning_ .
PMLR, 2023, pp. 28 492–28 518.

[30] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li,
N. Kanda, T. Yoshioka, X. Xiao _et al._, “Wavlm: Large-scale selfsupervised pre-training for full stack speech processing,” _IEEE_
_Journal of Selected Topics in Signal Processing_, vol. 16, no. 6,
pp. 1505–1518, 2022.

[31] Y. Yuan, Z. Chen, X. Liu, H. Liu, X. Xu, D. Jia, Y. Chen, M. D.
Plumbley, and W. Wang, “T-clap: Temporal-enhanced contrastive
language-audio pretraining,” _arXiv preprint arXiv:2404.17806_,
2024.

[32] G. Zhu and Z. Duan, “Cacophony: An improved contrastive
audio-text model,” _arXiv preprint arXiv:2402.06986_, 2024.

[33] M. Mazumder, S. Chitlangia, C. Banbury, Y. Kang, J. M. Ciro,
K. Achorn, D. Galvez, M. Sabini, P. Mattson, D. Kanter _et al._,
“Multilingual spoken words corpus,” in _Thirty-fifth Conference_
_on Neural Information Processing Systems Datasets and Bench-_
_marks Track (Round 2)_, 2021.

