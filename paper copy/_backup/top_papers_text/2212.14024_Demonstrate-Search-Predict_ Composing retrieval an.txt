DEMONSTRATE –SEARCH –PREDICT :
Composing retrieval and language models for knowledge-intensive NLP
Omar Khattab1Keshav Santhanam1Xiang Lisa Li1David Hall1
Percy Liang1Christopher Potts1Matei Zaharia1
Abstract
Retrieval-augmented in-context learning has
emerged as a powerful approach for addressing
knowledge-intensive tasks using frozen language
models (LM) and retrieval models (RM). Exist-
ing work has combined these in simple “retrieve-
then-read” pipelines in which the RM retrieves
passages that are inserted into the LM prompt.
To begin to fully realize the potential of frozen
LMs and RMs, we propose DEMONSTRATE –
SEARCH –PREDICT (DSP ), a framework that re-
lies on passing natural language texts in sophisti-
cated pipelines between an LM and an RM. DSP
can express high-level programs that bootstrap
pipeline-aware demonstrations, search for rele-
vant passages, and generate grounded predictions,
systematically breaking down problems into small
transformations that the LM and RM can handle
more reliably. We have written novel DSP pro-
grams for answering questions in open-domain,
multi-hop, and conversational settings, establish-
ing in early evaluations new state-of-the-art in-
context learning results and delivering 37–120%,
8–39%, and 80–290% relative gains against the
vanilla LM (GPT-3.5), a standard retrieve-then-
read pipeline, and a contemporaneous self-ask
pipeline, respectively. We release DSP athttps:
//github.com/stanfordnlp/dsp .
1. Introduction
In-context learning adapts a frozen language model (LM) to
tasks by conditioning the LM on a textual prompt including
task instructions and a few demonstrating examples (Mc-
Cann et al., 2018; Radford et al., 2019; Brown et al., 2020).
For knowledge-intensive tasks such as question answering,
fact checking, and information-seeking dialogue, retrieval
models (RM) are increasingly used to augment prompts
1Stanford University . Correspondence to:
Omar Khattab <okhattab@cs.stanford.edu >.
Preprint .
How many storeys are in the castle David Gregory inherited?
LM:Castle Gregory has three storeys.❌Hallucinates 
a fictitious castle
RM: “St. Gregory Hotel is a nine-floor boutique hotel in D.C...”
LM: St. Gregory Hotel has nine storeys.❌Retrieves a 
different building
LM: “Which castle did David Gregory inherit?”
RM: “David Gregory inherited Kinnairdy Castle in 1664...”
LM: “How many storyes does Kinnairdy Castle have?”
RM: “Kinnairdy Castle is a tower house, having five storeys…”
LM: Kinnairdy Castle has fivestoreys.Vanilla LM
Retrieve-
then-Read
Multi-Hop
DSP ProgramFigure 1. A comparison between three systems based on GPT-
3.5 (text-davinci-002 ). On its own, the LM often makes false
assertions. An increasingly popular retrieve-then-read pipeline
fails when simple search can’t ﬁnd an answer. In contrast, a task-
aware DSP program successfully decomposes the problem and
produces a correct response. Texts edited for presentation.
with relevant information from a large corpus (Lazaridou
et al., 2022; Press et al., 2022; Khot et al., 2022).
Recent work has shown such retrieval-augmented in-context
learning to be effective in simple “retrieve-then-read”
pipelines: a query is fed to the RM and the retrieved pas-
sages become part of a prompt that provides context for
the LM to use in its response. In this work, we argue that
the fact that both LMs and RMs consume (and generate or
retrieve) natural language texts creates an opportunity for
much more sophisticated interactions between them. Fully
realizing this would be transformative: frozen LMs and
RMs could serve as infrastructure across tasks, enabling
ML- and domain-experts alike to rapidly build grounded
AI systems at a high level of abstraction and with lower
deployment overheads and annotation costs.
Figure 1 begins to illustrate the power of retrieval-
augmented in-context learning, but also the limitations of
“retrieve-then-read” (Lazaridou et al., 2022; Izacard et al.,
2022). Our query is “How many storeys are in the castle
David Gregory inherited?” When prompted to answer this,
GPT-3.5 ( text-davinci-002 ; Ouyang et al. 2022) makes
up a ﬁctitious castle with incorrect attributes, highlighting
the common observation that knowledge stored in LM pa-
rameters is often unreliable (Shuster et al., 2021; Ishii et al.,
2022). Introducing an RM component helps, as the LM
can ground its responses in retrieved passages, but a rigidarXiv:2212.14024v2  [cs.CL]  23 Jan 2023
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
QHow many storeys are in...
Q In which city did Akeem 
Ellis play in 2017?
A Ellesmere PortQ When was the discoverer of 
Palomar 4 born?
A 1889Train
Demonstrate
defdemonstrate (x:Example ) -> Example :
x.demos = annotate (x.train, attempt )
return x
defattempt (d:Example ):
d= search(d)
d= predict (d)
if d.pred == d.answer: return d1QHow many storeys are in the castle...
Q When was the discoverer of Palomar 4 born?
A 1889
Hop1 Who discovered Palomar 4?
Psg1 Edwin Hubble discovered Palomar 4...
Hop2 When was Edwin Powell born?
Psg2 Edwin Powell Hubble (1889–1953) was...
Pred 1889
x : ExampleQ In which city did Akeem Ellis play...
A Ellesmere Port
... ...
Pred Waterloo❌Demos“How many storeys are in the 
castle David Gregory inherited?”QHow many storeys are in the...
Demos . . .
Hop1 Which castle did David Gregory inherit?
Psg1 David Gregory inherited Kinnairdy Castle...
Hop2 How many storeys are in Kinnairdy Castle?
Psg2 Kinnairdy Castle […] having five storeys...
QHow many storeys does the...
. . . . . .
Pred Five storeysSearch
defsearch(x:Example ) -> Example :
x.hop1 =generate (hop_template)( x).pred
x.psg1 =retrieve (x.hop1, k=1)[0]
x.hop2 =generate (hop_template)( x).pred
x.psg2 =retrieve (x.hop2, k=1)[0]
return x2Predict
defpredict (x:Example ) -> Example :
x.context = [x.psg1, x.psg2]
x.pred =generate (qa_template)( x).pred
return x3
“Five storeys”
Figure 2. A toy example of a DSP program for multi-hop question answering. Given an input question and a 2-shot training set, the
DEMONSTRATE stage programmatically annotates intermediate transformations on the training examples using a form of weak supervision.
Learning from a resulting demonstration , the SEARCH stage decomposes the complex input question and retrieves supporting information
over two retrieval hops. Finally, the P REDICT stage uses the demonstration and retrieved passages to answer the question.
retrieve-then-read strategy fails because the RM cannot ﬁnd
passages that directly answer the question.
We introduce the DEMONSTRATE –SEARCH –PREDICT
(DSP ) framework for in-context learning, which relies en-
tirely on passing natural language text (and scores) be-
tween a frozen RM and LM.DSP introduces a num-
ber of composable functions that bootstrap training exam-
ples ( DEMONSTRATE ), gather information from a knowl-
edge corpus ( SEARCH ), and generate grounded outputs
(PREDICT ), using them to systematically unify techniques
from the retrieval-augmented NLP and the in-context learn-
ing literatures (Lee et al., 2019; Khattab et al., 2021a; Anan-
tha et al., 2020; Gao et al., 2022; Izacard et al., 2022; Dohan
et al., 2022; Zelikman et al., 2022; Zhang et al., 2022).
We use DSP to suggest powerful strategies for knowledge-
intensive tasks with compositions of these techniques. This
reveals new conceptual possibilities for in-context learning
in general (§2), and it allows us to present rich programs
that set new state-of-the-art results (§3).
Figure 1 shows the path that a DSP program might take to
arrive at an answer, and Figure 2 illustrates how a deliberate
program achieves this. Instead of asking the LMto answer
this complex question, the program’s SEARCH stage uses the
LMto generate a query “Which castle did David Gregory
inherit?” The RM retrieves a passage saying Gregory inher-
ited the Kinnairdy Castle. After a second search “hop” ﬁnds
the castle’s number of storeys, the PREDICT stage queries
theLM with these passages to answer the original question.
Although this program implements behaviors such as query
generation, it requires no hand-labeled examples of these
intermediate transformations (i.e., of the queries and pas-
sages of both retrieval hops). Instead, the DEMONSTRATEstage uses labeled question–answer pairs to implement a
form of weak supervision that programmatically annotates
the transformations invoked within SEARCH andPREDICT .
We evaluate several DSP programs on answering questions
in open-domain, multi-hop, and conversational settings. In
them, we implement novel and reusable transformations
such as bootstrapping annotations for all of our pipelines
with weak supervision (§2.3), reliably rewriting questions to
resolve conversational dependencies and iteratively decom-
pose complex queries with summarization of intermediate
hops (§2.4), and generating grounded responses from mul-
tiple passages with self-consistency (§2.5). We report pre-
liminary results on Open-SQuAD, HotPotQA, and QReCC
using the frozen LMGPT-3.5 and RM ColBERTv2 (Khat-
tab & Zaharia, 2020; Santhanam et al., 2022b) with no
ﬁne-tuning. Our DSP programs deliver 37–120%, 8–39%,
and 80–290% relative gains against corresponding vanilla
LMs, a standard retrieve-then-read pipeline, and a contem-
poraneous self-ask pipeline (Press et al., 2022), respectively.
Future versions of this report will include additional test
tasks and LMchoices.
In summary, this work makes the following contributions.
First, we argue that simple task-agnostic pipelines for in-
context learning should give way to deliberate, task-aware
strategies. Second, we show that this shift need not be a
burden: with DSP , such strategies can be easily expressed
as short programs using composable operators. Third, this
composability spawns powerful capacities, like automati-
cally annotating demonstrations for complex pipelines from
end-task labels. Fourth, for three knowledge-intensive tasks,
we implement rich programs that establish state-of-the-art
results for in-context learning.
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
2. D EMONSTRATE –SEARCH –PREDICT
We now introduce the DSP framework and show its expres-
sive power by suggesting a number of strategies in which
theLM andRM can come together to tackle complex prob-
lems effectively. We show in §3 that such strategies out-
perform existing in-context learning methods. We begin by
discussing the LMandRM foundation modules on which
DSP is built (§2.1) and then the datatypes and control ﬂow
within DSP (§2.2). Subsequently, we discuss each of the
three inference stages: DEMONSTRATE (§2.3), SEARCH
(§2.4), and P REDICT (§2.5).
2.1. Pretrained Modules: LM and RM
ADSP program deﬁnes the communication between the
language model LMand the retrieval model RM.
Language Model We invoke a frozen language model
LM to conditionally generate (orscore ) text. For each
invocation, the program prepares a prompt that adapts the
LM to a speciﬁc function (e.g., answering questions or
generating queries). A prompt often includes instructions,
a few demonstrations of the desired behavior, and an input
query to be answered.
As in Figure 2, the LM generates not only: (i)the ﬁnal
answer to the input question (in the PREDICT stage), but also
(ii)intermediate “hop” queries to ﬁnd useful information
for the input question ( SEARCH ) as well as (iii)exemplar
queries that illustrate how to produce queries for questions
in the training set ( DEMONSTRATE ). This systematic use of
theLMis a hallmark of DSP programs.
Retrieval Model DSP programs also invoke a frozen re-
trieval model RM toretrieve the top- kmost “relevant”
text sequences for a given query . The RM canindex a
massive set of pre-deﬁned passages for scalable search, and
those passages can be updated without changing the retrieval
parameters. The RM accepts free-form textual inputs and
specializes in estimating the relevance (or similarity) of a
text sequence to a query.
As in Figure 2, the RM is responsible for retrieving (i)
passages for each query generated by the LM(during the
SEARCH stage), but also (ii)passages that are used within
demonstrations ( DEMONSTRATE ). In the latter case, the
RM’s contributions are less about providing directly rel-
evant information to the input question and more about
helping the LMadapt to the domain and task.
Though not utilized in this example, the RM is also used in
DSP for functions like retrieving “nearest-neighbor” demon-
strations from task training data ( DEMONSTRATE ) and se-
lecting well-grounded generated sequences from the LM
(PREDICT ).2.2. Datatypes and Control Flow
We have implemented the DSP framework in Python. The
present section introduces the core data types and compos-
able functions provided by the framework. We use illustra-
tive code snippets to ground the examples, and to convey
the power that comes from being able to express complex
interactions between the LMandRM in simple programs.
The Example Datatype To conduct a task, a DSP pro-
gram manipulates one or more instances of the Example
datatype. An Example behaves like a Python dictionary
with multiple ﬁelds. The program is typically provided with
a few training examples. The code snippet below illustrates
this for multi-hop question answering.
1from dsp import Example
2
3train = [ Example ( question =" When was the discoverer
of Palomar 4 born ?", answer =" 1889 "),
4 Example ( question ="In which city did Akeem
Ellis play in 2017? ", answer =" Ellesmere Port ")]
This snippet contains two labeled examples, each with a
multi-hop question (e.g., “In which city did Akeem Ellis
play in 2017?”) and its short answer (“Ellesmere Port”).
Arbitrary keys and values are allowed within an Example ,
though typical values are strings or lists of strings.
In this task, we are unlikely to ﬁnd an individual passage
that provides the answer to any question. For example, the
ﬁrst training example can probably be resolved only by ﬁrst
answering the question of who discovered Palomar (“Edwin
Hubble”) and then addressing the question of Hubble’s birth
date using different evidence passages. We typically assume
that the human-labeled training data do notinclude labels
for intermediate transformations (e.g., queries for individual
hops) that would be useful for following these steps, and so
it is the job of the DSP program to discover these strategies
via in-context learning.
A DSP Program The following code snippet is a com-
plete program for resolving multi-hop questions like those
in Figure 1, with help from train examples like those above.
1def multihop_program ( question : str ) -> str :
2 x = Example ( question = question , train = train )
3 x = multihop_demonstrate (x)
4 x = multihop_search (x)
5 x = multihop_predict (x)
6 return x. answer
7
8multihop_program (" How many storeys does the castle
David Gregory inherited have ?")
9# => " five storeys "
The program takes the input (here, a question) and outputs
the system output (its short answer). It starts by creating
anExample for the input question and assigning the train
ﬁeld to the training set from the previous snippet. Programs
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
invoke and compose DSP primitives (i.e., built-in functions)
to build the DEMONSTRATE ,SEARCH , and PREDICT trans-
formations that deﬁne the program.
Transformations A transformation is a function that
takes an Example as input and returns an Example , pop-
ulating new ﬁelds (or modifying existing ﬁelds) in it. This
program invokes three developer-deﬁned transformations,
namely, multihop_demonstrate ,multihop_search , and
multihop_predict . Transformations may themselves in-
voke other transformations, and they act analogously to
layers in standard deep neural network (DNN) program-
ming frameworks such as PyTorch, except that they pass
text data instead of tensors between each other and do not
involve backpropagation.
We categorize transformations according to their behavior
(or purpose) under one of the DEMONSTRATE ,SEARCH ,
andPREDICT stages. That said, DSP does not impose this
categorization and allows us to deﬁne functions that may
blend these stages. We will discuss each of the three stages
next.
2.3. D EMONSTRATE
It is known that including examples of the desired behavior
from the LMin its prompt typically leads to better perfor-
mance (Brown et al., 2020). In DSP , ademonstration is a
training example that has been prepared to illustrate speciﬁc
desired behaviors from the LM. ADEMONSTRATE transfor-
mation takes as input xof type Example and prepares a list
of demonstrations in x.demos , typically by selecting a sub-
set of the training examples in x.train andbootstrapping
new ﬁelds in them.
Bootstrapping Demonstrations Examples in the train-
ing set typically consist of the input text and the target
output of the task. The DEMONSTRATE stage can aug-
ment a training example by programmatically bootstrapping
annotations for intermediate transformations. In our run-
ning “multi-hop” example, the demonstrations illustrate
three LM-based transformations: (i)how to break down the
input question in order to gather information for answer-
ing it (i.e., ﬁrst-hop retrieval), (ii)how to use information
gathered in an earlier “hop” to ask follow-up questions, and
(iii)how to use the information gathered to answer complex
questions.
1Examples = list [ Example ]
2Transformation = Callable [[ Example ],
3 Optional [ Example ]]
4
5annotate ( train : Examples , fn: Transformation )
6 -> Examples
Akin to a specialized map, the annotate primitive accepts
a user-deﬁned transformation fnand applies it over a listof training examples. Whenever fnreturns an example
(rather than None ),annotate caches the intermediate pre-
dictions (i.e., the generated queries and retrieved passages).
These predictions serve as successful demonstrations for the
pipeline transformations. In simple uses, fnmay attempt
to answer the example “zero-shot” one or more times. This
is typically done by invoking the SEARCH andPREDICT
stages of the program. When an answer is produced, if
fnassesses it as correct, it returns a populated example in
which the intermediate predictions are present.
Case Study The snippet below deﬁnes the func-
tion multihop_demonstrate , called in Line 3 of
multihop_program , and illustrates the usage of annotate .
1from dsp import sample , annotate
2
3def attempt_example (d: Example ):
4 d = d. copy ( demos =[])
5 d = multihop_search (d)
6 d = multihop_predict (d)
7 return d if d. pred == d. answer else None
8
9def multihop_demonstrate (x: Example ):
10 demos = annotate (x.train , attempt_example )
11 return Example (x, demos = demos )
In Line 10, multihop_demonstrate invokes annotate ,
which bootstraps missing ﬁelds in training examples by
caching annotations from attempt_example . The transfor-
mation attempt_example takes a training example dand
attempts to answer it in a zero-shot fashion: it creates a copy
ofdwith no demonstrations (Line 4; i.e., zero-shot) and
invokes the multi-hop search and predict pipeline (Lines 5
and 6). Each transformation returns an updated version of
dwith additional ﬁelds populated. If the pipeline answers
correctly (Line 7), the updated dis returned.
Figure 2 illustrates this behavior. DEMONSTRATE trans-
forms a training question–answer pair to a fully-populated
demonstration, including ﬁelds such as hop1 andhop2 (i.e.,
queries for multi-hop search) as well as psg1 andpsg2 .
When the LMis later invoked to conduct a transformation,
say, generating a “second-hop” query during SEARCH , the
psg1 ﬁeld serves as context and the hop2 ﬁeld serves as a
label for this particular training example.
Discussion This simple case study illustrates the power of
composition in the DSP abstraction. Because the pipeline
is a well-deﬁned program in which transformations com-
municate by passing text attached to Example s, a simple
map-and-ﬁlter strategy can leverage the LM andRM to
bootstrap annotations for a full pipeline from end-task la-
bels. This is an extensible strategy, but even in its simplest
form it generalizes the approaches explored recently by Ze-
likman et al. (2022), Wei et al. (2022), Zhang et al. (2022),
and Huang et al. (2022) in which an LM self-generates
chain-of-thought rationales for an individual prompt.
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
By bootstrapping pipelines, DEMONSTRATE makes it easy
to explore complex strategies in SEARCH andPREDICT
without writing examples for every transformation. This
includes strategies that are challenging to explore without
custom annotations in traditional retrieval-augmented NLP.
For instance, Khattab et al. (2021a) introduces a pipeline
for multi-hop reasoning that is trained with weak supervi-
sion, extending work by Lee et al. (2019) and Khattab et al.
(2021b). In it, the target 3 or 4 passages that need to re-
trieved must be labeled but the system discovers the best
order of “hops” automatically.
In contrast, DSP allows us to build complex pipelines with-
out labels for intermediate steps, because we can compose
programs out of small transformations. If LMandRM can
accurately process such transformations “zero-shot” (i.e.,
without demonstrations) on at least one or two examples,
these examples can be discovered with end-task labels and
used as demonstrations.
To draw on our earlier analogy with DNN frameworks like
PyTorch, DEMONSTRATE aims to replace the function of
backpropagation in extensible ways by simulating the be-
havior of the program (corresponding to a “forward” pass)
and programmatically learning from errors. In doing this
with frozen models and with only end-task labels, DEMON -
STRATE introduces a high degree of modularity. In partic-
ular, without hand-labeling intermediate transformations,
developers may swap the training domain, update the train-
ing examples, or modify the program’s strategy, and use
annotate to automatically populate all of the intermediate
ﬁelds for demonstrations.
Selecting Demonstrations It is not always possible to ﬁt
all of the training examples in the context window of the
LM.DSP provides three primitives for selecting a subset
of training examples, namely, sample ,knn, and crossval .
1sample ( train : Examples , k: int )
2 -> Examples
3
4knn ( train : Examples , cast : Callable [[ Example ], str ])
5 -> fn( example : Example , k: int ) # currying
6 -> Examples
7
8crossval ( train : Examples , n: int , k: int )
9 -> fn( evaluate : Transformation )
10 -> Examples
As a baseline choice, kdemonstrations can be randomly
sampled from train using the sample primitive, an ap-
proach used by Brown et al. (2020) and much subsequent
work. We can also leverage the RM’s representations and se-
lect from the training set the knearest neighbors to the input
text, a strategy explored by Liu et al. (2021). Another strat-
egy is to apply cross-validation to select among a number of
sampled sets of demonstrations (Perez et al., 2021). For ex-
ample, given jtrain j= 100 training examples, crossvalwould select nsubsets of k= 5examples each, and return
the set with which a transformation evaluate performs best
on the remaining 95examples.
Compositions & Extensions By manipulating demon-
strations and higher-order transformations, these simple
selection and bootstrapping primitives can be combined to
conduct larger novel strategies. If the training set is very
large (e.g., jtrain j= 100 ;000), we can conduct knnto
ﬁnd the nearest k= 16 examples and only annotate these,
arriving at a system that learns incrementally in real-time. If
the training set is moderately large (e.g., jtrain j= 1000 ),
we can conduct crossval and cache the performance of all
prompts it evaluates on each training example. At test time,
we can use knnto ﬁnd k= 50 similar examples to the test
input and select the prompt that performs best on these k
examples, producing an adaptive system that is informed by
the quality of its pipeline on different types of examples.
2.4. S EARCH
TheSEARCH stage gathers passages to support transforma-
tions conducted by the LM. We assume a large knowledge
corpus—e.g., a snippet of Web, Wikipedia, or arXiv—that
is divided into text passages . Providing passages to the LM
facilitates factual responses, enables updating the knowl-
edge store without retraining, and presents a transparency
contract: when in doubt, users can check whether the system
has faithfully used a reliable source in making a prediction.
In the simplest scenarios, SEARCH can directly query the
RM, requesting the top- kpassages (from a pre-deﬁned in-
dex) that match an input question. This baseline instantia-
tion of SEARCH simulates retrieval in most open-domain
question answering systems, which implement a “retrieve-
then-read” pipeline, like Lee et al. (2019), Khattab et al.
(2021b), Lazaridou et al. (2022), and many others.
1from dsp import retrieve
2
3def simple_search (x):
4 passages = retrieve ( query =x. question , k =2)
5 return passages
SEARCH Strategies In many scenarios, the complexity
of the task demands more sophisticated SEARCH strategies
that empower the RM to ﬁnd relevant passages. Our run-
ning example (Figure 2) is one such scenario, in which we
suspect examples are likely to require multi-hop reasoning
in particular. Other settings, for instance, pose conversa-
tional challenges, in which the information need expressed
by a user can only be resolved by taking into account pre-
vious turns in the conversation, or demand more extensive
planning (Zhong et al., 2022).
In the retrieval-augmented NLP literature, multi-hop
search (Xiong et al., 2020; Khattab et al., 2021a) and con-
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
versational search (Del Tredici et al., 2021; Raposo et al.,
2022) pipelines have received much attention. These sys-
tems are typically ﬁne-tuned with many hand-labeled query
“rewrites” (Anantha et al., 2020), “decompositions” (Geva
et al., 2021; Min et al., 2019), or target hops (Yang et al.,
2018; Jiang et al., 2020). Supported with automatic anno-
tations from D EMONSTRATE , the S EARCH stage allows us
to simulate many such strategies and many others in terms
of passing queries, passages, and demonstrations between
theRM andLM. More importantly, SEARCH facilitates our
vision of advanced strategies in which the LMandRM co-
operate to incrementally plan a research path for which the
RM gathers information and the LMidentiﬁes next steps.
Case Study Let us build on our running multi-hop exam-
ple as a case study. We can deﬁne multihop_search_v2
(Line 4 in our core program), a slightly more advanced ver-
sion of the SEARCH transformation from Figure 2. This
transformation simulates the iterative retrieval component
of ﬁne-tuned retrieval-augmented systems like IRRR (Qi
et al., 2020), which reads a retrieved passage in every hop
and generates a search query (or a termination condition to
stop hopping), and Baleen (Khattab et al., 2021a), which
summarizes the information from many passages in each
hop for inclusion in subsequent hops.
1from dsp import generate
2
3def multihop_search_v2 (x, max_hops =3) :
4 x. hops = []
5
6 for hop in range ( max_hops ):
7 summary , query = generate ( hop_template )(x)
8 x. hops . append (( summary , query ))
9
10 if query == /quotesingle.VarN/A/quotesingle.Var: break
11
12 passages = retrieve (query , k =5)
13 x. context = [ summary ] + passages
14
15 return x
Inmultihop_search_v2 , Line 7 calls the generate prim-
itive, which invokes the LM to produce a query for each
retrieval hop. The LM is conditioned on a prompt that is
prepared using the hop_template template. (We discuss
prompt templates and the generate primitive in §2.5.) Here,
this template may be designed to generate a prompt that has
the following format (e.g., for the second hop).
1My task is to write a simple query that gathers
information for answering a complex question . I
write N/A if the context contains all
information required .
2
3{ Task demonstrations from x.demos , if any }
4
5Context : {x. context }
6Question : {x. question }
7Summary : Let /quotesingle.Vars summarize the above context .
__{ summary }__
8Search Query : __{ query }__As shown, the LM is instructed to read the context re-
trieved in earlier hops and a complex question. It is then
prompted to write: (i)a summary of the supplied con-
text and (ii)a search query that gathers information for
answering that question. The generated text will be ex-
tracted and assigned to the summary andquery variables in
(multihop_search_v2 ; Line 7). On Line 10, we terminate
the hops if the query is “N/A”. Otherwise, Line 12 retrieves
k= 5 passages using the query and Line 13 assigns the
context for the subsequent hop (or for PREDICT ), setting
that to include the summary of all previous hops as well as
the passages retrieved in the ﬁnal hop so far.
Comparison with self-ask It may be instructive to con-
trast this multi-hop DSP program with the recent “self-
ask” (Press et al., 2022) prompting technique, which we
compare against in §3. Self-ask can be thought of as a sim-
ple instantiation of DSP ’sSEARCH stage. In it, the LMasks
one or more “follow-up questions”, which are intercepted
and sent to a search engine. The search engine’s answers
are concatenated into the prompt and are used to answer
the question. This is essentially a simpliﬁed simulation of
IRRR (Qi et al., 2020).
As a general framework, DSP can express ideas like self-ask
and many other, more sophisticated pipelines as we discuss
in the present section. More importantly, DSP offers a num-
ber of intrinsic advantages that lead to large empirical gains:
80%–290% over self-ask. For instance, DSP programs are
deeply modular, which among other things means that DSP
programs will annotate and construct their own demonstra-
tions. Thus, they can be developed without labeling any
of the intermediate transformations (e.g., the queries gener-
ated). In addition, the LM prompts constructed by DSP get
automatically updated to align with the training data and re-
trieval corpus provided. In contrast, approaches like self-ask
rely on a hand-written prompt with hard-coded examples.
Moreover, DSP assigns the control ﬂow to an explicit pro-
gram and facilitates design patterns that invoke the LM(or
RM) to conduct small transformations. This allows us to
build steps that are dedicated to generating one or more re-
trieval queries, summarizing multiple passages per hop, and
answering questions. These steps are individually simpler
than the self-ask prompt, yet our multi-hop DSP program
deliberately composes them to build richer pipelines that are
thus more reliable. In contrast, self-ask delegates the con-
trol ﬂow to the LMcompletions, maintaining state within
the prompt itself and intercepting follow-up questions to
conduct search. We ﬁnd that this paradigm leads to a “self-
distraction” problem (§3.5) that DSP programs avoid.
Fusing Retrieval Results For improved recall and robust-
ness, we can also fuse the retrieval across multiple gen-
erated queries. Fusion has a long history in information
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
retrieval (Fox & Shaw, 1994; Xue & Croft, 2013; Kur-
land & Culpepper, 2018) and sequentially processing multi-
ple queries was explored recently by Gao et al. (2022) for
retroactively attributing text generated by LMs to citations.
Inspired by these, we include a fused_retrieval primitive
toDSP to offer a versatile mechanism for interacting with
frozen retrievers. It accepts an optional fusion function that
maps multiple retrieval lists into one. By default, DSP uses
a variant of CombSUM (Fox & Shaw, 1994), assigning each
passage the sum of its probabilities across retrieval lists.
To illustrate, the modiﬁcation below generates n= 10
queries for the transformation multihop_search_v2 .
c = generate ( hop_template , n =10) (x)
passages = fused_retrieval (c. queries , k =5)
summary = c. summaries [0] # highest - scoring summary
Compositions & Extensions To illustrate a simple com-
position, we can equip a chatbot with the capacity for con-
versational multi-hop search by combining a query rewriting
step, which produces a query that encompasses all of the
relevant conversational context, with the multi-hop transfor-
mation, as follows.
1def conversational_multihop_search (x):
2 x. question = generate ( conv_rewriting_template )(x)
3 return multihop_search_v2 (x)
Similar approaches can be used for correcting spelling mis-
takes or implementing pseudo-relevance feedback (Cao
et al., 2008; Wang et al., 2022a), in which retrieved passages
are used to inform a better search query, though this has not
been attempted with pretrained LMs to our knowledge.
2.5. P REDICT
The PREDICT stage generates the system output using
demonstrations (e.g., in x.demos ) and passages (e.g., in
x.context ).PREDICT tackles the challenges of reliably
solving the downstream task, which integrates much of the
work on in-context learning in general. Within DSP , it also
has the more specialized function of systematically aggre-
gating information across a large number of demonstrations,
passages, and candidate predictions.
Generating Candidates Generally, PREDICT has to pro-
duce one or more candidate predictions for the end-task.
To this end, the basic primitive in PREDICT isgenerate ,
which accepts a Template and (via currying) an Example
and queries the LM to produce one or more completions,
as explored earlier in §2.4. A corresponding primitive that
uses the RM in this stage is rank , which accepts a query
and one or more passages and returns their relevance scores.1Template # template : an object that can produce
prompts and parse completions
2
3generate ( template : Template )
4 -> fn( example : Example )
5 -> Completions # object with keys to access
extracted preds and scores
6
7rank ( query : str , passages : List [ str ])
8 -> List [ float ] # object with keys to access
passage texts and scores
ATemplate is an object that can produce prompts, that is,
map an Example to a string, and extract ﬁelds out of com-
pletions. For instance, we can map an example xthat has a
question and retrieved passages to the following prompt:
1My task is to answer questions using Web documents .
2
3{ Task demonstrations from x.demos , if any }
4
5Context : {x. passage }
6Question : {x. question }
7Rationale : Let /quotesingle.Vars think step by step . __{ rationale }__
8Answer : __{ answer }__
As this illustrates, the LMwill be asked to generate a chain-
of-thought rationale (CoT; Wei et al. 2022; Kojima et al.
2022) and an answer, and the generated text will be ex-
tracted back into the rationale andanswer keys of each
completion.
Each invocation to the LMcan sample multiple candidate
predictions. Selecting a “best” prediction is the subject of
much work on decoding (Wiher et al., 2022; Li et al., 2022),
but a frozen and general-purpose LMmay not support cus-
tom modiﬁcations to decoding. Within these constraints, we
present several high-level strategies for selecting predictions
and aggregating information in DSP via the LMandRM.
Selecting Predictions Among multiple candidates, we
can simply extract the most popular prediction. When a CoT
is used to arrive at the answer, this is the self-consistency
method of Wang et al. (2022c), which seeks to identify
predictions at which multiple distinct rationales arrive.
1from dsp import generate , majority
2
3def multihop_predict (x):
4 candidates = generate ( template_qa )(x)
5 return x. copy ( answer = majority ( candidates ). answer )
DSP generalizes this in two ways. First, we can sample
multiple “pipelines of transformations” (PoT) within the pro-
gram, rather than locally with “chains of thought” (CoT) in
one transformation. These chains may even invoke different
paths in the program, as illustrated below.
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
1from dsp import branch
2
3def pipeline (x):
4 return multihop_predict ( multihop_search_v2 (x))
5
6def PoT_program ( question : str ) -> str :
7 x = Example ( question = question , train = train )
8 x = multihop_demonstrate (x)
9
10 candidates = branch ( pipeline , n=5, t =0.7) (x)
11 return x. copy ( answer = majority ( candidates ). answer )
In the snippet above, Line 10 invokes the primitive branch
which samples ndifferent PoTs with a high temperature
(e.g., t= 0:7) and accumulates their intermediate and
ﬁnal predictions. In this example, our pipeline invokes
multihop_search_v2 (§2.4), which applies a variable num-
ber of retrieval hops depending on the questions generated,
before doing PREDICT . That is, PoT_program potentially
invokes multiple distinct paths in the program (i.e., with dif-
ferent multi-hop queries and number of hops in each) across
branches. It then selects the majority answer overall.
DSP generalizes self-consistency in a second way. When
sampling our CoTs or PoTs provides multiple candidates,
we can select the top- k(e.g., top-4) predictions and then
compare them directly. For instance, we may prompt the
LMto compare these choices as MCQ candidates, a trans-
formation for which DEMONSTRATE can automatically pre-
pare exemplars. This effectively simulates the LM recursion
of Levine et al. (2022), though unlike their approach it does
not require a large training set or updating any (prompt-
tuning) weights. One such implementation is illustrated in
openqa_predict below.
1def openqa_predict (x):
2 preds = generate ( template_qa , n =20) (x). answers
3 x. choices = most_common (preds , k =4)
4
5 queries = [f"{x. question } {c}"
6 for c in x. choices ]
7
8 x. passages = fused_retrieval ( queries )
9 x. answer = generate ( TemplateMCQ )(x). answer
10 return x
As an alternative comparison approach, we can invoke the
RM viarank to ﬁnd the prediction that is most grounded in
a retrieved contexts (i.e., most similar to the concatenation
of the retrieved passages) or, given an RM that can score
completions (Krishna et al., 2022), simply the prediction
that has the highest score given the prompt.
Aggregating Information When only a few demonstra-
tions or passages are selected, we can simply concate-
nate them all into the prompt. For instance, GPT-3.5
text-davinci-002 has a context window of 4097 tokens,
which we ﬁnd to be reasonably large for accommodating
several (e.g., 3–5) demonstrations, which individually in-
clude their own passages and rationales.To deal with a larger number of demonstrations or passages,
we can branch in parallel to process individual subsets
of the passages or demonstrations and then aggregate the
individual answers using one of the scoring methods pre-
sented earlier. Indeed, Lewis et al. (2020) and Lazaridou
et al. (2022) have explored marginalization as a way to com-
bine scores across passages and Le et al. (2022) ensemble
prompts across demonstrations, which can be expressed in
this way.
An alternative aggregation strategy is to accumulate informa-
tion across passages sequentially, rather than independently.
This is effectively how our multi-hop approach works (§2.4).
Such a strategy has also been employed recently by Gao
et al. (2022) for retroactively attributing text generated by
LMs to citations. They generate many queries but instead
of fusion (§2.4), they run their pipeline on each query and
use its outputs to alter the input to subsequent queries.1
3. Evaluation
We now consider how to implement DSP programs for three
diverse knowledge-intensive NLP tasks: open-domain ques-
tion answering (QA), multi-hop QA, and conversational QA.
All of these tasks are “open-domain”, in the sense that sys-
tems are given a short question or participate in a multi-turn
conversation without being granted access to context that
answers these questions.
We build and evaluate intuitive compositions of the func-
tions explored in §2 for each task. We show that, despite
low development effort, the resulting DSP programs exhibit
strong quality and deliver considerable empirical gains over
vanilla in-context learning and a standard retrieve-then-read
pipeline with in-context learning.
3.1. Evaluation Methodology
In this report, we consider one development dataset for each
of the tasks we consider, namely, the open-domain version
of SQuAD (Rajpurkar et al., 2016; Lee et al., 2019), the
multi-hop HotPotQA (Yang et al., 2018) dataset in the open-
domain “fullwiki” setting, and the conversational question
answering QReCC (Anantha et al., 2020; Vakulenko et al.,
2022) dataset, which we used for developing the DSP ab-
stractions. We report the validation set accuracy on all three
datasets and discuss them in detail §3.5.
Unless otherwise stated, systems are given access to 16-
shot training examples, that is, each DSP program can use
(up to) 16 questions—or conversations, where applicable—
randomly sampled from the respective training set. We
1Though most of the functionality in this section is imple-
mented, the primitives branch ,knn, and crossval are currently
work-in-progress.
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
subsample the validation and test sets to 1000 questions
(or 400 conversations, where applicable) and report average
quality across ﬁve seeds where each seed ﬁxes a single k-
shot training set of examples. To control the language model
API spending budget, each seed processes one ﬁfth of the
evaluation examples (e.g., 200 questions per seed, for a total
of 1000 unique questions).
We also dedicate held-out test datasets (e.g., Open-
NaturalQuestions; Kwiatkowski et al. 2019) and test tasks
(e.g., claim veriﬁcation, as in FEVER; Thorne et al. 2018)
that we only use for evaluating pre-deﬁned DSP programs
rather than development. We will include these results in a
future version of this report.
3.2. Pretrained Modules
RM We use ColBERTv2 (Santhanam et al., 2022b), a
state-of-the-art retriever based on late interaction (Khattab
& Zaharia, 2020). We choose ColBERTv2 for its highly
effective zero-shot search quality and efﬁcient search (San-
thanam et al., 2022a). However, our DSP programs are
agnostic to how the retriever represents examples or scores
passages, so essentially any retriever can be used.
In addition, by making retrieval a ﬁrst-class construct, DSP
allows us to change or update the search index over time.
We simulate this in our experiments by aligning each of our
datasets with the nearest Wikipedia corpus among the Dec
2016 Wikipedia dump from Chen et al. 2017, the Nov 2017
Wikipedia “abstracts” dump from Yang et al. 2018, and the
Dec 2018 Wikipedia dump from Karpukhin et al. 2020.
LM We use the GPT-3.5 ( text-davinci-002 ; Brown
et al. 2020; Ouyang et al. 2022) language model. Unless
otherwise stated, we use greedy decoding when generating
n= 1 prediction. We sample with temperature t= 0:7
when n > 1, like related work (Wang et al., 2022c).
3.3. Baselines
Vanilla LM The vanilla LM baselines represent the few-
shot in-context learning paradigm used by Brown et al.
(2020). The open-domain QA and multi-hop QA base-
lines randomly sample 16 demonstrations (i.e., all of the
examples available to each program in our evaluation) from
the training set and do not augment these demonstrations
with evidence. Similarly, the conversational QA baseline
samples four conversations. The vanilla baselines do not
search for passages relevant to the input query.
1def vanilla_LM_QA ( question : str ) -> str :
2 demos = sample (train , k =16)
3 x = Example ( question = question , demos = demos )
4 return generate ( qa_template )(x). predRetrieve-then-Read The “retrieve-then-read” baselines
use the RM to support each example with a potentially rele-
vant passage before submitting the prompt to the LM. This
emulates the pipelines used by state-of-the-art open-domain
question answering systems (Khattab et al., 2021b; Izacard
& Grave, 2020; Hofstätter et al., 2022). In conversational
QA, we concatenate the ﬁrst turn and the ﬁnal question, an
approach that we found to perform much better than simply
using the ﬁnal turn. For multi-hop QA, we retrieve and
concatenate two passages per question.
1def retrieve_then_read_QA ( question : str ) -> str :
2 demos = sample (train , k =16)
3 passages = retrieve ( question , k =1)
4 x = Example ( question = question ,
5 passages = passages ,
6 demos = demos )
7 return generate ( qa_template )(x). pred
Self-ask We also compare against self-ask (Press et al.,
2022), a contemporaneous pipeline that can be thought of
as a speciﬁc instantiation of DSP ’sSEARCH stage followed
by a simple PREDICT step. For direct comparison with
our methods, we modify the self-ask control ﬂow to query
the same ColBERTv2 index used in our DSP experiments
instead of Google Search. We evaluate two conﬁgurations of
self-ask. The ﬁrst uses the original self-ask prompt template,
which contains four hand-written demonstrations. In the
second conﬁguration, we modify the prompt template to
apply a number of changes that we ﬁnd are empirically
useful for HotPotQA.2
3.4. Proposed DSP Programs
We build on transformations presented in §2. Our programs
for all three tasks have the following structure, illustrated
for open-domain QA.
1def openqa_program ( question : str ) -> str :
2 x = Example ( question = question , train = train )
3 x = openqa_demonstrate (x)
4 x = openqa_search (x)
5 x = openqa_predict (x)
6 return x. answer
The exception is that the conversational QA program,
2In particular: (i)use ColBERTv2-style passages in the hand-
crafted demonstrations of self-ask (i.e., instead of the original
Google-style snippets), (ii)concatenate 16-shot training examples
from the task (i.e., question–answer pairs) as a preﬁx of the prompt,
(iii)ask the model to generate a short intermediate answer per
retrieval step, and (iv)explicitly ask the model to generate a follow-
up “search query” at each step. We found the ﬁnal item to be
important because self-ask’s default prompt often produces follow-
up questions that are not self-contained (e.g., “what is the name of
the national park?”, which is not an informative search query). We
also ﬁx the casing in the prompt to be consistent.
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
Table 1. Development results comparing a task-aware DSP program against baseline vanilla LM and retrieve-then-read LM as well as
recent and contemporaneous in-context learning approaches with and without retrieval. All of our runs use GPT-3.5 and our retrieval-based
rows use ColBERTv2. The results marked with{are collected from related work as of mid-December 2022, and attributed to their
individual sources in the main text. As we discuss in the main text, the marked results are not generally apples-to-apples comparisons,
since they span a variety of evaluation settings. Nonetheless, we report them here as qualitative reference points.
Open-SQuAD HotPotQA QReCC
EM F1 EM F1 F1 nF1
Vanilla LM 16.2 25.6 28.3 36.4 29.8 18.4
No-retrieval LM SoTA 20.2{– 33.8{44.6{– –
Retrieve-then-Read 33.8 46.1 36.9 46.1 31.6 22.2
Self-ask w/ ColBERTv2 Search 9.3 17.2 25.2 33.2 – –
+ Reﬁned Prompt 9.0 15.7 28.6 37.3 – –
Retrieval-augmented LM SoTA 34.0{– 35.1{– – –
Task-aware DSP Program 36.6 49.0 51.4 62.9 35.0 25.3
convqa_program , accepts turns (i.e., a list of strings, rep-
resenting the conversational history) instead of a single
question . Unless otherwise stated, our programs default to
greedy decoding during the D EMONSTRATE stage.
ForSEARCH , our open-domain QA program uses the ques-
tion directly for retrieving k= 7 passages and concate-
nates these passages into our QA prompt with CoT. For
PREDICT , it generates n= 20 reasoning chains and uses
self-consistency (SC; Wang et al. 2022c) to select its ﬁnal
prediction. For DEMONSTRATE , our open-domain QA pro-
gram uses the following approach, slightly simpliﬁed for
presentation. In it, the parameter k= 3passed to annotate
requests annotating only three demonstrations, which will
then be used in the prompts.
1def openqa_demonstrate (x: Example ) -> Example :
2 demos = sample (x.train , k =16)
3
4 def openqa_attempt (d: Example ) -> Example :
5 d. demos = all_but (demos , d) # all ( raw )
examples different from d
6
7 d = openqa_search (d, k =2)
8 if not passage_match (d): return None # skip
examples where search fails
9
10 d = openqa_predict (d, sc= False )
11 if not answer_match (d): return None # skip
examples where predict fails
12
13 return d
14
15 x. demos = annotate (demos , openqa_attempt , k =3)
16 return x
Our multi-hop program adopts a very similar approach for
DEMONSTRATE andPREDICT . For SEARCH , it uses the
approach described in §2.4, with the following adjustments.
It uses result fusion across n= 10 queries per hop and,
among the npredictions, uses the summary corresponding
to the largest average log-probability. It uses a ﬁxed number
of hops for HotPotQA, i.e., two hops. In each prompt (i.e.,each hop and QA), it concatenates the summaries of all
previous hops (i.e., hop 1 onwards) and a total of k= 5
passages divided between the hops (i.e., ﬁve passages from
the ﬁrst hop or two passages from the ﬁrst and three from
the second).
For conversational QA, we use a simple PREDICT which
generates a response with greedy decoding, conditioned
on all of the previous turns of the conversation and ﬁve
retrieved passages. For SEARCH , our conversational QA
pipeline generates n= 10 re-written queries (and also uses
the simple query as the retrieve-and-read baseline; §3.3) and
fuses them as in §2.4. We implement DEMONSTRATE simi-
lar to openqa_demonstrate , but sample only four examples
(i.e., four conversational turns; instead of 16 questions as in
open-domain QA) for demonstrating the task for the higher-
order transformation convqa_attempt , which is passed to
annotate (not shown for brevity).
1def convqa_attempt (d: Example ) -> Example :
2 d. demos = all_but (demos , d) # all ( raw )
examples that don /quotesingle.Vart intersect with the
conversation of d
3
4 d = convqa_search (d, k =2)
5 if max ( precision (d. answer , p) for p in
d. passages ) < .8: return None # skip examples
where search fails
6
7 d = convqa_predict (d, n =20)
8 if max (F1(c.pred , d. answer ) for c in
d. candidates ) < .75: return None # skip
examples where predict fails out of n =20
attempts
9
10 return d
3.5. Development Datasets & Results
Open-SQuAD We conduct the open-domain version of
SQuAD over the Wikipedia 2016 corpus from Chen et al.
(2017), as processed by Khattab et al. (2021b). We use the
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
same train/validation/test splits as in Karpukhin et al. (2020)
and Khattab et al. (2021b).
Table 1 reports the answer EM and F1. The task-aware DSP
program achieves 36.6% EM, outperforming the vanilla LM
baseline by 126% EM relative gains. This indicates the im-
portance of grounding the LM’s predictions in retrieval, and
it shows that state-of-the-art retrievers like ColBERTv2 have
the capacity to do so off-the-shelf. The proposed DSP pro-
gram also achieves relative gains of 8% in EM and 6% in F1
over the retrieve-then-read pipeline, highlighting that non-
trivial gains are possible by aggregating information across
several retrieved passages as we do with self-consistency.
These in-context learning results are competitive with a
number of popular ﬁne-tuned systems. For instance, on
the Open-SQuAD test set, DPR achieves 29.8% EM, well
below our 16-shot DSP program. On the Open-SQuAD
dev set, the powerful Fusion-in-Decoder (Izacard & Grave,
2020) “base” approach achieves approximately 36% (i.e.,
very similar quality to our system) when invoked with ﬁve
retrieved passages. Nonetheless, with the default setting
of reading 100 passages, their system reaches 48% EM in
this evaluation. This may indicate that similar gains are
possible for our DSP program if the PREDICT stage is made
to aggregate information across many more passages.
For comparison, we also evaluate the self-ask pipeline,
which achieves 9.3% EM, suggesting that its ﬁxed pipeline
is ineffective outside its default multi-hop setting. Study-
ing a few examples of its errors reveals that it often de-
composes questions in tangential ways and answers these
questions instead. We refer to this behavior of the LMas
“self-distraction”, and we believe it adds evidence in favor of
our design decisions in DSP . To illustrate self-distraction,
when self-ask is prompted with “When does The Kidnap-
ping of Edgardo Mortara take place?”, it asks “What is The
Kidnapping of Edgardo Mortara“ and then asks when it was
published, a tangential question. Thus, self-ask answers
“1997”, instead of the time The Kidnapping of Edgardo
Mortara takes place (1858).
For reference, Table 1 also reports (as No-retrieval LM
SoTA) the concurrent in-context learning results from Si
et al. (2022) using code-davinci-002 , who achieve 20.2%
EM without retrieval and 34.0% EM with retrieval, albeit
on a different sample and split of the SQuAD data. Overall,
their approaches are very similar to the baselines we im-
plement (vanilla LM and retrieve-then-read), though their
retrieval-augmented approach retrieves (and concatenates
into the prompt) 10 passages from a Wikipedia dump.
HotPotQA We use the open-domain “fullwiki” setting
of HotPotQA using its ofﬁcial Wikipedia 2017 “abstracts”
corpus. The HotPotQA test set is hidden, so we reserve
the ofﬁcial validation set for our testing. We sub-dividethe training set into 90%/10% train/validation splits. In the
training (and thus validation) split, we keep only examples
marked as “hard” in the original dataset, which matches the
designation of the ofﬁcial validation and test sets.
We report the ﬁnal answer EM and F1 in Table 1. On
HotPotQA, the task-aware DSP program outperforms the
baselines and existing work by very wide margins, exceed-
ing the vanilla LM, the retrieve-then-read baseline, and the
self-ask pipeline by 82%, 39%, and 80%, respectively, in
EM. This highlights the effectiveness of building up more
sophisticated programs that coordinate the LM andRM for
the S EARCH step.
These results may be pegged against the evaluation on Hot-
PotQA in a number of concurrent papers. We ﬁrst compare
with non-retrieval approaches, though our comparisons must
be tentative due to variation in evaluation methodologies. Si
et al. (2022) achieve 25.2% EM with CoT prompting. With
a “recite-and-answer” technique for PaLM-62B (Chowdh-
ery et al., 2022), Sun et al. (2022) achieve 26.5% EM. Wang
et al. (2022b) achieve 33.8% EM and 44.6 F1 when apply-
ing a self-consistency prompt for PaLM-540B. Next, we
compare with a contemporaneous retrieval-based approach:
Yao et al. (2022) achieve 35.1% EM using a system capable
of searching using a Wikipedia API. All of these approaches
trail our task-aware DSP program, which achieves 51.4%
EM, by large margins.
QReCC We use QReCC (Anantha et al., 2020) in an open-
domain setting over Wikipedia 2018. QReCC does not have
an ofﬁcial development set, so we sub-divide the training
set into 90%/10% train/validation splits. For the ﬁrst ques-
tion in every conversation, we use the rewritten question
as the original question often assumes access to a ground-
truth document. We also ﬁlter low-quality examples from
QReCC.3
We conduct the QReCC conversations in an auto-regressive
manner. At turn t > 1of a particular conversation, the
system sees its own responses (i.e., not the ground truth
responses) to previous turns of the conversation. We report
the novel-F1 metric (nF1; Paranjape et al. 2022), which
computes the F1 overlap between the system response and
the ground truth while discounting common stopwords and
terms present in the question (or earlier questions). The
results are shown in Table 1, and follow the same general
pattern as SQuAD and HotPotQA.
3We remove conversations that have one or more empty ground-
truth answers and conversations that have only one or two ques-
tions. We also ﬁnd many conversations that include “what other
interesting facts are in this article?”, which conﬂict with the open-
domain formulation and have no well-deﬁned answer. Hence, we
remove any conversation that includes the keywords “other inter-
esting” or “else”, which we found to be markers of low quality.
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
4. Conclusion
For a long time, the dominant paradigm for building models
in AI has centered around multiplication of tensor repre-
sentations, and in the deep learning era this has given rise
to highly modular (layer-wise) designs that allow for fast
development and wide exploration. However, these design
paradigms require extensive domain expertise, and even
experts face substantial challenges when it comes to com-
bining different pretrained components into larger systems.
The promise of in-context learning is that we can build com-
plex systems from pretrained components using only natural
language as the medium for giving systems instructions and,
as we argue for, allowing components to communicate with
each other. In this new paradigm, the building blocks are
pretrained models and the core operations are natural lan-
guage instructions and operations on natural language texts.
If we can realize this potential, then we can broaden partici-
pation in AI system development, rapidly prototype systems
for new domains, and maximize the value of specialized
pretrained components.
In the current paper, we introduced the DEMONSTRATE –
SEARCH –PREDICT (DSP ) framework for retrieval aug-
mented in-context learning. DSP consists of a number of
simple, composable functions for implementing in-context
learning systems as deliberate programs —instead of end-
task prompts—for solving knowledge intensive tasks. We
implemented DSP as a Python library and used it to write
programs for Open-SQuAD, HotPotQA, and QReCC. These
programs deliver substantial gains over previous in-context
learning approaches. However, beyond any particular per-
formance number, we argue that the central contribution of
DSP is in helping to reveal a very large space of conceptual
possibilities for in-context learning in general.
Acknowledgements
We thank Ashwin Paranjape, Amir Ziai, and Rick Battle for
valuable discussions and feedback. This work was partially
supported by IBM as a founding member of the Stanford
Institute for Human-Centered Artiﬁcial Intelligence (HAI).
This research was supported in part by afﬁliate members and
other supporters of the Stanford DAWN project—Ant Fi-
nancial, Facebook, Google, and VMware—as well as Cisco,
SAP, and the NSF under CAREER grant CNS-1651570.
Any opinions, ﬁndings, and conclusions or recommenda-
tions expressed in this material are those of the authors and
do not necessarily reﬂect the views of the National Science
Foundation. We thank Giuseppe Attanasio for his public
LATEX GitHub-style Python code formatting gist.4We also
thank Riley Goodside for his public tips on formatting LM
4https://gist.github.com/g8a9/
07c2be12ae02cfad4aa430d77dc940cbprompts (at @goodside on Twitter).
References
Anantha, R., Vakulenko, S., Tu, Z., Longpre, S., Pulman,
S., and Chappidi, S. Open-domain question answering
goes conversational via question rewriting. arXiv preprint
arXiv:2010.04898 , 2020.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:
1877–1901, 2020.
Cao, G., Nie, J.-Y ., Gao, J., and Robertson, S. Selecting
good expansion terms for pseudo-relevance feedback. In
Proceedings of the 31st annual international ACM SIGIR
conference on Research and development in information
retrieval , pp. 243–250, 2008.
Chen, D., Fisch, A., Weston, J., and Bordes, A. Reading
Wikipedia to answer open-domain questions. In Proceed-
ings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pp.
1870–1879, Vancouver, Canada, 2017. Association for
Computational Linguistics. doi: 10.18653/v1/P17-1171.
URL https://aclanthology.org/P17-1171 .
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311 , 2022.
Del Tredici, M., Barlacchi, G., Shen, X., Cheng, W., and
de Gispert, A. Question rewriting for open-domain con-
versational qa: Best practices and limitations. In Pro-
ceedings of the 30th ACM International Conference on
Information & Knowledge Management , pp. 2974–2978,
2021.
Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D.,
Lopes, R. G., Wu, Y ., Michalewski, H., Saurous, R. A.,
Sohl-Dickstein, J., et al. Language model cascades. arXiv
preprint arXiv:2207.10342 , 2022.
Fox, E. A. and Shaw, J. A. Combination of multiple searches.
NIST special publication SP , 243, 1994.
Gao, L., Dai, Z., Pasupat, P., Chen, A., Chaganty, A. T., Fan,
Y ., Zhao, V . Y ., Lao, N., Lee, H., Juan, D.-C., et al. At-
tributed text generation via post-hoc research and revision.
arXiv preprint arXiv:2210.08726 , 2022.
Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and
Berant, J. Did aristotle use a laptop? a question answering
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
benchmark with implicit reasoning strategies. Transac-
tions of the Association for Computational Linguistics , 9:
346–361, 2021.
Hofstätter, S., Chen, J., Raman, K., and Zamani, H. Fid-
light: Efﬁcient and effective retrieval-augmented text
generation. arXiv preprint arXiv:2209.14290 , 2022.
Huang, J., Gu, S. S., Hou, L., Wu, Y ., Wang, X., Yu, H., and
Han, J. Large language models can self-improve. arXiv
preprint arXiv:2210.11610 , 2022.
Ishii, Y ., Madotto, A., and Fung, P. Survey of hallucination
in natural language generation. ACM Comput. Surv , 1(1),
2022.
Izacard, G. and Grave, E. Leveraging passage retrieval with
generative models for open domain question answering.
arXiv preprint arXiv:2007.01282 , 2020.
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni,
F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and
Grave, E. Few-shot learning with retrieval augmented lan-
guage models. arXiv preprint arXiv:2208.03299 , 2022.
Jiang, Y ., Bordia, S., Zhong, Z., Dognin, C., Singh, M.,
and Bansal, M. HoVer: A dataset for many-hop fact
extraction and claim veriﬁcation. In Findings of the
Association for Computational Linguistics: EMNLP
2020 , pp. 3441–3460, Online, 2020. Association for
Computational Linguistics. doi: 10.18653/v1/2020.
ﬁndings-emnlp.309. URL https://aclanthology.
org/2020.findings-emnlp.309 .
Karpukhin, V ., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov,
S., Chen, D., and Yih, W.-t. Dense passage retrieval
for open-domain question answering. In Proceedings
of the 2020 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP) , pp. 6769–6781,
Online, 2020. Association for Computational Linguis-
tics. doi: 10.18653/v1/2020.emnlp-main.550. URL
https://aclanthology.org/2020.emnlp-main.550 .
Khattab, O. and Zaharia, M. Colbert: Efﬁcient and effective
passage search via contextualized late interaction over
BERT. In Huang, J., Chang, Y ., Cheng, X., Kamps, J.,
Murdock, V ., Wen, J., and Liu, Y . (eds.), Proceedings
of the 43rd International ACM SIGIR conference on re-
search and development in Information Retrieval, SIGIR
2020, Virtual Event, China, July 25-30, 2020 , pp. 39–
48. ACM, 2020. doi: 10.1145/3397271.3401075. URL
https://doi.org/10.1145/3397271.3401075 .
Khattab, O., Potts, C., and Zaharia, M. Baleen: Robust
Multi-Hop Reasoning at Scale via Condensed Retrieval.
InThirty-Fifth Conference on Neural Information Pro-
cessing Systems , 2021a.Khattab, O., Potts, C., and Zaharia, M. Relevance-guided
supervision for openqa with ColBERT. Transactions of
the Association for Computational Linguistics , 9:929–
944, 2021b.
Khot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,
Clark, P., and Sabharwal, A. Decomposed prompting:
A modular approach for solving complex tasks. arXiv
preprint arXiv:2210.02406 , 2022.
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,
Y . Large language models are zero-shot reasoners. arXiv
preprint arXiv:2205.11916 , 2022.
Krishna, K., Chang, Y ., Wieting, J., and Iyyer, M. Rankgen:
Improving text generation with large ranking models.
arXiv preprint arXiv:2205.09726 , 2022.
Kurland, O. and Culpepper, J. S. Fusion in information
retrieval: Sigir 2018 half-day tutorial. In The 41st Inter-
national ACM SIGIR Conference on Research & Devel-
opment in Information Retrieval , pp. 1383–1386, 2018.
Kwiatkowski, T., Palomaki, J., Redﬁeld, O., Collins, M.,
Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin,
J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang,
M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S.
Natural questions: A benchmark for question answering
research. Transactions of the Association for Computa-
tional Linguistics , 7:452–466, 2019. doi: 10.1162/tacl_a_
00276. URL https://aclanthology.org/Q19-1026 .
Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grig-
orev, N. Internet-augmented language models through
few-shot prompting for open-domain question answering.
arXiv preprint arXiv:2203.05115 , 2022.
Le, N. T., Bai, F., and Ritter, A. Few-shot anaphora reso-
lution in scientiﬁc protocols via mixtures of in-context
experts. arXiv preprint arXiv:2210.03690 , 2022.
Lee, K., Chang, M.-W., and Toutanova, K. Latent re-
trieval for weakly supervised open domain question an-
swering. In Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics , pp. 6086–
6096, Florence, Italy, 2019. Association for Computa-
tional Linguistics. doi: 10.18653/v1/P19-1612. URL
https://aclanthology.org/P19-1612 .
Levine, Y ., Dalmedigos, I., Ram, O., Zeldes, Y ., Jan-
nai, D., Muhlgay, D., Osin, Y ., Lieber, O., Lenz, B.,
Shalev-Shwartz, S., et al. Standing on the shoul-
ders of giant frozen language models. arXiv preprint
arXiv:2204.10019 , 2022.
Lewis, P. S. H., Perez, E., Piktus, A., Petroni, F., Karpukhin,
V ., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rock-
täschel, T., Riedel, S., and Kiela, D. Retrieval-Augmented
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
Generation for Knowledge-Intensive NLP Tasks. In
Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.,
and Lin, H. (eds.), Advances in Neural Information
Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual , 2020. URL https:
//proceedings.neurips.cc/paper/2020/hash/
6b493230205f780e1bc26945df7481e5-Abstract.
html .
Li, X. L., Holtzman, A., Fried, D., Liang, P., Eisner, J.,
Hashimoto, T., Zettlemoyer, L., and Lewis, M. Con-
trastive decoding: Open-ended text generation as opti-
mization. arXiv preprint arXiv:2210.15097 , 2022.
Liu, J., Shen, D., Zhang, Y ., Dolan, B., Carin, L., and Chen,
W. What makes good in-context examples for gpt- 3?
arXiv preprint arXiv:2101.06804 , 2021.
McCann, B., Keskar, N. S., Xiong, C., and Socher, R.
The natural language decathlon: Multitask learning as
question answering. arXiv:1806.08730, 2018. URL
https://arxiv.org/abs/1806.08730 .
Min, S., Zhong, V ., Zettlemoyer, L., and Hajishirzi,
H. Multi-hop reading comprehension through ques-
tion decomposition and rescoring. arXiv preprint
arXiv:1906.02916 , 2019.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,
K., Ray, A., et al. Training language models to fol-
low instructions with human feedback. arXiv preprint
arXiv:2203.02155 , 2022.
Paranjape, A., Khattab, O., Potts, C., Zaharia, M., and
Manning, C. D. Hindsight: Posterior-guided Training
of Retrievers for Improved Open-ended Generation. In
International Conference on Learning Representations ,
2022. URL https://openreview.net/forum?id=Vr_
BTpw3wz .
Perez, E., Kiela, D., and Cho, K. True few-shot learning
with language models. Advances in Neural Information
Processing Systems , 34:11054–11070, 2021.
Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A.,
and Lewis, M. Measuring and narrowing the com-
positionality gap in language models. arXiv preprint
arXiv:2210.03350 , 2022.
Qi, P., Lee, H., Sido, O., Manning, C. D., et al. Retrieve,
rerank, read, then iterate: Answering open-domain ques-
tions of arbitrary complexity from text. arXiv preprint
arXiv:2010.12527 , 2020. URL https://arxiv.org/
abs/2010.12527 .Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
Sutskever, I., et al. Language models are unsupervised
multitask learners. OpenAI blog , 1(8):9, 2019.
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD:
100,000+ questions for machine comprehension of text.
InProceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing , pp. 2383–
2392, Austin, Texas, 2016. Association for Computa-
tional Linguistics. doi: 10.18653/v1/D16-1264. URL
https://aclanthology.org/D16-1264 .
Raposo, G., Ribeiro, R., Martins, B., and Coheur, L. Ques-
tion rewriting? assessing its importance for conversa-
tional question answering. In European Conference on
Information Retrieval , pp. 199–206. Springer, 2022.
Santhanam, K., Khattab, O., Potts, C., and Zaharia, M.
PLAID: An Efﬁcient Engine for Late Interaction Re-
trieval. arXiv preprint arXiv:2205.09707 , 2022a.
Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C.,
and Zaharia, M. ColBERTv2: Effective and efﬁcient
retrieval via lightweight late interaction. In Proceedings
of the 2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies , pp. 3715–3734, Seattle, United
States, July 2022b. Association for Computational Lin-
guistics. doi: 10.18653/v1/2022.naacl-main.272. URL
https://aclanthology.org/2022.naacl-main.272 .
Shuster, K., Poff, S., Chen, M., Kiela, D., and Weston, J.
Retrieval augmentation reduces hallucination in conver-
sation. arXiv preprint arXiv:2104.07567 , 2021.
Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber,
J., and Wang, L. Prompting gpt-3 to be reliable. arXiv
preprint arXiv:2210.09150 , 2022.
Sun, Z., Wang, X., Tay, Y ., Yang, Y ., and Zhou, D.
Recitation-augmented language models. arXiv preprint
arXiv:2210.01296 , 2022.
Thorne, J., Vlachos, A., Christodoulopoulos, C., and Mit-
tal, A. FEVER: a large-scale dataset for fact extrac-
tion and VERiﬁcation. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers) , pp. 809–819,
New Orleans, Louisiana, 2018. Association for Compu-
tational Linguistics. doi: 10.18653/v1/N18-1074. URL
https://aclanthology.org/N18-1074 .
Vakulenko, S., Kiesel, J., and Fröbe, M. SCAI-QReCC
shared task on conversational question answering. In Pro-
ceedings of the Thirteenth Language Resources and Eval-
uation Conference , pp. 4913–4922, Marseille, France,
DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
June 2022. European Language Resources Association.
URL https://aclanthology.org/2022.lrec-1.525 .
Wang, X., Macdonald, C., Tonellotto, N., and Ounis, I.
Colbert-prf: Semantic pseudo-relevance feedback for
dense passage and document retrieval. ACM Transac-
tions on the Web , 2022a.
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and
Zhou, D. Rationale-augmented ensembles in language
models. arXiv preprint arXiv:2207.00747 , 2022b.
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,
and Zhou, D. Self-consistency improves chain of
thought reasoning in language models. arXiv preprint
arXiv:2203.11171 , 2022c.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,
Le, Q., and Zhou, D. Chain of thought prompting elic-
its reasoning in large language models. arXiv preprint
arXiv:2201.11903 , 2022.
Wiher, G., Meister, C., and Cotterell, R. On decoding
strategies for neural text generators. arXiv preprint
arXiv:2203.15721 , 2022.
Xiong, W., Li, X. L., Iyer, S., Du, J., Lewis, P., Wang,
W. Y ., Mehdad, Y ., Yih, W.-t., Riedel, S., Kiela, D., et al.
Answering complex open-domain questions with multi-
hop dense retrieval. arXiv preprint arXiv:2009.12756 ,
2020. URL https://arxiv.org/abs/2009.12756 .
Xue, X. and Croft, W. B. Modeling reformulation using
query distributions. ACM Transactions on Information
Systems (TOIS) , 31(2):1–34, 2013.
Yang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W. W.,
Salakhutdinov, R., and Manning, C. D. Hotpotqa: A
dataset for diverse, explainable multi-hop question an-
swering. arXiv preprint arXiv:1809.09600 , 2018.
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
K., and Cao, Y . React: Synergizing reasoning and acting
in language models. arXiv preprint arXiv:2210.03629 ,
2022.
Zelikman, E., Wu, Y ., and Goodman, N. D. Star: Boot-
strapping reasoning with reasoning. arXiv preprint
arXiv:2203.14465 , 2022.
Zhang, Z., Zhang, A., Li, M., and Smola, A. Automatic
chain of thought prompting in large language models.
arXiv preprint arXiv:2210.03493 , 2022.
Zhong, V ., Shi, W., Yih, W.-t., and Zettlemoyer, L. Romqa:
A benchmark for robust, multi-evidence, multi-answer
question answering. arXiv preprint arXiv:2210.14353 ,
2022.