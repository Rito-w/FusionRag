Title: DAT: Dynamic Alpha Tuning for Hybrid Retrieval in Retrieval-Augmented Generation
arXiv ID: 2503.23013
Score: 0.9940
Total Pages: 12
Extraction Time: 2025-06-21 01:03:08
================================================================================


--- Page 1 ---
Preprint. Under review.
DAT: Dynamic Alpha T uning for Hybrid Retrieval in
Retrieval-Augmented Generation
Hsin-Ling Hsu
National Chengchi University
Taipei, Taiwan
112306092@nccu.edu.twJengnan Tzeng∗
National Chengchi University
Taipei, Taiwan
jengnan@math.nccu.edu.tw
Abstract
Hybrid retrieval techniques in Retrieval-Augmented Generation (RAG)
systems enhance information retrieval by combining dense and sparse (e.g.,
BM25-based) retrieval methods. However, existing approaches struggle
with adaptability, as fixed weighting schemes fail to adjust to different
queries. To address this, we propose DAT (Dynamic Alpha Tuning), a novel
hybrid retrieval framework that dynamically balances dense retrieval and
BM25 for each query. DAT leverages a large language model (LLM) to
evaluate the effectiveness of the top-1 results from both retrieval methods,
assigning an effectiveness score to each. It then calibrates the optimal
weighting factor through effectiveness score normalization, ensuring a
more adaptive and query-aware weighting between the two approaches.
Empirical results show that DAT consistently significantly outperforms
fixed-weighting hybrid retrieval methods across various evaluation metrics.
Even on smaller models, DAT delivers strong performance, highlighting its
efficiency and adaptability.
1 Introduction
Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) systems have emerged as a
powerful paradigm for enhancing the factuality and reliability of large language model
(LLM) outputs by grounding responses in external knowledge sources. At the core of
effective RAG systems lies the retrieval component, which is responsible for identifying and
surfacing the most relevant documents from a knowledge base in response to user queries.
The quality of retrieval directly impacts the overall performance of RAG systems, making it
a critical area for optimization.
Hybrid retrieval (Ma et al., 2020; Sawarkar et al., 2024; Berntson, 2023) approaches combining
sparse (e.g., BM25) and dense methods have demonstrated superior performance compared
to either method alone. BM25 (Robertson et al., 1994) excels at precise keyword matching
through term frequency calculations, while dense retrieval (Karpukhin et al., 2020) captures
semantic relationships that may not involve direct lexical overlap. While the complementary
strengths of these methods are well established, effectively balancing their contributions
remains challenging. Current approaches (Bruch et al., 2023) typically employ a fixed
weighting parameter ( α) determined through offline tuning on validation datasets. This
static weighting scheme, however, fails to account for the diverse nature of user queries,
where the optimal balance between keyword matching and semantic similarity varies
significantly based on query characteristics and knowledge base structure.
Recent efforts to address this limitation include approaches that assign different αvalues
based on query types (e.g., fact-seeking, concept-seeking, etc.) (Theja, 2024). However, these
methods still rely on predetermined categories with fixed weights and often overlook the
complex interplay between individual queries and the knowledge base. If this assumption
holds—that many queries benefit more from extreme values (i.e., pure BM25 or pure dense
∗Corresponding author
1arXiv:2503.23013v1  [cs.IR]  29 Mar 2025

--- Page 2 ---
Preprint. Under review.
retrieval)—then using a compromise value such as α=0.5, while seemingly optimal on
average, may in fact lead to suboptimal performance for most individual queries. This
would pose a significant challenge to hybrid retrieval optimization.
These limitations and opportunities motivate our research questions:
•How can we effectively combine sparse and dense retrieval methods to maximize
retrieval performance?
•How can a retrieval system adapt to the specific relationship between each query
and the knowledge base to determine optimal retrieval parameters?
•To what extent can dynamic weight adjustment improve individual query perfor-
mance compared to fixed weighting approaches?
•Can we design a hybrid retrieval system that balances effectiveness and efficiency
by leveraging minimal LLM reasoning?
Figure 1: Comparison between the traditional fixed alpha approach (top) and our proposed
DAT approach (bottom). While fixed alpha methods use predetermined weights regardless
of query characteristics, DAT dynamically adjusts the retrieval weighting coefficient based
on query-specific features, optimizing retrieval performance.
To address these challenges, we introduce DAT , a novel framework that adaptively adjusts
the retrieval weighting coefficient based on query-specific characteristics. The key insight
of our approach is that the optimal hybrid weighting αfor each query should reflect the
relative effectiveness of sparse and dense retrieval methods in that specific context. As
shown in Figure 1, DAT dynamically computes an optimal αvalue for each query while
minimizing computational overhead.
Our approach evaluates the relative effectiveness of each retrieval method by comparing
the top-1 retrieved result from both BM25 and dense retrieval. This design is based on
the understanding that sparse retrieval effectiveness depends primarily on term overlap
2

--- Page 3 ---
Preprint. Under review.
between queries and documents, while dense retrieval relies on effective semantic embed-
ding alignment. By assessing only the top result from each method with an LLM-based
effectiveness scoring mechanism, we can efficiently determine which retrieval method
performs better for a given query without the computational burden of evaluating multiple
documents. This lightweight evaluation provides a strong signal for estimating relative
retrieval strength for dynamic αcalculation.
Our contributions are four-fold:
•We propose a query-adaptive framework that dynamically calibrates the weighting
between sparse and dense retrieval methods, eliminating reliance on predetermined,
static αvalues.
•We introduce an LLM-based effectiveness scoring mechanism that evaluates re-
trieval effectiveness based on the specific relationship between each query and the
knowledge base, rather than on general query characteristics.
•We demonstrate that evaluating only the top result from each retrieval method
provides sufficient signal for effective weighting decisions, offering a significantly
more cost-efficient approach to hybrid retrieval optimization.
•We demonstrate through extensive experiments that DAT significantly outperforms
fixed-weight hybrid approaches, particularly for queries where retrieval methods
exhibit varying effectiveness.
Our empirical results show that DAT consistently improves retrieval performance across
various evaluation metrics, with particularly substantial gains on challenging queries where
standard hybrid approaches struggle. Moreover, we demonstrate that our approach reduces
the variance in performance across individual queries, providing more consistent user
experiences.
2 Related Work
Hybrid retrieval systems combine the complementary strengths of multiple retrieval meth-
ods. BM25 (Robertson et al., 1994), the predominant sparse retrieval algorithm, calculates
relevance based on term frequency and inverse document frequency, efficiently handling
exact keyword matching. Meanwhile, dense retrieval approaches (Karpukhin et al., 2020)
leverage vector embeddings to capture semantic relationships beyond lexical overlap. The
effectiveness of combining sparse and dense methods has been comprehensively demon-
strated in hybrid RAG systems, where improved retrieval quality significantly enhances
generation accuracy (Ma et al., 2020; Sawarkar et al., 2024; Berntson, 2023).
The challenge of determining optimal weighting coefficients has traditionally been ad-
dressed through offline tuning (Bruch et al., 2023), where experiments on validation sets
establish a fixed weight that is then applied universally to all future queries, regardless of
their characteristics. Some approaches (Jeong et al., 2024) attempt refinement by classify-
ing queries into predefined types. LlamaIndex (Theja, 2024) proposed assigning different
weights based on predefined query categories, though this approach still relies on static
weights per category.
Our work differs by leveraging LLMs’ reasoning capabilities to dynamically assess retrieval
quality and calibrate weighting parameters per query, without relying on predefined cate-
gories or fixed weights from offline tuning. This approach aligns with recent trends in using
LLMs as judges (Gu et al., 2024) but applies this concept specifically to adaptive hybrid
retrieval optimization. Unlike previous approaches that optimize parameters across entire
query sets, DAT addresses the limitations of fixed-weight systems by dynamically adapting
to each query’s unique characteristics and its relationship to the knowledge base.
3

--- Page 4 ---
Preprint. Under review.
3 Hybrid Retrieval
Hybrid retrieval combines sparse and dense retrieval to leverage both keyword matching
and semantic understanding. Sparse methods like BM25 score documents based on lexical
overlap:
BM25 (q,d) =n
∑
i=1IDF(qi)·f(qi,d)·(k1+1)
f(qi,d) +k1·(1−b+b·|d|
avgdl)(1)
Dense retrieval encodes queries and documents into vectors using embedding functions Eq
and Ed, with similarity computed via cosine similarity:
Sim dense(q,d) =cos(Eq(q),Ed(d)) =Eq(q)·Ed(d)
||Eq(q)|| · || Ed(d)||(2)
To combine these methods, hybrid systems typically use techniques such as the rela-
tiveScoreFusion algorithm (Kulawiak & Hwang, 2023) which balances the influence of
different retrieval approaches. First, we normalize the similarity scores from both retrieval
methods using min-max scaling:
˜Sdense(q,d) =Sim dense(q,d)−mind′∈Ddense(q)(Sim dense(q,d′))
maxd′∈Ddense(q)(Sim dense(q,d′))−mind′∈Ddense(q)(Sim dense(q,d′))(3)
˜SBM25(q,d) =BM25 (q,d)−mind′∈DBM25(q)(BM25 (q,d′))
maxd′∈DBM25(q)(BM25 (q,d′))−mind′∈DBM25(q)(BM25 (q,d′))(4)
where ˜Sdense(q,d)and ˜SBM25(q,d)are normalized scores in the range [0, 1]. Hybrid systems
typically combine these normalized scores using a fixed weighting parameter α:
R(q,d) =α·˜Sdense(q,d) + ( 1−α)·˜SBM25(q,d) (5)
This fixed αis determined via offline tuning on validation data and then applied uniformly
to all queries. However, such a static strategy fails to account for the varying nature of user
queries, limiting retrieval effectiveness across diverse scenarios.
4 DAT
To overcome the limitations of static weighting in hybrid retrieval, we introduce DAT, a
query-adaptive framework that dynamically adjusts the retrieval weighting coefficient based
on the effectiveness of each method for a given query. The central intuition is that different
queries inherently favor different retrieval strategies—some require precise keyword overlap
(favoring BM25), while others rely on semantic alignment (favoring dense retrieval).
DAT harnesses the reasoning capabilities of large language models (LLMs) to estimate the
optimal weighting coefficient α(q)for each query at runtime. Unlike traditional approaches
that depend on predefined heuristics or offline-tuned parameters, our method adapts
on-the-fly to each query’s unique interaction with the knowledge base.
Given a query q, we retrieve the top-1 result from both sparse and dense retrieval methods:
dv,1∈Ddense(q)anddb,1∈DBM25(q). These top-ranked documents are treated as represen-
tative indicators of each method’s retrieval effectiveness for the specific query. This focused
sampling strategy minimizes computational cost while providing sufficient signal to inform
adaptive weighting decisions.
4.1 LLM-Based Retrieval Effectiveness Scoring
A key component of DAT is the use of LLMs as evaluators of retrieval quality. We posit
that LLMs, with their deep semantic understanding, can assess the relevance of a retrieved
document to the original query and thereby estimate each retrieval method’s relative
effectiveness.
4

--- Page 5 ---
Preprint. Under review.
To formalize this, we define a scoring function S(q,d) =fLLM(q,d), which returns an effec-
tiveness score in the discrete range {0, 1, 2, 3, 4, 5 }—with higher values indicating greater
effectiveness. The scoring rubric is carefully designed to reflect retrieval effectiveness:
•5 points : Direct hit—the retrieved document directly answers the question.
•3–4 points : Good wrong result—the document is conceptually close to the correct
answer, indicating high likelihood that correct answers are nearby.
•1–2 points : Bad wrong result—the document is loosely related but misleading, with
low likelihood that correct answers are nearby.
•0 points : Completely off-track—the result is totally unrelated to the query.
Our prompting strategy (see Appendix A) guides the LLM to prioritize factual alignment
and informational completeness over superficial similarity or stylistic matching. The LLM
independently evaluates each of the top-1 documents and assigns scores: Sv(q) =S(q,dv,1)
for dense retrieval and Sb(q) = S(q,db,1)for BM25. This decoupled assessment ensures
that the relative retrieval effectiveness is directly captured and can inform downstream
weighting.
4.2 Dynamic Alpha Calculation
Using the LLM-assigned scores, we compute the dynamic weighting coefficient α(q)through
a case-aware formulation that ensures robust behavior across various retrieval outcomes:
α(q) =

0.5, if Sv(q) =0 and Sb(q) =0,
1.0, if Sv(q) =5 and Sb(q)̸=5,
0.0, if Sb(q) =5 and Sv(q)̸=5,
Sv(q)
Sv(q)+Sb(q)otherwise.(6)
This rule-based approach ensures:
• Equal weighting (0.5) when both retrieval methods fail to return relevant content.
•Exclusive preference (1.0 or 0.0) when one method yields a perfect result and the
other does not.
• Proportional weighting when both methods return partially relevant results.
For stability and implementation consistency, the final α(q)value is rounded to one decimal
place before being applied in the hybrid scoring function.
4.3 Final Score Fusion
With the dynamically determined α(q), we compute the final hybrid ranking score by
applying the weighted combination to the normalized scores from both retrieval methods:
R(q,d) =α(q)·˜Sdense(q,d) + ( 1−α(q))·˜SBM25(q,d) (7)
Documents are then ranked based on R(q,d), and the top- Kresults form the final retrieval
output Dfinal(q) ={d1,d2, ...,dK}that is passed to the generation component of the RAG
system. Through this dynamic approach, DAT effectively overcomes the limitations of fixed-
weight hybrid retrieval methods by intelligently adapting to each query’s characteristics.
This query-specific adaptation leads to more relevant and accurate retrieval results across
diverse query types, enhancing the overall performance of RAG systems.
5 Experiments
5.1 Experimental Setup
Datasets and Preprocessing To evaluate the effectiveness and generalizability of our pro-
posed method, we conducted experiments on two benchmark datasets: SQuAD (Rajpurkar
5

--- Page 6 ---
Preprint. Under review.
et al., 2016), a widely used dataset for evaluating retrieval-based question answering in
English, and DRCD (Shao et al., 2019), a large-scale traditional Chinese machine reading
comprehension dataset.
While SQuAD provides a standard benchmark, we include DRCD to examine whether the
proposed method can also perform well in a different language setting. As Chinese is one
of the most widely spoken languages, DRCD serves as a valuable testbed for assessing the
broader applicability of our approach.
For each dataset, we constructed an evaluation corpus by randomly sampling articles from
the original document collection. For each selected article, we included all its paragraphs
P={p1,p2,. . .}and the corresponding questions Q={q1,q2,. . .}such that each question
qi∈ Q is answerable by a span in paragraph pi∈ P. The sampling process continued until
the number of questions approached 3000, stopping before the next sampled article would
exceed this threshold. This yields a paragraph corpus Peval⊂ P and a query set Qeval⊂ Q ,
with aligned pairs (qi,pi)forming the ground truth for retrieval.
To better focus our evaluation, we identified a subset of queries Qhybrid ⊂ Q eval where
hybrid retrieval strategies can actually make a difference. Our analysis revealed that for
many queries in Qeval\ Qhybrid , retrieval performance remained identical regardless
of the αvalue used—suggesting these queries were too simple to benefit from hybrid
approaches and could be optimally retrieved using either BM25 or dense retrieval alone. In
contrast, the Qhybrid subset specifically contains queries where BM25 and dense retrieval
produce different rankings, and where the choice of αdirectly impacts whether the correct
document appears at the top position. This hybrid-sensitive subset serves as a focused
testbed for evaluating the effectiveness of dynamic weighting strategies in scenarios where
hybrid retrieval is truly beneficial. Detailed dataset statistics are summarized in Table 1.
Dataset Articles Paragraphs Questions Hybrid-Sensitive
SQuAD 13 585 2976 1111
DRCD 318 908 3000 1523
Table 1: Dataset statistics for SQuAD and DRCD evaluation corpora.
The retrieval task involves identifying the most relevant paragraph ˆpi∈ P evalfor each query
qi∈ Q eval. A retrieval is considered successful if ˆpimatches the ground truth paragraph pi
that contains the answer.
Metrics and Evaluation Protocol To evaluate retrieval performance, we use Precision@1
and Mean Reciprocal Rank at 20 (MRR@20). Precision@1 measures the fraction of queries
where the correct answer appears as the top-ranked retrieved document. MRR@20 assesses
ranking quality by computing the reciprocal rank of the first correct document (up to
position 20) and averaging across queries. These metrics effectively quantify retrieval
accuracy and ranking effectiveness. We conduct our evaluation in two phases: a Complete
Dataset Evaluation on the entire query set Qevaland a more focused Hybrid-Sensitive
Analysis on the subset Qhybrid where hybrid retrieval methods can make a meaningful
difference.
Baseline Methods We compare our proposed DAT framework against several baseline
retrieval methods. The first baseline is BM25 Only ( α=0), a sparse retrieval approach using
only BM25 scores. For English (SQuAD) datasets, we use standard word tokenization, while
for Chinese (DRCD) datasets, we adopt the tokenizer from ckiplab/albert-base-chinese1.
The second baseline is Dense Only ( α=1), which ranks paragraphs based on cosine similar-
ity between query and paragraph embeddings obtained from the text-embedding-3-large
model (OpenAI, 2024c). The third baseline is Fixed Hybrid ( α=α∗), a hybrid method that
linearly combines BM25 and dense scores with a fixed weighting parameter α∗. For both
datasets, we conducted exhaustive grid search over αvalues from 0 to 1 with a step size of
1https://huggingface.co/ckiplab/albert-base-chinese
6

--- Page 7 ---
Preprint. Under review.
0.1, and found that α∗=0.6 maximized retrieval accuracy on both validation sets, making
it the optimal fixed weighting value for our experiments.
Model Implementation For our proposed DAT method, we experiment with three dif-
ferent base models to demonstrate the robustness of our approach across various model
sizes and architectures: GPT-4o, OpenAI’s model (OpenAI, 2024a) estimated to have ap-
proximately 200B parameters (Abacha et al., 2025); GPT-4o-mini, OpenAI’s model (OpenAI,
2024b) estimated to have approximately 8B parameters (Abacha et al., 2025); and DeepSeek-
R1-Distill-Qwen-14B, DeepSeek’s 14B parameter open source model (DeepSeek-AI, 2025).
5.2 Results
5.2.1 Complete Dataset Evaluation
We first evaluate all methods on the complete datasets Qevalfor each dataset. Table 2 shows
the accuracy of αselection for both SQuAD and DRCD, measuring how often each method
selects the optimal weighting value for a given query. We define the optimal weighting
value as the αthat produces the highest retrieval ranking for the ground truth paragraph pi
given query qi.
The results in Table 2 show that DAT variants achieve higher alpha selection accuracy than
fixed weighting approaches on both datasets, with GPT-4o reaching 0.9234 accuracy on
SQuAD and GPT-4o-mini achieving 0.9013 on DRCD compared to 0.8975 and 0.8623 for the
Fixed Hybrid approach, respectively.
The results in Table 3 demonstrate that our DAT approach consistently outperforms both
single-method retrieval and fixed-weight hybrid retrieval across both datasets. For SQuAD,
even the DAT variant using the small model (DeepSeek-R1-Distill-Qwen-14B) achieves
significant improvements over the best fixed hybrid approach, with a ˜2% increase in
Precision@1. Similarly for DRCD, all DAT variants provide notable improvements, with
GPT-4o achieving a ˜3.3% increase in Precision@1.
Interestingly, we observe that while dense retrieval performs relatively well on the SQuAD
dataset, BM25 shows stronger performance on the DRCD dataset, suggesting different
retrieval dynamics across datasets. Despite these differences, DAT effectively adapts to both
scenarios by dynamically selecting appropriate alpha values.
Method SQuAD DRCD
BM25 Only ( α=0.0) 0.7981 0.8110
Dense Only ( α=1.0) 0.7789 0.6156
Fixed Hybrid ( α=0.6) 0.8975 0.8623
DAT (DeepSeek-R1-Distill-Qwen-14B) 0.9163 0.8897
DAT (GPT-4o-mini) 0.9194 0.9013
DAT (GPT-4o) 0.9234 0.9010
Table 2: Alpha Selection Accuracy on Complete Datasets Qeval
MethodSQuAD DRCD
Precision@1 MRR@20 Precision@1 MRR@20
BM25 Only ( α=0.0) 0.7594 0.8223 0.7630 0.8134
Dense Only ( α=1.0) 0.7396 0.8119 0.5743 0.6708
Fixed Hybrid ( α=0.6) 0.8461 0.8997 0.8113 0.8619
DAT (DeepSeek-R1-Distill-Qwen-14B) 0.8663 0.9079 0.8347 0.8711
DAT (GPT-4o-mini) 0.8676 0.9093 0.8417 0.8796
DAT (GPT-4o) 0.8740 0.9130 0.8440 0.8807
Table 3: Retrieval Performance on Complete Datasets Qeval
7

--- Page 8 ---
Preprint. Under review.
5.2.2 Hybrid-Sensitive Analysis
While the complete dataset evaluation demonstrates the overall effectiveness of DAT, we
now focus specifically on the hybrid-sensitive subsets Qhybrid (1111 queries for SQuAD and
1523 for DRCD) to examine performance specifically on queries where different retrieval
methods produce varying rankings, making the weighting between methods critical for
optimal results.
Method SQuAD DRCD
BM25 Only ( α=0.0) 0.4590 0.6277
Dense Only ( α=1.0) 0.4077 0.2429
Fixed Hybrid ( α=0.6) 0.7255 0.7288
DAT (DeepSeek-R1-Distill-Qwen-14B) 0.7759 0.7827
DAT (GPT-4o-mini) 0.7840 0.8056
DAT (GPT-4o) 0.7948 0.8050
Table 4: Alpha Selection Accuracy on Hybrid-Sensitive Subsets Qhybrid
MethodSQuAD DRCD
Precision@1 MRR@20 Precision@1 MRR@20
BM25 Only ( α=0.0) 0.3906 0.5420 0.5555 0.6446
Dense Only ( α=1.0) 0.3375 0.5143 0.1838 0.3636
Fixed Hybrid ( α=0.6) 0.6229 0.7493 0.6507 0.7401
DAT (DeepSeek-R1-Distill-Qwen-14B) 0.6769 0.7712 0.6967 0.7582
DAT (GPT-4o-mini) 0.6805 0.7750 0.7104 0.7749
DAT (GPT-4o) 0.6976 0.7849 0.7150 0.7771
Table 5: Retrieval Performance on Hybrid-Sensitive Subsets Qhybrid
The results in Table 4 demonstrate that DAT variants achieve significantly higher alpha
selection accuracy compared to fixed weighting approaches on both hybrid-sensitive subsets,
with GPT-4o reaching 0.7948 accuracy for SQuAD and GPT-4o-mini achieving 0.8056 for
DRCD versus approximately 0.73 for the Fixed Hybrid approach on both datasets. This
improved ability to dynamically select optimal weightings directly translates to enhanced
retrieval performance shown in Table 5.
The results on both hybrid-sensitive subsets reveal several important insights:
1.The performance gap between single-method retrieval and hybrid methods is sub-
stantially wider on these subsets, highlighting the importance of effective weighting
in challenging cases.
2.DAT consistently outperforms the fixed hybrid approach, with the best variant
(GPT-4o) achieving a ˜7.5% improvement in Precision@1 for SQuAD and a ˜6.4%
improvement for DRCD over fixed hybrid weighting.
3.Even the smaller model variants demonstrate significant improvements, with
DeepSeek-R1-Distill-Qwen-14B achieving a ˜5.4% improvement in Precision@1
for SQuAD and a ˜4.6% improvement for DRCD over fixed hybrid weighting.
These findings confirm that our dynamic alpha tuning approach is particularly valuable for
complex queries where different retrieval methods exhibit varying effectiveness, precisely
the scenarios where intelligent weighting is most needed.
8

--- Page 9 ---
Preprint. Under review.
6 Scoring Mechanism Analysis
To provide deeper insight into DAT’s effectiveness, we analyze how the LLM-based scoring
mechanism evaluates retrieval quality and dynamically adjusts αvalues through represen-
tative case studies from both datasets.
6.1 SQuAD Query Example
For the SQuAD query “What gun did the Royal Navy start using?”, the correct answer
mentions Britain’s 3.7-inch HAA gun. The Top-1 result from dense retriever returned: “By
the early 20th century balloon, or airship, guns, for land and naval use were attracting
attention. Various types of ammunition were proposed...” The Top-1 result from BM25
retriever returned: “AAA battalions were also used to help suppress ground targets. Their
larger 90 mm M3 gun would prove... Also available to the Americans at the start of the war
was the 120 mm M1 gun...”
The LLM’s reasoning process determined:
•The dense result provides topical relevance to naval guns but lacks specificity about
Royal Navy adoption. Score: 3/5.
•The BM25 result focuses on American artillery unrelated to the Royal Navy. Score:
2/5.
This yielded α=3/(3+2) = 0.6, appropriately favoring the semantically-related dense
result that was more likely to lead to relevant information.
6.2 DRCD Query Example
For the DRCD query “ 水分子中的質子在高溫中與鋯進行無氧性氧化反應後什麼物質會產
生?” (What substance is produced when protons in water molecules react anaerobically
with zirconium at high temperatures?), the Top-1 result from dense retriever returned: “ 氫
在氧化後會失去它的電子，形成氫陽離子。氫陽離子不含電子，其原子核通常只含一個質
子...” (When hydrogen is oxidized, it loses its electron and becomes a hydrogen ion...). The
Top-1 result from BM25 retriever returned: “ 在無氧條件下，鐵和合成鋼會被水分子中的
質子緩慢氧化，而水則會還原成分子氫...” (Under anaerobic conditions, iron and steel are
oxidized by protons in water molecules, producing molecular hydrogen...).
The LLM evaluated:
•The dense result explains hydrogen ions but lacks discussion of their reaction with
zirconium. Score: 3/5.
•The BM25 result discusses a reaction mechanism with iron that parallels what occurs
with zirconium, correctly identifying hydrogen gas as the product. Score: 4/5.
This produced α=3/(3+4) =0.43 (rounded to 0.4), appropriately weighting toward the
more relevant result.
These examples demonstrate how DAT’s scoring mechanism effectively balances retrieval
methods based on result quality rather than relying on predefined query categories. By
evaluating the specific relationship between each query and the retrieved contents, the
system adapts to diverse information needs.
7 Conclusion
We introduced DAT, a framework that dynamically adjusts the weighting between sparse
and dense retrieval for each query by leveraging LLMs to evaluate document effectiveness.
Unlike static weighting schemes, DAT adaptively selects the optimal αper query, achieving a
balance between performance and efficiency—even with smaller models. Experiments show
9

--- Page 10 ---
Preprint. Under review.
that DAT consistently outperforms fixed-weight hybrids, especially on hybrid-sensitive
queries, and remains robust across different LLM sizes. These results underscore the
limitations of static approaches and highlight the value of query-adaptive strategies.
References
Asma Ben Abacha, Wen wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, and
Thomas Lin. Medec: A benchmark for medical error detection and correction in clinical
notes. arXiv preprint arXiv:2412.19260 , 2025.
Alec Berntson. Azure ai search: Outperforming vector search with hybrid retrieval and
reranking. https://techcommunity.microsoft.com/blog/azure-ai-services-blog/az
ure-ai-search-outperforming-vector-search-with-hybrid-retrieval-and-reranki
ng/3929167 , 2023.
Sebastian Bruch, Siyu Gai, and Amir Ingber. An analysis of fusion functions for hybrid
retrieval. ACM Transactions on Information Systems , 42(1):1–35, August 2023. ISSN 1558-
2868. doi: 10.1145/3596512. URL http://dx.doi.org/10.1145/3596512 .
DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement
learning, 2025. URL https://arxiv.org/abs/2501.12948 .
Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li,
Yinghan Shen, Shengjie Ma, Honghao Liu, Yuanzhuo Wang, and Jian Guo. A survey on
llm-as-a-judge. arXiv preprint arXiv: 2411.15594 , 2024.
Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park. Adaptive-
RAG: Learning to adapt retrieval-augmented large language models through question
complexity. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the
2024 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies (Volume 1: Long Papers) , pp. 7036–7050, Mexico City, Mexico,
June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.
389. URL https://aclanthology.org/2024.naacl-long.389/ .
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,
Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answer-
ing. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 6769–6781,
Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/20
20.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550/ .
Dirk Kulawiak and Joon-Pil Hwang. Unlocking the power of hybrid search - a deep dive
into weaviate’s fusion algorithms. https://weaviate.io/blog/hybrid-search-fusion-a
lgorithms , 2023.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¨aschel, Sebastian Riedel,
and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In
H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems , volume 33, pp. 9459–9474. Curran Associates, Inc., 2020.
URL https://proceedings.neurips.cc/paper files/paper/2020/file/6b493230205f7
80e1bc26945df7481e5-Paper.pdf .
Ji Ma, Ivan Korotkov, Keith B. Hall, and Ryan T. McDonald. Hybrid first-stage retrieval
models for biomedical literature. In Conference and Labs of the Evaluation Forum , 2020.
OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/ , 2024a.
OpenAI. Gpt-4o mini: advancing cost-efficient intelligence. https://openai.com/index/g
pt-4o-mini-advancing-cost-efficient-intelligence/ , 2024b.
OpenAI. text-embedding-3-large. https://platform.openai.com/docs/models/text-emb
edding-3-large , 2024c.
10

--- Page 11 ---
Preprint. Under review.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+
questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras
(eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing ,
pp. 2383–2392, Austin, Texas, November 2016. Association for Computational Linguistics.
doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264 .
Stephen Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike
Gatford. Okapi at trec-3. pp. 0–, 01 1994.
Kunal Sawarkar, Abhilasha Mangal, and Shivam Raj Solanki. Blended rag: Improving rag
(retriever-augmented generation) accuracy with semantic search and hybrid query-based
retrievers. In 2024 IEEE 7th International Conference on Multimedia Information Processing
and Retrieval (MIPR) , volume 24, pp. 155–161. IEEE, August 2024. doi: 10.1109/mipr6220
2.2024.00031. URL http://dx.doi.org/10.1109/MIPR62202.2024.00031 .
Chih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng, and Sam Tsai. Drcd: a chinese machine
reading comprehension dataset. arXiv preprint arXiv:1806.00920 , 2019.
Ravi Theja. Llamaindex: Enhancing retrieval performance with alpha tuning in hybrid
search in rag. https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-per
formance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00 , 2024.
A Prompt Template
You are an evaluator assessing the retrieval effectiveness of dense
retrieval ( Cosine Distance ) and BM25 retrieval for finding the
correct answer .
## Task :
Given a question and two top1 search results ( one from dense retrieval ,
one from BM25 retrieval ), score each retrieval method from **0 to 5**
based on whether the correct answer is likely to appear in top2 ,
top3 , etc .
### ** Scoring Criteria :**
1. ** Direct hit --> 5 points **
- If the retrieved document directly answers the question , assign **5
points **.
2. ** Good wrong result ( High likelihood correct answer is nearby ) --> 3-4
points **
- If the top1 result is ** conceptually close ** to the correct answer (
e.g., mentions relevant entities , related events , partial answer ),
it indicates the search method is in the right direction .
- Give **4** if it 's very close , **3** if somewhat close .
3. ** Bad wrong result ( Low likelihood correct answer is nearby ) --> 1-2
points **
- If the top1 result is ** loosely related but misleading ** (e.g.,
shares keywords but changes context ), correct answers might not be
in top2 , top3 .
- Give **2** if there 's a small chance correct answers are nearby ,
**1** if unlikely .
4. ** Completely off - track --> 0 points **
- If the result is ** totally unrelated **, it means the retrieval
method is failing .
---
### ** Given Data :**
- ** Question :** "{ question }"
11

--- Page 12 ---
Preprint. Under review.
- ** dense retrieval Top1 Result :** "{ vector_reference }"
- ** BM25 retrieval Top1 Result :** "{ bm25_reference }"
---
### ** Output Format :**
Return two integers separated by a space :
- ** First number :** dense retrieval score .
- ** Second number :** BM25 retrieval score .
- Example output : 3 4
( Vector : 3, BM25 : 4)
** Do not output any other text .**
B Limitations
While our DAT framework demonstrates significant improvements over fixed-weight hybrid
retrieval methods, several limitations warrant discussion. DAT’s dependence on LLM-based
effectiveness scoring introduces computational overhead that may increase latency and cost
in production environments, despite our efforts to mitigate this by only evaluating top-1
results. However, as large language models continue to evolve and computational hardware
advances, we anticipate these efficiency constraints will gradually diminish, expanding the
practical deployment scenarios for our approach.
12
