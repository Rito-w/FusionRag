#!/usr/bin/env python3
"""
æ·±å…¥åˆ†æç°æœ‰è®ºæ–‡ï¼Œæå–åˆ›æ–°ç‚¹å’ŒæŠ€æœ¯æ–¹æ¡ˆ
"""
import os
import json
from pathlib import Path
import PyPDF2
import re
from datetime import datetime

def extract_text_from_pdf(pdf_path):
    """ä»PDFä¸­æå–æ–‡æœ¬"""
    try:
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
        return text
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–PDF {pdf_path}: {e}")
        return ""

def analyze_paper_content(text, paper_name):
    """åˆ†æè®ºæ–‡å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯"""
    analysis = {
        'paper_name': paper_name,
        'abstract': '',
        'key_contributions': [],
        'methodology': '',
        'experiments': '',
        'limitations': '',
        'innovation_points': []
    }
    
    # æå–æ‘˜è¦
    abstract_match = re.search(r'Abstract\s*\n(.*?)\n\s*(?:1\s+Introduction|Introduction)', text, re.DOTALL | re.IGNORECASE)
    if abstract_match:
        analysis['abstract'] = abstract_match.group(1).strip()[:500] + "..."
    
    # æŸ¥æ‰¾å…³é”®è¯
    innovation_keywords = [
        'dynamic', 'adaptive', 'attention', 'fusion', 'hybrid', 'retrieval',
        'multimodal', 'cross-modal', 'reranking', 'weight', 'learning',
        'neural', 'transformer', 'embedding', 'index', 'query'
    ]
    
    found_keywords = []
    for keyword in innovation_keywords:
        if keyword.lower() in text.lower():
            count = text.lower().count(keyword.lower())
            if count > 3:  # åªè®°å½•å‡ºç°é¢‘ç‡è¾ƒé«˜çš„å…³é”®è¯
                found_keywords.append(f"{keyword}({count})")
    
    analysis['innovation_points'] = found_keywords
    
    # æŸ¥æ‰¾æ–¹æ³•è®ºéƒ¨åˆ†
    method_match = re.search(r'(?:Methodology|Method|Approach)\s*\n(.*?)\n\s*(?:\d+\s+|Experiment)', text, re.DOTALL | re.IGNORECASE)
    if method_match:
        analysis['methodology'] = method_match.group(1).strip()[:300] + "..."
    
    return analysis

def analyze_category(category_path, category_name):
    """åˆ†ææŸä¸ªç±»åˆ«çš„æ‰€æœ‰è®ºæ–‡"""
    print(f"\nğŸ“š åˆ†æç±»åˆ«: {category_name}")
    
    pdf_files = list(Path(category_path).glob("*.pdf"))
    analyses = []
    
    for pdf_file in pdf_files:
        print(f"  ğŸ“„ åˆ†æ: {pdf_file.name}")
        text = extract_text_from_pdf(pdf_file)
        if text:
            analysis = analyze_paper_content(text, pdf_file.name)
            analyses.append(analysis)
            print(f"    âœ… å®Œæˆï¼Œå…³é”®è¯: {', '.join(analysis['innovation_points'][:5])}")
        else:
            print(f"    âŒ æ— æ³•æå–æ–‡æœ¬")
    
    return analyses

def main():
    """ä¸»åˆ†æå‡½æ•°"""
    print("ğŸ” å¼€å§‹æ·±å…¥åˆ†æç°æœ‰è®ºæ–‡...")
    
    # å®šä¹‰è¦åˆ†æçš„é‡ç‚¹ç±»åˆ«
    categories = {
        "01_hybrid_retrieval": "æ··åˆæ£€ç´¢",
        "02_multimodal_retrieval": "å¤šæ¨¡æ€æ£€ç´¢", 
        "07_core_papers": "æ ¸å¿ƒè®ºæ–‡"
    }
    
    all_analyses = {}
    
    for cat_dir, cat_name in categories.items():
        if os.path.exists(cat_dir):
            analyses = analyze_category(cat_dir, cat_name)
            all_analyses[cat_dir] = analyses
        else:
            print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {cat_dir}")
    
    # ä¿å­˜åˆ†æç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_file = f"paper_analysis_results_{timestamp}.json"
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(all_analyses, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ åˆ†æç»“æœä¿å­˜è‡³: {result_file}")
    
    # ç”Ÿæˆåˆ›æ–°ç‚¹æ€»ç»“
    generate_innovation_summary(all_analyses)
    
    return result_file

def generate_innovation_summary(all_analyses):
    """ç”Ÿæˆåˆ›æ–°ç‚¹æ€»ç»“"""
    print("\n" + "="*60)
    print("ğŸ¯ åˆ›æ–°ç‚¹åˆ†ææ€»ç»“")
    print("="*60)
    
    # ç»Ÿè®¡æ‰€æœ‰å…³é”®è¯
    all_keywords = {}
    total_papers = 0
    
    for category, analyses in all_analyses.items():
        print(f"\nğŸ“‚ {category}:")
        total_papers += len(analyses)
        
        for analysis in analyses:
            print(f"  ğŸ“„ {analysis['paper_name'][:50]}...")
            if analysis['abstract']:
                print(f"    æ‘˜è¦: {analysis['abstract'][:100]}...")
            print(f"    å…³é”®è¯: {', '.join(analysis['innovation_points'][:8])}")
            
            # ç»Ÿè®¡å…³é”®è¯
            for keyword_count in analysis['innovation_points']:
                keyword = keyword_count.split('(')[0]
                if keyword in all_keywords:
                    all_keywords[keyword] += 1
                else:
                    all_keywords[keyword] = 1
    
    print(f"\nğŸ“Š æ€»è®¡åˆ†æäº† {total_papers} ç¯‡è®ºæ–‡")
    print("\nğŸ”¥ çƒ­é—¨æŠ€æœ¯å…³é”®è¯:")
    sorted_keywords = sorted(all_keywords.items(), key=lambda x: x[1], reverse=True)
    for keyword, count in sorted_keywords[:15]:
        print(f"  {keyword}: {count} ç¯‡è®ºæ–‡")
    
    # åŸºäºåˆ†æç»“æœæå‡ºåˆ›æ–°æ–¹å‘
    print("\nğŸ’¡ åŸºäºè®ºæ–‡åˆ†æçš„åˆ›æ–°æ–¹å‘å»ºè®®:")
    
    if 'dynamic' in [k for k, v in sorted_keywords[:10]]:
        print("\n1. ğŸ¯ åŠ¨æ€æƒé‡æ··åˆæ£€ç´¢ - é«˜ä¼˜å…ˆçº§")
        print("   - ç°æœ‰ç ”ç©¶: å·²æœ‰åŠ¨æ€è°ƒæ•´çš„åˆæ­¥æ¢ç´¢")
        print("   - åˆ›æ–°ç©ºé—´: æ›´æ™ºèƒ½çš„æƒé‡å­¦ä¹ æœºåˆ¶")
        print("   - æŠ€æœ¯è·¯å¾„: å¼ºåŒ–å­¦ä¹  + æ³¨æ„åŠ›æœºåˆ¶")
    
    if 'fusion' in [k for k, v in sorted_keywords[:10]]:
        print("\n2. ğŸ”„ å¤šæ¨¡æ€èåˆä¼˜åŒ– - é«˜ä¼˜å…ˆçº§") 
        print("   - ç°æœ‰ç ”ç©¶: åŸºç¡€çš„èåˆæ–¹æ³•")
        print("   - åˆ›æ–°ç©ºé—´: æŸ¥è¯¢æ„ŸçŸ¥çš„åŠ¨æ€èåˆ")
        print("   - æŠ€æœ¯è·¯å¾„: Cross-attention + å±‚æ¬¡åŒ–èåˆ")
    
    if 'reranking' in [k for k, v in sorted_keywords[:10]]:
        print("\n3. ğŸ“ˆ æ™ºèƒ½é‡æ’åºç³»ç»Ÿ - ä¸­ç­‰ä¼˜å…ˆçº§")
        print("   - ç°æœ‰ç ”ç©¶: ä¼ ç»Ÿé‡æ’åºæ–¹æ³•")
        print("   - åˆ›æ–°ç©ºé—´: å¤šå› å­å­¦ä¹ æ’åº")
        print("   - æŠ€æœ¯è·¯å¾„: ç¥ç»æ’åº + ç”¨æˆ·åé¦ˆ")

if __name__ == "__main__":
    main()
