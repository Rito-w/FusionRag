## **Safeguarding Multimodal Knowledge Copyright in the** **RAG-as-a-Service Environment**

**Tianyu Chen** **[1]** **, Jian Lou** **[2]** **,Wenjie Wang** **[1]** _[†]_

1 Shanghaitech University 2 Sun Yat-sen University
```
   {chenty12024,wangwj1}@shanghaitech.edu.cn jian.lou@hoiying.net
```

**Abstract**

As Retrieval-Augmented Generation (RAG) evolves into service-oriented platforms
(Rag-as-a-Service) with shared knowledge bases, protecting the copyright of contributed data becomes essential. Existing watermarking methods in RAG focus
solely on textual knowledge, leaving image knowledge unprotected. In this work,
we propose _AQUA_, the first watermark framework for image knowledge protection in Multimodal RAG systems. _AQUA_ embeds semantic signals into synthetic
images using two complementary methods: acronym-based triggers and spatial
relationship cues. These techniques ensure watermark signals survive indirect
watermark propagation from image retriever to textual generator, being efficient,
effective and imperceptible. Experiments across diverse models and datasets show
that _AQUA_ enables robust, stealthy, and reliable copyright tracing, filling a key gap
in multimodal RAG protection.

**1** **Introduction**


Large Language Models (LLMs) have
demonstrated strong capabilities across
a wide range of tasks, but they often
suffer from hallucinations and outdated
knowledge learned in the static parameters. To mitigate these limitations,
Retrieval-Augmented Generation (RAG)

[Lewis et al., 2020, Guu et al., 2020, Asai
et al., 2023a] has emerged as a promising paradigm that augments LLMs with
up-to-date external knowledge retrieved
at inference time. RAG has further
evolved into a service-oriented model Figure 1: Overview of the RAG-as-a-Service (RaaS) work
flow. Data providers contribute proprietary knowledge to a

known as RAG-as-a-Service (RaaS) plat
shared knowledge base used by RAG service providers to

forms, where platforms like LlamaIndex

[Liu, 2022] facilitate the construction of serve end users. Data providers can issue watermark probe

queries to RAG services. If the watermark is detected in

shared knowledge bases contributed by

an unauthorized provider, it indicates unauthorized use.

multiple knowledge providers (Figure 1).
Importantly, these systems adopt a “usable but not visible” policy: RAG service provider can leverage
the contributed knowledge without directly accessing the raw data.

While this model facilitates a virtuous cycle between knowledge providers and RAG service providers,
it also raises **critical copyright concerns** : knowledge providers need mechanisms to trace data usage
and restrict access to authorized services. Since unauthorized RAG providers typically utilize the
entire shared knowledge base, knowledge provider can embed watermarks at the knowledge base

1 Implementation of _AQUA_ is public available at `[https://github.com/tychenn/AQUA](https://github.com/tychenn/AQUA)`

_†_ Corresponding author

Preprint. Under review.

Figure 2: Challenges of watermarking multimodal RAG knowledge compared with plain-text RAG,
and image watermarking in traditional settings.

level. This ensures that if watermark signals appear in a RAG provider’s output, they serve as
definitive evidences that the entire database has been used, as illustrated in the lower part of Figure 1.

Existing watermarking methods for RaaS primarily focused on _textual knowledge_ . For example,
WARD [Jovanovi´c et al., 2025] embedding watermarks into text segments by LLM-based red-green
list strategies; RAG [©] [Guo et al., 2025] leverages watermarked Chain-of-Thought (CoT) to protect
the copyright of textual knowledge. However, these methods are modality-specific, limited to text
modality and cannot be directly applied to non-textual knowledge due to the distinct characteristics of
other modalities. In practice, knowledge is often multimodal, and RaaS increasingly incorporate both
textual and visual content [Riedler and Langer, 2024, Xia et al., 2024b,a]. This exposes a fundamental
gap and leaves a critical vulnerability in the copyright protection of Multimodal RaaS. To address
this gap, we focus on a representative subclass: text-to-text (T2T) Multimodal RAG, where generator
integrates retrieved image knowledge and textual query to generate textual responses [Yasunaga et al.,
2022, Chen et al., 2022a, Lin and Byrne, 2022, Sun et al., 2024, Zhu et al., 2024].

Compared to plain-text RAG, applying watermarking strategies in T2T Multimodal RAG poses unique
challenges. First, unlike plain-text RAG where both retriever and generator operate in the textual
domain, enabling _**direct watermark propagation**_, Multimodal RAG involves cross-modal processing,
where the watermark must be embedded in image and later reflected in generated text (Figure 2
(A)). This leads to _**indirect watermark propagation**_, making it harder to ensure the watermark signal
survives retrieval and generation. Second, unlike textual watermarks typically involving unusual
tokens resulting in _**obvious distribution shift**_ from original knowledge [Chen et al., 2024c, Cheng
et al., 2024], image knowledge differs at the pixel level while preserving semantic naturalness,
resulting in _**unapparent distribution shifts**_ (Figure 2 (B)), which makes watermark images harder to
be consistently retrieved by probe queries compared to plain-text RAG. Moreover, existing image
watermarking methods [Luo et al., 2020, Chen et al., 2024a] typically add imperceptible perturbations
onto the image using optimization-based methods, which are _**implicit**_ and designed for detection
directly on the image itself. However, these methods are not suitable for Multimodal RAG, as
watermark images are required to be _**explicitly**_ retrieved by the system in response to specific queries,
creating a fundamental challenge for reliable watermark in retrieval-based multimodal settings.

To address above challenges in image knowledge copyright protection, we propose _AQUA_, a novel
watermarking framework tailored for T2T Multimodal RAG. Specifically, _AQUA_ watermarking
framework includes two complementary watermarking methods: _AQUA_ _acronym_ and _AQUA_ _spatial_ .
_AQUA_ _acronym_ addresses indirect watermark propagation by embedding uncommon acronyms and
their full names into synthetic images. In the verification phase, these acronyms are decoded
through the Optical Character Recognition (OCR) abilities of generators (Vision-Language Models
(VLMs) [Achiam et al., 2023, Team et al., 2023, Huang et al., 2023] ) to generate detectable textual
response: the full name of the acronyms. Despite cross-modal transformation, the textual nature
of the signal embedded in the image increases its chance of surviving end-to-end processing. For
models with limited OCR ability, _AQUA_ _spatial_ is designed to create synthetic images with special
object configurations (e.g. unusual positional relationships), and leverage generators’ understanding
of spatial semantics to answer position-related probe queries. These positional relationships can
bridge the gap between image semantics and textual outputs, allowing indirect watermark propagation
from retriever to generator. Both methods introduces semantic distinctiveness by embedding subtle
semantic cues into natural-looking images, allowing explicit retrieval while maintaining a high
retrieval rate. Together, these two methods provide a flexible, robust solution to the unique challenges
of watermarking in Multimodal RAG systems, supporting both black-box and white-box deployments.

Despite simplicity, our novel insights of using synthetic images with special acronyms texts and
special positional relationships as watermark carriers are particularly effective and efficient in

2

bridging the gap between image-based watermarking and textually detectable outputs, enabling robust
copyright tracing in Multimodal RAG. We evaluate _AQUA_ across diverse Multimodal RAG and
datasets spanning different domains. The experimental results demonstrate that _AQUA_ (1) enables
the watermark images to be retrieved and reflected in the generated textual output, (2) prevent
false positives from common image content, (3) remain imperceptible to users and undetectable by
unauthorized filtering mechanisms, and (4) is robust to common image transformations.

Our contribution can be summarized as follow:

 - We propose _AQUA_, the first watermarking framework tailored for image knowledge copyright protection in Multimodal RAG systems, addressing indirect watermark propagation, and successful
retrieval under unapparent distribution shifts and explicit watermark injection;

 We design two complementary watermarking strategies, _AQUA_ _acronym_, _AQUA_ _spatial_ to support
more realistic black-box scenarios;

 - We perform comprehensive experiments on two well-known datasets, utilizing four prevalent
pretrained VLMs (LLaVA-NeXT, InternVL3, Qwen-VL-Chat and Qwen2.5-VL-Instruct) to
assess the effectiveness, harmlessness, stealthiness and robustness of _AQUA_ .

 - _AQUA_ can serve as a crucial baseline methodology for the emerging research area focused on
copyright protection for multimodal datasets in RaaS.

**2** **Related Works**

**2.1** **Multimodal Retrieval-Augmented Generation**

**Plain-text Retrieval-Augmented Generation (RAG).** [Lewis et al., 2020, Singh et al., 2021, Wang
et al., 2023, Asai et al., 2023b, Xu et al., 2024, Zhao et al., 2024, Tan et al., 2025] proposed several
RAG frameworks to address the hallucination in LLMs. They mainly focus on textual external
knowledge, and these methods are inadequate to process and integrate non-textual modalities inherent
in real-world information.

**Multimodal Retrieval-Augmented Generation (Multimodal RAG).** [Yu et al., 2024, Mei et al.,
2025, Papageorgiou et al., 2025] extends the RAG framework to bridge this gap, explicitly designed to
incorporate diverse data modalities into both the retrieval and generation stages. A common strategy
for enabling cross-modal retrieval is to employ powerful multimodal encoders (e.g. CLIP [Radford
et al., 2021] ), to map different modalities (e.g., text and images) into a shared semantic embedding
space. This unification allows standard vector search algorithms like cosine similarity to retrieve
relevant items across modalities based on semantic relatedness.

**2.2** **RAG Watermarking**
Several watermarking approaches have been proposed to protect the copyright of textual knowledge
in RAG. WARD [Jovanovi´c et al., 2025] uses the LLM red-green list watermarking technology to
watermark all the texts in the RAG knowledge base [Kirchenbauer et al., 2023, Gloaguen et al.,
2024] . RAG-WM [Lv et al., 2025] presents a black-box RAG watermarking approach that leverages
interactions among multiple LLMs to generate high-quality watermarks. RAG [©] [Guo et al., 2025]
leverages Chain-of-Thought (CoT) [Wei et al., 2022] to establish a watermarking approach. DMIRAG [Liu et al., 2025] performs dataset membership inference by injecting a small number of
synthetic, watermarked "canary" documents into the Intellectual Property (IP) dataset. However,
existing methods on watermarking knowledge base in RAG system have exclusively focused on
purely textual data. To the best of our knowledge, no prior work has addressed the protection of
knowledge copyright in Multimodal RAG systems, particularly those integrating image and text
modalities, via watermarking techniques.

**3** **Preliminary**

In this section, we will first outline the workflow of the T2T Multimodal RAG system and define the
notations in Section 3.1. Then, we establish the threat model of protecting the knowledge copyright
in Multimodal RAG system, and define the roles and interactions of an _Adversary_ and a _Defender_ in
Section 3.2.

**3.1** **Multimodal RAG System Workflow**
The T2T Multimodal RAG system contains three components: a retriever _E_, a generator _G_, and an
external image knowledge base _D_ . The retriever consists of a text encoder _E_ _text_ and a image encoder
_E_ _img_ . Images _I_ _i_ in the external knowledge base _D_ = _{I_ 1 _, . . ., I_ _n_ _}_ are pre-processed to a latent space
through the image encoder: _e_ _I_ _i_ = _E_ _img_ ( _I_ _i_ ) _∈_ R _[d]_ .

3

**Knowledge Retrieval.** The retriever accepts the user’s text query _T_ as input, and process it into
the same latent space as image: _e_ _T_ : _e_ _T_ = _E_ _text_ ( _T_ ) _∈_ R _[d]_ . Then the retriever employs a similarity
function, Sim( _·, ·_ ) := R _[d]_ _×_ R _[d]_ _→_ Score (e.g., cosine similarity), to find the most relevant image
knowledge according to user’s text query: _s_ _i_ = Sim( _e_ _T_ _, e_ _I_ _i_ ) . Based on these similarity scores _s_ _i_, the
retriever selects the top-k most relevant images as output:

_D_ _retrieved_ = _R_ ( _D, T, k_ ) = _{I_ _s_ (1) _, I_ _s_ (2) _, . . ., I_ _s_ ( _k_ ) _},_ where _S_ top-k = _{s_ (1) _, s_ (2) _, . . ., s_ ( _k_ ) _}_ (1)

**Augmented Generation.** The original text query _T_ and the retrieved set of images _D_ _retrieved_ are
combined and passed to the generator _G_ to produce the final answer: _A_ = _G_ ( _D_ _retrieved_ _, T_ )

**3.2** **Threat Model**

We consider the image knowledge copyright protection in Multimodal RAG service.

**Defender** represents the knowledge provider, aiming to detect and prevent unauthorized use of
their proprietary image knowledge by external Multimodal RAG services. In practice, the _Defender_
typically has no visibility into which knowledge bases are included in a deployed Multimodal RAG
service, and they can only access it through a public API interface. _Defender_ can only operate on their
own datasets to implement protection mechanisms such as injecting watermarks before contributing
their data to a RaaS.

**Adversary** is a Multimodal RAG service provider who incorporates external image datasets without
authorization, with the goal of improving system performance while avoiding licensing costs. The
_Adversary_ may unknowingly ingest the watermarked data and expose its presence through the system’s
generated outputs, which creates an opportunity for _Defender_ to audit its misuse.

**4** **Methodology**

_AQUA_ is a watermarking framework designed to protect the image knowledge copyrights in Multimodal RAG service, meeting four key requirements: effectiveness, harmlessness, stealthiness, and
robustness. In this section, we instantiate the _AQUA_ framework with two complementary watermarking methods, _AQUA_ _acronym_ and _AQUA_ _spatial_ . For each method, we will first introduce the principle
of designing watermarks and then clarify how this method can be verified in statistical strategies.

Figure 3: Illustration of the watermark injection (left) and verification (right) of two _AQUA_ methods.

**4.1** _**AQUA**_ _arconym_

**Watermark Injection.** _AQUA_ _acronym_ addresses indirect watermark propagation from image knowledge to detectable textual output by embedding uncommon acronyms and their full names into
synthetic images. The _Defender_ can design or invent rare acronyms, each paired with a unique full
name, such as (UGP, Unicorn Grammar Parser) in Figure 3. Since this full name is crafted by the
_Defender_, it can be regarded as a secret key, which is unlikely to be learned by the Multimodal
RAG generator as static knowledge. Despite cross-modal transformation, the textual nature of the
signal embedded in the image increases its chance of surviving end-to-end processing. The acronym
pair can also be generated in large quantities using LLM (e.g., Gemini-2.5-Pro), with the ability of
In-Context Learning (ICL) [Brown et al., 2020] and the prompt provided in Appendix A.1, and more
examples are relegated in Appendix A.2. Each pair is then embedded as a watermark image and
injected into the image knowledge base: _D_ = _D_ _original_ _∪_ _D_ _watermark_ . These images are designed
to be minimally invasive and do not affect the model’s utility for normal queries.

4

**Watermark Verification.** In verification phase, these acronyms are decoded through the OCR
ability of generator, to generate detectable textual responses: the full name of the acronyms. Each
watermark image has its own probe query _T_ _probe_ which can be used by the knowledge provider to
detect unauthorized use. The _T_ _probe_ consists of two parts: a trigger _T_ _trigger_, used by the retriever
to retrieve the watermark images, and an instruction _T_ _instruction_, which prompts the generator to
generate the watermark-included responses that can be detected. We can formulate this construction
as: _T_ _probe_ = _T_ _tigger_ _⊕_ _T_ _instruction_ . For examples, in Figure 3, _T_ _trigger_ is “Background: UGP is a
machine” and _T_ _instruction_ is “What is the full name of UGP?”. Following [Wu et al., 2024, Ha et al.,
2025] to verify the watermark signal, we define a strict exact match protocol Eval( _·, ·_ ) based on a
normalization function Norm( _·_ ) that lowercases and strips whitespace from both generated output
_O_ _RAG_ and the verification signature _S_ :

Eval( _O_ _RAG_ _, S_ ) = I[Norm( _S_ ) _⊆_ Norm( _O_ _RAG_ )] (2)
where I[ _·_ ] is the indicator function, returning 1 if the condition (substring presence) is true, and 0
otherwise. The predefined signature (e.g., "Unicorn Grammar Parser") serves as the ground truth.
Due to the inherent randomness of generation (e.g., temperature, top-k/top-p sampling) [Ackley et al.,
1985, Fan et al., 2018, Holtzman et al., 2019], the presence of a watermark signal is not guaranteed
even when the corresponding image is retrieved. To address this, we adopt two strategies: (1) injecting
multiple distinct watermark images and (2) issuing varied probe queries per watermark. We define
the _Verification Success Rate_ (VSR) as:


_N_ _ds_
� Eval _j_ ( _O_ _RAG_ _i_ _, S_ _i_ ) (3)

_i_ =1


1
VSR =
_N_ _wm_ _· N_ _ds_


_N_ _wm_
�

_j_ =1


where _N_ _wm_ is the number of watermark images and _N_ _ds_ is the number of distinct queries per image.
_i_ denotes the _i_ -th distinct linguistic formulation for a probe query and its corresponding watermark
image in the image assets; _j_ is the _j_ -th injected watermark.

**Hypothesis Testing.** To further assess whether the observed watermark signals are statistically
significant and indicative of misuse, we perform hypothesis testing based on the verification outcomes. Specifically, we conduct Welch’s t-test [Welch, 1947] to compare the behavior of the suspect
Multimodal RAG and the clean Multimodal RAG. Null Hypothesis ( _H_ 0 ) indicates there is no statistical evidence suggesting the suspect Multimodal RAG including the watermark image datasets:
_H_ 0 : _µ_ _suspect_ = _µ_ _clean_, where the VSR of the suspect Multimodal RAG is equal to the VSR of the
clean one. Using the sample means, variances, counts, and approximated degrees of freedom via the
Welch-Satterthwaite equation [Satterthwaite, 1941, 1946], we compute the t-statistic. The p-value is
compared against a significance level (e.g., _α_ = 0 _._ 05 ) to decide whether to reject _H_ 0 and conclude
potential unauthorized use.

**4.2** _**AQUA**_ _spatial_
**Watermark Injection.** For those models with limited OCR capabilities, we propose _AQUA_ _spatial_,
which is designed to create synthetic images with special object configurations (e.g., unusual positional relationships), and leverage generators’ understanding of spatial semantics to answer positionrelated probe queries. Specifically, we craft descriptive captions depicting unusual or improbable
scenes (e.g., “A red apple on the head of a reading dog.”) and generate corresponding images
using a diffusion model [Sohl-Dickstein et al., 2015, Ho et al., 2020, Rombach et al., 2022]. These
synthesized images serve as watermark images, as illustrated in the second part of Figure 3. Similar
to _AQUA_ _acronym_, these watermark images are injected into the dataset and can be scaled using
LLM-based in-context generation of diverse captions. More examples are relegated to Appendix
A.2. These positional relationships can bridge the gap between image semantics and textual outputs,
allowing indirect watermark propagation from retriever to generator.

**Watermark Verification and Hypothesis Testing.** The verification and the hypothesis testing are
similar to that of _AQUA_ _acronym_ method. Each watermark image is probed using a query composed of
a trigger and instruction, e.g., _T_ _trigger_ = “There is a dog reading a book.” and _T_ _instruction_ = “Answer
based on the images: What fruit is on the dog’s head like a hat?”. The expected signature is “Apple”.
As before, the system output is evaluated using the exact-match protocol Eval( _·, ·_ ), and Welch’s t-test
is applied to determine whether the suspect system statistically includes the watermarked dataset.

**4.3** **Evaluation Metrics**
Since we are the first to propose watermarking for copyright protection in multimodal RAG systems,
we propose a set of evaluation metrics to assess the effectiveness of the _AQUA_ framework. These

5

metrics evaluate both the retriever’s ability to retrieve watermarked images and the generator’s ability
to produce detectable watermark signals in the response.

**Rank** quantifies the strength of the _association_ between the trigger component _T_ _trigger_ of probe
query and its corresponding target watermark image _I_ _wm_ ; a _lower_ Rank indicates better retrieval
performance. For a given query, _D_ _retrieved_ = ( _I_ 1 _, I_ 2 _, . . ., I_ _k_ ) indicates the top- _k_ retrieved images
knowledge. The Rank is defined as the 1-based index _r_ of _I_ _wm_ within _D_ _retrieved_ . If _I_ _wm_ is not
present within the top _k_ retrieved images, a penalty value, set to twice the retrieval depth ( 2 _k_ ), is
assigned. Formally, the Rank is calculated as:

_r_ _,_ if _∃_ _r ∈{_ 1 _, . . ., k}_ such that _I_ _r_ = _I_ _wm_
Rank( _I_ _wm_ _, D_ _retrieved_ _, k_ ) = (4)
�2 _k_ _,_ otherwise

**Conditional Generation Success Rate (CGSR)** measures the proportion of successful generations
where the verification signature _S_ is correctly produced, given that the corresponding watermark
image has been successfully retrieved. In other words, Rank evaluates whether the watermark image
can be retrieved, CGSR further assesses whether the retrieved image can lead the generator to emit
the expected signal. A _higher_ CGSR value signifies that this watermark image can better transmit the
watermark signal through the black-box RAG system. Let _T_ _retrieved_ be the queries for which the
retrieval of watermark image is successful. The CGSR is then defined as the success rate over the
subset of successful retrievals:


CGSR =


� _t∈T_ _retrieved_ [Eval][(] _[O]_ _RAG_ [(] _[t]_ [)] _[, S]_ [(] _[t]_ [)] [)] (5)

_|T_ _retrieved_ _|_


**SimScore quantifies the output** quantifies the _semantic_ similarity between a watermark probe query
and a benign query with similar intent, as judged by an LLM (Gemini-2.5-Pro), with scores ranging
from 0 to 100%. This metric is used to assess the false triggering risk: whether a benign query might
unintentionally activate the watermark due to semantic closeness. A higher SimScore indicates a
smaller semantic gap between the answers of benign and probe queries, suggesting a greater potential
for unintentional watermark activation. The prompting details are provided in Appendix A.1.

**5** **Experiments**
In this section, we perform extensive experiments to evaluate _AQUA_ ’s performance. We cover
the experimental setup (Section 5.1), and two baselines (Section 5.2), followed by assessments of
effectiveness (Section 5.3), harmlessness (Section 5.4), stealthiness (Section 5.5), and robustness
(Section 5.6).

**5.1** **Experimental Setup**
**Datasets.** We utilize two widely used multimodal datasets: _MMQA_ [Talmor et al., 2021] and _WebQA_

[Chang et al., 2022]. Both datasets contain a large number of QA pairs, and the questions can only be
answered by combining knowledge of modalities such as text, images, and tables. Our intention is
to use _AQUA_ to protect the copy right of image modality. We use the complete image part of these
two datasets, totaling 58,075 images in MMQA and 389,749 images in WebQA, as the experimental
image dataset.

**Multimodal RAG Componets.** The Multimodal RAG system consists of two parts: Retriever and
Generator. For _Retriever_, we use the Contrastive Language–Image Pre-training (CLIP) [Radford et al.,
2021], specifically the ‘openai/clip-vit-large-patch14’ variant. _Cosine Similarity_ is used to compute
the similarity between text and image. Following the usual search strategies [Caffagni et al., 2024,
Mortaheb et al., 2025, Ha et al., 2025], we set clip-top-k=5, ensuring the retriever selects the five most
relevant images as knowledge. The _Generator_ contains the following four different VLM variants:
LLaVA-NeXT (7B), InternVL3 (8B), Qwen-VL-Chat (7B), and Qwen2.5-VL-Instruct (7B) [Liu
et al., 2024, Chen et al., 2024d, Bai et al., 2023, Team, 2025]. To control the diversity of the outputs,
we configure the decoding process for each VLM using standard sampling parameters, sampling
temperature ( T = 1 _._ 2 ), top-k sampling ( top_k = 5 ), nucleus sampling ( top_p = 0 _._ 9 ). These settings
are maintained consistently across experiments unless otherwise noted.

**Devices.** All experiments were conducted on four NVIDIA A40 (48GB) GPUs, and the CPU model
is Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz.

**5.2** **Baseline**

We propose two baselines to compare with our method: a _Naive_ method and an optimization-based
method _Opt._

6

_**Naive**_ baseline uses _usual_ images as watermark images. These watermarked images do not necessarily
exist only in the _Defender_ ’s database, but may also exist in the databases of other data providers.
More than 10,000 usual images are randomly crawled from the Internet containing more than 100
different fields, and several of them was selected as watermark images.

_**Opt.**_ adopts a conventional image watermarking approach by adding imperceptible optimized patterns.
These adversarial patterns are optimized by distilling a special phase into the image. Given a base
image _I_ _base_, a perturbation _δ_ is optimized such that the generated response of the generator _G_ includes
a pre-defined phase _P_, when queried with a textual prompt _T_ . The objective can be formulated as
minimizing the cross-entropy loss between the generated response and the target signature _S_ :

min _L_ ( _G_ ( _I_ _base_ + _δ, T_ ) _, P_ ) (6)
_δ_

We adopt Projected Gradient Descent (PGD) [Goldstein, 1964, Levitin and Polyak, 1966] to optimize
the perturbation iteratively, as it is a widely-adopted and effective adversarial perturbation generation
method:
_δ_ _t_ +1 = Π _∥·∥_ _p_ _≤ϵ_ ( _δ_ _t_ _−_ _α · ∇_ _δ_ _t_ _L_ ( _M_ ( _I_ _base_ + _δ_ _t_ _, q_ ) _, P_ )) (7)

where _α_ represents the step size (learning rate), and projection operator Π _∥·∥_ _p_ _≤ϵ_ ( _·_ ) ensures the
perturbation remains within an _L_ _p_ -norm ball of radius _ϵ_, preserving visual imperceptibility. The final
watermarked image is _I_ _wm_ = _I_ _base_ + _δ_ _[∗]_ .

**5.3** **Effectiveness of** _**AQUA**_ Table 1: Effectiveness of _AQUA_ . _Models_ indicate which model is

In this section, we evaluate the used as the generator. _AQUA_ _acronym_ and _AQUA_ _spatial_ represent
effectiveness of our _AQUA_ . The the two watermarking methods. _Naive_ and _Opt._ denotes the
Rank value and CGSR of each baseline methods.


In this section, we evaluate the
effectiveness of our _AQUA_ . The
Rank value and CGSR of each
method are calculated, and following the paradigm of [Yao
et al., 2024], we use 50 different watermark images for each
method, and each image corresponds to 10 probe queries
with different sentence structures.
The experiment is repeated 10
times to obtain the p-values in
the Table 1. The experimental
results clearly demonstrate the
effectiveness of our _AQUA_ . Calculation by Welch’s t-test, the
p-values are significantly lower
than the standard significance
level ( _α_ = 0.05). This allows us
to confidently reject the null hypothesis _H_ 0 : _µ_ _suspect_ = _µ_ _clean_,
providing strong statistical evidence that the _AQUA_ method enables reliable detection of the injected watermarks.

**p-value vs.** **Query times.**
While all methods can eventually produce statistically significant results (i.e., low p-values)
to reject the null hypothesis,
the number of queries required
to reach significance is a crucial factor in real-world applications—especially when each
query may incur cost or be subject to limitations. To evaluate
query efficiency, we measure how
efficiently each method achieves


Figure 4: TPR vs. FPR of
two methods and two base
lines.

7


**Models** **Methods** **MMQA** **WebQA**

Rank _↓_ CGSR _↑_ p-value _↓_ Rank _↓_ CGSR _↑_ p-value _↓_

_Naive_ 2 _._ 86 28 _._ 16% 0 _._ 32 4 _._ 56 13 _._ 28% 0 _._ 93


LLaVA

- NeXT

InternVL3

Qwen-VL
-Chat

Qwen2.5VL-Instruct


_Opt._ 1 _._ 45 31 _._ 03% 3 _._ 33 _e_ _[−]_ [4] 1 _._ 90 22 _._ 86% 3 _._ 94 _e_ _[−]_ [2]

_Naive_ 2 _._ 86 27 _._ 11% 0 _._ 41 4 _._ 56 17 _._ 12% 0 _._ 65

_Opt._ 1 _._ 45 19 _._ 34% 5 _._ 39 _e_ _[−]_ [3] 1 _._ 90 19 _._ 45% 3 _._ 87 _e_ _[−]_ [3]

_Naive_ 2 _._ 86 15 _._ 79% 0 _._ 59 4 _._ 56 5 _._ 71% 0 _._ 91

_Opt._ 1 _._ 45 21 _._ 29% 9 _._ 05 _e_ _[−]_ [3] 1 _._ 90 18 _._ 91% 1 _._ 21 _e_ _[−]_ [3]

_Naive_ 2 _._ 86 38 _._ 15% 0 _._ 25 4 _._ 56 15 _._ 87% 0 _._ 86

_Opt._ 1 _._ 45 19 _._ 96% 7 _._ 35 _e_ _[−]_ [3] 1 _._ 90 18 _._ 51% 6 _._ 77 _e_ _[−]_ [3]


Figure 5: The relationship between p-value and query times.

statistical significance. Figure 5 shows that _AQUA_ _acronym_ and _AQUA_ _spatial_ can obtain a p-value less
than the significance level within _**30**_ queries, while the _Opt._ baseline requires more than _**200**_ queries,
indicating that _AQUA_ is significantly better than the two baselines.

**FPR vs. TPR.** In order to obtain theoretical support for why watermark images created by _AQUA_ can
obtain statistical verifications with fewer queries than the baselines, we present the False Positive Rate
(FPR) vs. True Positive Rate (TPR) plot (Figure 4). FPRs are gained from measuring the generator’s
(LLaVA-NeXT) output without watermark images in the database, and TPRs are obtained with 1,
2, 3, 5, and 10 watermark images for each probe query. The result of _AQUA_ is far away from the
Random line, which is equivalent to widening the distance between the watermarked statistics and
the clean statistics, which is beneficial for hypothesis testing.

**Real-world Deployment.** The above experiments have proved the effectiveness of the _AQUA_ method,
but in reality, we cannot obtain the mean and variance before and after the watermarks are injected on
a RAG service at the same time. We can only get one mean and variance ( _µ_ ˆ _suspect_, ˆ _s_ [2] _suspect_ [) from]
the suspected RAG service, so we propose a verification strategy with a predefined VSR’s _reference_
_distribution_ . We first characterize the reference distribution of a clean Multimodal RAG using mean
and variance ( _µ_ _clean_, _σ_ _clean_ [2] [), and the same with a watermarked one (] _[µ]_ _[wm]_ [,] _[ σ]_ _wm_ [2] [). Subsequently, we]
can perform Welch’s t-test between ( _µ_ ˆ _suspect_, ˆ _s_ [2] _suspect_ [) and two respective reference distributions.]

The null hypotheses ( _H_ 0 ) for two hypothesis tests are: _**Suspect vs. Clean:**_ _H_ 0 [(1)] : ˆ _µ_ _suspect_ _< µ_ _clean_
and _**Suspect vs. Watermarked:**_ _H_ 0 [(2)] : ˆ _µ_ _suspect_ _> µ_ _wm_ . To avoid a false accusation, the significance
level _α_ can be set to a very low value (e.g. 3 _e_ _[−]_ [5] in [Jovanovi´c et al., 2025]). Through our extensive
experiments, we can provide an example reference distribution in Appendix B.

**5.4** **Harmlessness of** _**AQUA**_
**Normal Query.** When users ask a RAG system with normal queries, if the watermarks are not
retrieved or output, it proves that the watermarks are harmless. We use normal queries (e.g. “What
animals race in the Kentucky Derby?”) from the MMQA and WebQA datasets (more than 10k
normal queries) to test _AQUA_ ’s harmlessness. Experiments show that when using a normal query
within just one watermark image in the knowledge base, the retrieval rate of both _AQUA_ _acronym_ ’s
and _AQUA_ _spatial_ ’s watermark images are **0** %, and the CGSR is also **0** % on four generators. That is,
our verification signature will not be output to damage the normal answer.

**Relevant Query.** Rel- Table 2: Examples of relevant queries and corresponding results.
evant queries are used **Type** **Probe Query** **Relevant Query** **Rank** **SimScore** _↑_
to test whether the injected watermark image Acronym-replace What is the subtitle of UGP? ATM? What is the subtitle of 10 _._ 00 100%
will affect the normal

Acronym- What is the subtitle of What is UGP? 1 _._ 07 70 _._ 18%

output results when the no_instruction UGP?
question asked by the Spatial- What fruit is the monkey What is the monkey hold- 2 _._ 93 75 _._ 87%
user is extremely simi- imprecise holding like a phone? ing?
lar to the probe query.
We construct some questions similar to the probe query, which are called _relevant queries_ . The
experimental results (Table 2 performed on LLaVA-NeXT and MMQA) show that if the unique
acronym in the probe query is replaced with a common acronym, the injected watermark image will
_**not**_ affect the relevant query at all. Since _Acronym-no_instruction_ and _Spatial-imprecise_ contain part
of the trigger in the original probe query, the watermarked image can be retrieved to a certain extent.
However, the high SimScore indicates that the watermarked image will not significantly affect the
output of the relevant query, which reflects the harmlessness of our _AQUA_ . More results are shown in
Appendix C.


Table 2: Examples of relevant queries and corresponding results.


**Type** **Probe Query** **Relevant Query** **Rank** **SimScore** _↑_


Acronym- What is the subtitle of What is the subtitle of 10 _._ 00 100%
replace UGP? ATM?

Acronym- What is the subtitle of What is UGP? 1 _._ 07 70 _._ 18%
no_instruction UGP?

Spatial- What fruit is the monkey What is the monkey hold- 2 _._ 93 75 _._ 87%
imprecise holding like a phone? ing?


**5.5** **Stealthiness of** _**AQUA**_

**PCA Visualization.** Many previous works

[Chen et al., 2018, Tran et al., 2018, Boler
et al., 2022] propose how to filter harmful
images from poisoned databases. Similarly,

[Chen et al., 2024b, Gummadi et al., 2024,
Yao et al., 2025] mention how to filter harmful text queries in the RAG system. So, if the
embeddings of watermark images and probe
queries are similar to the embeddings of origi

(a) Image embeddings (b) Text embeddings

Figure 6: PCA comparison on embeddings. Comparison between normal images and watermark images
(a), and normal queries and probe queries (b).

8

nal images and normal queries in the dataset, detecting and filtering the watermark images is difficult
for _Adversary_ . We employ Principal Component Analysis (PCA) [Pearson, 1901, Hotelling, 1933]
to visualize the embeddings of watermark images and their probe queries compared to those of
original images and normal queries from the MMQA dataset (Fig. 6). For this visualization, we
select five watermarked samples and their corresponding probe queries generated by each of _AQUA_
methods; 300 original images and their corresponding normal queries are randomly sampled from
the MMQA datasets. These results indicate that _AQUA_ maintains strong stealthiness while preserving
high retrieval performance.

**Retrieval Ratio vs. Watermark Number.** This experiment
aims to test whether the number of injected watermarks affects
the retrieval rate of watermarked images by normal queries,
providing further evidence for _AQUA_ ’s stealthiness. Figure
7 shows that as the number of injected watermark images increases, the retrieval ratio of watermark images constructed by
our _AQUA_ _acronym_ and _AQUA_ _spatial_ are both less than **0.1%**
and does not increase significantly, further demonstrating the
stealthiness of our approach.

**5.6** **Robustness of** _**AQUA**_
Table 3: The Rank and p-value values of the watermark image
after the following transformations.

**Transformations** _AQUA_ _acronym_ _AQUA_ _spatial_
Rank _↓_ p-value _↓_ Rank _↓_ p-value _↓_


Rescale 1 _._ 03 7 _._ 57 _e_ _[−]_ [274] 1 _._ 36 3 _._ 62 _e_ _[−]_ [62]

Rotate 1 _._ 07 3 _._ 76 _e_ _[−]_ [142] 1 _._ 61 3 _._ 41 _e_ _[−]_ [45]

Gaussian 1 _._ 07 4 _._ 10 _e_ _[−]_ [264] 1 _._ 46 1 _._ 64 _e_ _[−]_ [56]

Rescale + Rotate + Gaussian 1 _._ 06 1 _._ 70 _e_ _[−]_ [256] 1 _._ 79 1 _._ 43 _e_ _[−]_ [39]


Figure 7: The retrieval rate of watermarks under normal query as the
number of injected watermark images increases.


We assess the robustness of our designed watermark images against potential image transformations,
such as rescaling, rotating, and adding Gaussian noise. As in the setting of experiment 5.3, the p-value
is obtained through 512 query trials. The four specific transformations are: _Rescale_ : rescaling images
through bilinear interpolation with a scaling factor of 1.5; _Rotate_ : rotating images 45 degrees clockwise; _Gaussian_ : adding a Gaussian blur with a radius of 3.0 on images; _Rescale+Rotate+Gaussian_ :
applying all these three transformations to an image. The experimental results in Table 3 show
that the watermark images designed by _AQUA_ can still maintain a good retrieval rate and statistical
verification results after some image transformations, demonstrating great robustness.

**6** **Discussion**

**Limitation.** Currently, _AQUA_ uses LLMs to generate a large number of watermark images, which
can only ensure its average performance and cannot reach the theoretical upper limit. And lower
watermark image quality will slightly affect the number of queries during verification.

**Future Works.** We are currently focusing on the image dataset protection in a classic T2T Multimodal
RAG ecosystem. However, there are still many popular architectures in the Multimodal RAG field,
such as text and image-to-text multimodal RAG systems [Yasunaga et al., 2022, Chen et al., 2022a],
text and image-to-image multimodal RAG systems Yasunaga et al. [2022], Chen et al. [2022b],
Shalev-Arkushin et al. [2025], and so on. Next, we will apply the design concepts of _AQUA_ to more
types of Multimodal RAG systems.

**7** **Conclusion**

This research focuses on safeguarding image dataset copyright in T2T Multimodal RAG systems.
We proposed _AQUA_, a watermarking framework that meets four design requirements: effectiveness,
harmlessness, stealthiness, and robustness. Two complementary watermarking strategies in _AQUA_
can protect the copyright of image datasets through statistical verification methods using only a few
watermark images. Since _AQUA_ is the first method to protect data copyright through watermarking in
the realistic black-box Multimodal RAG scenarios, we consider that _AQUA_ can serve as a crucial
baseline for future studies in Multimodal RAG data protection, contributing to more robust copyright
protection in this important area.

9

**References**

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.
_arXiv preprint arXiv:2303.08774_, 2023.

David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for boltzmann
machines. _Cognitive science_, 9(1):147–169, 1985.

Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to
retrieve, generate, and critique through self-reflection. In _The Twelfth International Conference on_
_Learning Representations_, 2023a.

Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to
retrieve, generate, and critique through self-reflection, 2023b. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2310.11511)`
`[2310.11511](https://arxiv.org/abs/2310.11511)` .

Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,
and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization,
text reading, and beyond, 2023. URL `[https://arxiv.org/abs/2308.12966](https://arxiv.org/abs/2308.12966)` .

William Boler, Ashley Dale, and Lauren Christopher. Trusted data anomaly detection (tada) in ground
truth image data. In _2022 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)_, pages
1–6. IEEE, 2022.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. _Advances in neural information processing systems_, 33:1877–1901, 2020.

Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi,
and Rita Cucchiara. Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms.
In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages
1818–1826, 2024.

Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk.
Webqa: Multihop and multimodal qa. In _Proceedings of the IEEE/CVF conference on computer_
_vision and pattern recognition_, pages 16495–16504, 2022.

Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung
Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by
activation clustering. _arXiv preprint arXiv:1811.03728_, 2018.

Feiyu Chen, Wei Lin, Ziquan Liu, and Antoni B Chan. A secure image watermarking framework
with statistical guarantees via adversarial attacks on secret key networks. In _European Conference_
_on Computer Vision_, pages 428–445. Springer, 2024a.

Jianfa Chen, Emily Shen, Trupti Bavalatti, Xiaowen Lin, Yongkai Wang, Shuming Hu, Harihar
Subramanyam, Ksheeraj Sai Vepuri, Ming Jiang, Ji Qi, et al. Class-rag: Real-time content
moderation with retrieval augmented generation. _arXiv preprint arXiv:2410.14881_, 2024b.

Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W Cohen. Murag: Multimodal
retrieval-augmented generator for open question answering over images and text. _arXiv preprint_
_arXiv:2210.02928_, 2022a.

Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-augmented
text-to-image generator. _arXiv preprint arXiv:2209.14491_, 2022b.

Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li. Agentpoison: Red-teaming
llm agents via poisoning memory or knowledge bases, 2024c. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2407.12784)`
`[2407.12784](https://arxiv.org/abs/2407.12784)` .

Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong
Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning
for generic visual-linguistic tasks. In _Proceedings of the IEEE/CVF conference on computer vision_
_and pattern recognition_, pages 24185–24198, 2024d.

10

Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Ping Yi, Zhuosheng Zhang, and
Gongshen Liu. Trojanrag: Retrieval-augmented generation can be backdoor driver in large language
models. _arXiv preprint arXiv:2405.13401_, 2024.

Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. _arXiv preprint_
_arXiv:1805.04833_, 2018.

Thibaud Gloaguen, Nikola Jovanovi´c, Robin Staab, and Martin Vechev. Black-box detection of
language model watermarks. _arXiv preprint arXiv:2405.20777_, 2024.

Alan A Goldstein. Convex programming in hilbert space. 1964.

Venkata Gummadi, Pamula Udayaraju, Venkata Rahul Sarabu, Chaitanya Ravulu, Dhanunjay Reddy
Seelam, and S Venkataramana. Enhancing communication and data transmission security in rag
using large language models. In _2024 4th International Conference on Sustainable Expert Systems_
_(ICSES)_, pages 612–617. IEEE, 2024.

Junfeng Guo, Yiming Li, Ruibo Chen, Yihan Wu, Chenxi Liu, Yanshuo Chen, and Heng Huang.
Towards copyright protection for knowledge bases of retrieval-augmented language models via
ownership verification with reasoning. _arXiv preprint arXiv:2502.10440_, 2025.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented
language model pre-training. In _International conference on machine learning_, pages 3929–3938.
PMLR, 2020.

Hyeonjeong Ha, Qiusi Zhan, Jeonghwan Kim, Dimitrios Bralios, Saikrishna Sanniboina, Nanyun
Peng, Kai-Wei Chang, Daniel Kang, and Heng Ji. Mm-poisonrag: Disrupting multimodal rag with
local and global poisoning attacks. _arXiv preprint arXiv:2502.17832_, 2025.

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in_
_neural information processing systems_, 33:6840–6851, 2020.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. _arXiv preprint arXiv:1904.09751_, 2019.

Harold Hotelling. Analysis of a complex of statistical variables into principal components. _Journal_
_of educational psychology_, 24(6):417, 1933.

Jiaxing Huang, Jingyi Zhang, Kai Jiang, Han Qiu, and Shijian Lu. Visual instruction tuning towards
general-purpose multimodal model: A survey. _arXiv preprint arXiv:2312.16602_, 2023.

Nikola Jovanovi´c, Robin Staab, Maximilian Baader, and Martin Vechev. Ward: Provable rag dataset
inference via llm watermarks. _ICLR_, 2025.

John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A
watermark for large language models. In _International Conference on Machine Learning_, pages
17061–17084. PMLR, 2023.

Evgeny S Levitin and Boris T Polyak. Constrained minimization methods. _USSR Computational_
_mathematics and mathematical physics_, 6(5):1–50, 1966.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in neural information processing systems_, 33:
9459–9474, 2020.

Weizhe Lin and Bill Byrne. Retrieval augmented visual question answering with outside knowledge.
_arXiv preprint arXiv:2210.03809_, 2022.

Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavanext: Improved reasoning, ocr, and world knowledge, January 2024. URL `[https://llava-vl.](https://llava-vl.github.io/blog/2024-01-30-llava-next/)`
`[github.io/blog/2024-01-30-llava-next/](https://llava-vl.github.io/blog/2024-01-30-llava-next/)` .

Jerry Liu. LlamaIndex, 11 2022. URL `[https://github.com/jerryjliu/llama_index](https://github.com/jerryjliu/llama_index)` .

11

Yepeng Liu, Xuandong Zhao, Dawn Song, and Yuheng Bu. Dataset protection via watermarked
canaries in retrieval-augmented llms. _arXiv preprint arXiv:2502.10673_, 2025.

Xiyang Luo, Ruohan Zhan, Huiwen Chang, Feng Yang, and Peyman Milanfar. Distortion agnostic
deep watermarking. In _Proceedings of the IEEE/CVF conference on computer vision and pattern_
_recognition_, pages 13548–13557, 2020.

Peizhuo Lv, Mengjie Sun, Hao Wang, Xiaofeng Wang, Shengzhi Zhang, Yuxuan Chen, Kai Chen,
and Limin Sun. Rag-wm: An efficient black-box watermarking approach for retrieval-augmented
generation of large language models. _arXiv preprint arXiv:2501.05249_, 2025.

Lang Mei, Siyu Mo, Zhihan Yang, and Chong Chen. A survey of multimodal retrieval-augmented
generation. _arXiv preprint arXiv:2504.08748_, 2025.

Matin Mortaheb, Mohammad A Amir Khojastepour, Srimat T Chakradhar, and Sennur Ulukus.
Re-ranking the context for multimodal retrieval augmented generation. _arXiv preprint_
_arXiv:2501.04695_, 2025.

George Papageorgiou, Vangelis Sarlis, Manolis Maragoudakis, and Christos Tjortjis. A multimodal
framework embedding retrieval-augmented generation with mllms for eurobarometer data. _AI_, 6
(3):50, 2025.

Karl Pearson. Liii. on lines and planes of closest fit to systems of points in space. _The London,_
_Edinburgh, and Dublin philosophical magazine and journal of science_, 2(11):559–572, 1901.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In _International conference on machine learning_, pages
8748–8763. PmLR, 2021.

Monica Riedler and Stefan Langer. Beyond text: Optimizing rag with multimodal inputs for industrial
applications. _arXiv preprint arXiv:2410.21943_, 2024.

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF confer-_
_ence on computer vision and pattern recognition_, pages 10684–10695, 2022.

Franklin E Satterthwaite. Synthesis of variance. _Psychometrika_, 6(5):309–316, 1941.

Franklin E Satterthwaite. An approximate distribution of estimates of variance components. _Biomet-_
_rics bulletin_, 2(6):110–114, 1946.

Rotem Shalev-Arkushin, Rinon Gal, Amit H Bermano, and Ohad Fried. Imagerag: Dynamic image
retrieval for reference-guided image generation. _arXiv preprint arXiv:2502.09411_, 2025.

Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. End-to-end training of
multi-document reader and retriever for open-domain question answering. _Advances in Neural_
_Information Processing Systems_, 34:25968–25981, 2021.

Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In _International conference on machine learning_,
pages 2256–2265. pmlr, 2015.

Liwen Sun, James Zhao, Megan Han, and Chenyan Xiong. Fact-aware multimodal retrieval augmentation for accurate medical radiology report generation. _arXiv preprint arXiv:2407.15268_,
2024.

Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco,
Hannaneh Hajishirzi, and Jonathan Berant. Multimodalqa: Complex question answering over text,
tables and images. _arXiv preprint arXiv:2104.06039_, 2021.

Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, and Ji-Rong Wen. Htmlrag:
Html is better than plain text for modeling retrieved knowledge in rag systems. In _Proceedings of_
_the ACM on Web Conference 2025_, pages 1733–1746, 2025.

12

Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,
Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly
capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.

Qwen Team. Qwen2.5-vl, January 2025. URL `[https://qwenlm.github.io/blog/qwen2.](https://qwenlm.github.io/blog/qwen2.5-vl/)`
`[5-vl/](https://qwenlm.github.io/blog/qwen2.5-vl/)` .

Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. _Advances in_
_neural information processing systems_, 31, 2018.

Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua
Xiao, and Wei Wang. Knowledgpt: Enhancing large language models with retrieval and storage
access on knowledge bases. _arXiv preprint arXiv:2308.11761_, 2023.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in_
_neural information processing systems_, 35:24824–24837, 2022.

Bernard L Welch. The generalization of ‘student’s’problem when several different population
varlances are involved. _Biometrika_, 34(1-2):28–35, 1947.

Chen Henry Wu, Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, and Aditi Raghunathan. Adversarial attacks on multimodal agents. _arXiv e-prints_, pages arXiv–2406, 2024.

Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James
Zou, and Huaxiu Yao. Mmed-rag: Versatile multimodal rag system for medical vision language
models. _arXiv preprint arXiv:2410.13085_, 2024a.

Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, and Huaxiu Yao.
Rule: Reliable multimodal rag for factuality in medical vision language models. In _Proceedings of_
_the 2024 Conference on Empirical Methods in Natural Language Processing_, pages 1081–1093,
2024b.

Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang,
and Zheng Li. Retrieval-augmented generation with knowledge graphs for customer service
question answering. In _Proceedings of the 47th International ACM SIGIR Conference on Research_
_and Development in Information Retrieval_, pages 2905–2909, 2024.

Hongwei Yao, Jian Lou, Zhan Qin, and Kui Ren. Promptcare: Prompt copyright protection by
watermark injection and verification. In _2024 IEEE Symposium on Security and Privacy (SP)_,
pages 845–861. IEEE, 2024.

Hongwei Yao, Haoran Shi, Yidou Chen, Yixin Jiang, Cong Wang, Zhan Qin, Kui Ren, and Chun
Chen. Controlnet: A firewall for rag-based llm system. _arXiv preprint arXiv:2504.09593_, 2025.

Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike
Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling.
_arXiv preprint arXiv:2211.12561_, 2022.

Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang,
Xu Han, Zhiyuan Liu, et al. Visrag: Vision-based retrieval-augmented generation on multi-modality
documents. _arXiv preprint arXiv:2410.10594_, 2024.

Siyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He, Luna K Qiu, and Lili Qiu. Retrieval augmented
generation (rag) and beyond: A comprehensive survey on how to make your llms use external data
more wisely. _arXiv preprint arXiv:2409.14924_, 2024.

Yinghao Zhu, Changyu Ren, Zixiang Wang, Xiaochen Zheng, Shiyun Xie, Junlan Feng, Xi Zhu,
Zhoujun Li, Liantao Ma, and Chengwei Pan. Emerge: Enhancing multimodal electronic health
records predictive modeling with retrieval-augmented generation. In _Proceedings of the 33rd ACM_
_International Conference on Information and Knowledge Management_, pages 3549–3559, 2024.

13

**A** **Detailed Information of** _**AQUA**_

**A.1** **Prompt**

**Acronym Prompt.** Here is an example prompt (Figure 8) where we use LLM (Gemini-2.5-Pro) to
build a large number of acronyms and their full names. {num_of_watermark} indicates the number
of watermarks you want.
```
Here is an example:
(UGP, Unicorn Grammar Parser)
Please create {num_of_watermark} pairs of uncommon acronyms and their
full names based on this example.
```

Figure 8: Prompt used to construct a large number of acronym pairs.

**SimScore Prompt.** This is the prompt (Figure 9) where we ask LLM (Gemini-2.5-Pro) to determine
whether two strings are semantically similar. {Clean_Answer} and {Watermark_Answer} indicate
the response of Multimodal RAG’s generator with the un-watermarked dataset and the watermarked
one, respectively.
```
Determine the semantic similarity between the following two strings and
give your score on a scale of 0-100:
String 1: {Clean_Answer}
String 2: {Watermark_Answer}
Just answer with numbers.
```

Figure 9: Prompt used to obtain the SimScore between two strings.

**A.2** **Watermark Examples**

In this section, we will provide several classic watermark examples. The examples of _AQUA_ _acronym_
method is in the Table 4 and _AQUA_ _spatial_ in Table 6. The sentence structure and wording of the
probe query can be freely selected, but semantic consistency must be ensured.

14

Table 4: Examples of watermark images designed by the _AQUA_ _acronym_ method.
**Watermark Image** **Probe Query** **Verification Signature**


1. What’s the meaning of UGP?
2. Background: UGP is a machine.
What is the full name of UGP?

3. Provide the full name of UGP.

1. Output the meaning of BLT.
2. What does the acronym BLT
stand for based on the images?
3. Please output the subtitle of
acronym BLT.

1. Please state the full phrase that
BLT abbreviates.
2. Could you provide the expansion for the acronym BLT?
3. What words do the letters in
BLT represent?

1. What is the meaning of the
acronym TPB?
2. Could you spell out what TPB
represents?
3. Give me the full form of TPB.


Unicorn Grammar Parser

Bouncing Llama Technologies

Quantum Walrus Xylophone

Temporal Platypus Bagpipe


1. What does XCO signify?
2. Could you decode XCO? Xenon Cubist Ottoman
3. Give me the full form of TPB.

Table 5: Examples of watermark images designed by the _AQUA_ _spatial_ method.
**Watermark Image** **Probe Query** **Verification Signature**


1. What fruit is the monkey holding like a
phone?
2. The monkey seems to be making a call.
What object is it using?
3. Instead of a telephone, what yellow
item is the monkey pretending with?

1. Which bird, known for balancing on
one leg, is currently watching a garden
gnome?
2. Identify the avian creature standing
on a single leg and observing a garden

gnome.
3. A garden gnome is being watched by a
bird resting on one leg. What type of bird
is this?

1. Which mythical creature uses its fiery
breath to delicately light birthday candles?
2. This fire-breathing beast is gently lighting birthday candles. What is it?
3. Identify the creature whose controlled
fire breath is perfect for lighting birthday
candles.

15


Banana

Flamingo

Dragon

Table 6: Examples of watermark images designed by the _Opt._ baseline.

**Watermark Image** **Probe Query** **Verification Signature**


1. Three basketball. Does any image contain word ‘AQUA’?
2. Three basketball. Can word AQUA be
found within these images?
3. Three basketball. Is ‘AQUA’ visible
anywhere in the image?

1. Find the full moon image with black
background. What word is in the image?
2. Find the full moon image with black
background. What’s the word shown in
this picture?
3. Find the full moon image with black
background. Can you tell me the word
displayed in the image?

1. Find an image within seven books.
Does any image contain word AQUA?
2. Find an image within seven books. Is
AQUA word in any picture?
3. Find an image within seven books.
Does any image contain word AQUA?

1. Find a picture of a panda with its mouth
wide open. What word is shown in the
picture?
2. Find an image within seven books.
What’s written in the picture?
3. Find a picture of a panda with its mouth
wide open. What text appears on the image?

16


Yes

AQUA

Yes

AQUA

Table 7: This table shows the Rank and SimScore of relevant queries. Supplemented the experiments
of three other models.

**MMQA** **WebQA**
**Models** **Type**

Rank SimScore _↑_ Rank SimScore _↑_

Acronym-replace 5 _._ 00 100% 5 _._ 00 100%
LLaVA-NeXT

Acronym-no_instruction 1 _._ 07 70 _._ 18% 1 _._ 24 67 _._ 53%

Spatial-imprecise 2 _._ 93 75 _._ 87% 3 _._ 17 71 _._ 27%

Acronym-replace 5 _._ 00 100% 5 _._ 00 100%
InternVL3

Acronym-no_instruction 1 _._ 07 71 _._ 28% 1 _._ 2 68 _._ 29%

Spatial-imprecise 2 _._ 93 68 _._ 92% 3 _._ 17 63 _._ 31%

Acronym-replace 5 _._ 00 100% 5 _._ 00 100%
Qwen-VL-Chat

Acronym-no_instruction 1 _._ 07 56 _._ 42% 1 _._ 24 51 _._ 58%

Spatial-imprecise 2 _._ 93 63 _._ 60% 3 _._ 17 56 _._ 20%

Acronym-replace 5 _._ 00 100% 5 _._ 00 100%
Qwen2.5-VL-Instruct

Acronym-no_instruction 1 _._ 07 82 _._ 85% 1 _._ 24 78 _._ 51%

Spatial-imprecise 2 _._ 93 78 _._ 23% 3 _._ 17 69 _._ 82%

**B** **Reference Distribution**

_AQUA_ _acronym_ and _AQUA_ _spatial_ need to use different means and variances to characterize their
respective reference distributions. Since this reference distribution is related to the specific watermark
image constructed and its performance, here we can give an example reference distribution through
our extensive experiments:

    - _AQUA_ _acronym_ : ( _µ_ _clean_, _σ_ _clean_ [2] [) = (0.005, 0.02); (] _[µ]_ _[wm]_ [,] _[ σ]_ _wm_ [2] [) = (0.6, 0.2)]

    - _AQUA_ _spatial_ : ( _µ_ _clean_, _σ_ _clean_ [2] [) = (0.2, 0.2); (] _[µ]_ _[wm]_ [,] _[ σ]_ _wm_ [2] [) = (0.55, 0.25)]

**C** **More Results of Harmlessness of** _**AQUA**_

This section is a supplement to the experiment section on harmlessness of _AQUA_ (Section 5.4) in
the main text, adding three more models as generators and another WebQA dataset. The results are
shown in Table 7.

**D** **More Results of Robustness of** _**AQUA**_

In this section, we will test the robustness of our _AQUA_ on more models in Table 8. Because
robustness has little to do with the dataset, here we show the experimental results on the MMQA
dataset as in the main text.

17

Table 8: This table shows the Rank and p-value of watermark image after some common transformations.

**Models** **Transformations** _AQUA_ _acronym_ _AQUA_ _spatial_
Rank _↓_ p-value _↓_ Rank _↓_ p-value _↓_

Rescale 1 _._ 03 7 _._ 57 _e_ _[−]_ [274] 1 _._ 36 3 _._ 62 _e_ _[−]_ [62]


LLaVA-NeXT

InternVL3

Qwen-VL-Chat

Qwen2.5-VL-Instruct


Rotate 1 _._ 07 3 _._ 76 _e_ _[−]_ [142] 1 _._ 61 3 _._ 41 _e_ _[−]_ [45]

Gaussian 1 _._ 07 4 _._ 10 _e_ _[−]_ [264] 1 _._ 46 1 _._ 64 _e_ _[−]_ [56]

Rescale + Rotate + Gaussian 1 _._ 06 1 _._ 70 _e_ _[−]_ [256] 1 _._ 79 1 _._ 43 _e_ _[−]_ [39]

Rescale 1 _._ 03 4 _._ 85 _e_ _[−]_ [231] 1 _._ 36 4 _._ 85 _e_ _[−]_ [46]

Rotate 1 _._ 07 9 _._ 17 _e_ _[−]_ [112] 1 _._ 61 1 _._ 59 _e_ _[−]_ [31]

Gaussian 1 _._ 07 7 _._ 69 _e_ _[−]_ [229] 1 _._ 46 3 _._ 76 _e_ _[−]_ [39]

Rescale + Rotate + Gaussian 1 _._ 06 6 _._ 04 _e_ _[−]_ [216] 1 _._ 79 8 _._ 24 _e_ _[−]_ [27]

Rescale 1 _._ 03 4 _._ 58 _e_ _[−]_ [154] 1 _._ 36 4 _._ 15 _e_ _[−]_ [45]

Rotate 1 _._ 07 8 _._ 08 _e_ _[−]_ [106] 1 _._ 61 6 _._ 82 _e_ _[−]_ [32]

Gaussian 1 _._ 07 5 _._ 26 _e_ _[−]_ [142] 1 _._ 46 9 _._ 14 _e_ _[−]_ [38]

Rescale + Rotate + Gaussian 1 _._ 06 5 _._ 12 _e_ _[−]_ [125] 1 _._ 79 6 _._ 78 _e_ _[−]_ [25]

Rescale 1 _._ 03 3 _._ 19 _e_ _[−]_ [269] 1 _._ 36 7 _._ 68 _e_ _[−]_ [68]

Rotate 1 _._ 07 7 _._ 54 _e_ _[−]_ [238] 1 _._ 61 4 _._ 15 _e_ _[−]_ [51]

Gaussian 1 _._ 07 1 _._ 23 _e_ _[−]_ [251] 1 _._ 46 8 _._ 48 _e_ _[−]_ [62]

Rescale + Rotate + Gaussian 1 _._ 06 5 _._ 47 _e_ _[−]_ [268] 1 _._ 79 6 _._ 49 _e_ _[−]_ [46]

18

