# Deep Learning Model Acceleration and Optimization Strategies for Real-Time Recommendation Systems


Junli Shao*

College of Literature Science, and the Arts

University of Michigan, Ann Arbor, USA
[*Corresponding author: dereks513a@gmail.com](mailto:dereks513a@gmail.com)

Jing Dong

Fu Foundation School of Engineering and Applied Science

Columbia University,New York, NY, USA,

jd3768@columbia.edu

Dingzhou Wang

Pratt School of Engineer,
Duke University, Durham, NC, USA

wangdingzhou.research@gmail.com

_**Abstract**_ — _**With the rapid growth of Internet services,**_
_**recommendation systems play a central role in delivering**_
_**personalized content. Faced with massive user requests and**_
_**complex model architectures, the key challenge for real-time**_
_**recommendation systems is how to reduce inference latency and**_
_**increase system throughput without sacrificing recommendation**_
_**quality. This paper addresses the high computational cost and**_
_**resource bottlenecks of deep learning models in real-time settings**_
_**by proposing a combined set of modeling- and system-level**_
_**acceleration and optimization strategies. At the model level, we**_
_**dramatically reduce parameter counts and compute requirements**_
_**through lightweight network design, structured pruning, and**_
_**weight quantization. At the system level, we integrate multiple**_
_**heterogeneous**_ _**compute**_ _**platforms**_ _**and**_ _**high-performance**_
_**inference libraries, and we design elastic inference scheduling**_
_**and load-balancing mechanisms based on real-time load**_
_**characteristics. Experiments show that, while maintaining the**_
_**original recommendation accuracy, our methods cut latency to**_
_**less than 30% of the baseline and more than double system**_
_**throughput, offering a practical solution for deploying large-scale**_
_**online recommendation services.**_

—
_**Keywords**_ _**real-time recommendation systems; deep learning;**_
_**model acceleration; pruning; heterogeneous computing**_

I. I NTRODUCTION

Real-time recommendation systems must deliver fast,
accurate results under heavy load, but deep learning models
are often too costly for such environments. Combining LLMs
with GNNs improves accuracy but adds latency and
complexity. We propose an integrated framework using
model-level optimizations (lightweight nets, sparse attention,
pruning, quantization, distillation) and system-level strategies
(heterogeneous computing, elastic scheduling, load
balancing). This approach cuts latency to under 40% and
doubles throughput while keeping accuracy loss below 1%,
enabling scalable real-time recommendations.


Kowei Shih

Independent Researcher,Shenzhen,China

[skw19@tsinghua.org.cn](mailto:skw19@tsinghua.org.cn)

Dannier Li

School of Computing,
University of Nebraska - Lincoln

Lincoln, NE, USA,
[dannierli@outlook.com](mailto:dannierli@outlook.com)

Chengrui Zhou

Fu Foundation School of Engineering and Applied Science

Columbia University, New York, NY, USA,

[zhou.chengrui@columbia.edu](mailto:zhou.chengrui@columbia.edu)

II. C HALLENGES OF D EEP L EARNING M ODELS IN R EAL -T IME

R ECOMMENDATION S YSTEMS

_A._ _Dual Constraints of Latency and Throughput_

Fig. 1. Illustration of the end-to-end reasoning process of a deep learning

driven real-time recommender system

In Offline recommendation allows higher latency, but
real-time systems must complete the full pipeline—from user
action to result delivery—within tens to hundreds of
milliseconds (Figure 1). Once a user clicks (①), features are
processed and merged with history, passed through a DNN
(②), and ranked results returned (③). Any delay harms user
engagement.

To meet these demands, models like Shen et al.’s MultiScale CNN-LSTM-Attention [4] improve accuracy and
speed by combining CNNs, LSTMs, and attention for better
spatial-temporal modeling.

Real-time systems also face high throughput—up to tens
of thousands of QPS during spikes. Inefficient scheduling or
system bottlenecks worsen delays. Scalable models and
adaptive pipelines help maintain performance under load.

Latency and throughput often trade off: faster responses

need optimized hardware; batching improves throughput but
adds delay. Thus, every stage—especially from inference to
ranking—must be co-optimized using lightweight models,
pruning, and quantization to ensure low latency and high
throughput [[5-10] ] .

_B._ _Model Complexity and Resource Consumption_

In the real-time recommender system shown in Figure 1,
deep model inference (step 2) is the link with the most
intensive computational cost in the end-to-end process. The
time complexity and parameter scale of the model directly
determine the delay of single inference and the overall
resource consumption. Assuming that the input feature
dimension is d, the vector dimension after Embedding is dₑ,
the width of the hidden layer is h, and the depth of the
network is L, the parameter quantity of the fully connected
network can be approximately expressed as formula 1.

P ≈ dₑ · h + (L – 1) · h² + h · 1 ≈ O(L h²) (1)

In the case dₑ≪h, P≈L h²; And the floating-point
operations (FLOPs) of a batch inference with candidate set
size m can be expressed as formula 2.

FLOPs ≈m(dₑ · h + (L – 1) h² + h) ≈O(m L h²) (2)

Inference latency τ can be approximated as τ ≈ α·mLh² +
β, where α depends on hardware throughput and β on fixed
overheads. Memory use includes parameters M_params = P
× b_p and activations M_act = m × h × bₐ, with b_p and bₐ as
byte sizes per value. Quantizing from 32-bit to 8-bit cuts
memory and bandwidth by ~4×. However, increasing m, L,
or h greatly raises computation and memory—doubling h
quadruples compute, doubling m roughly doubles latency—
making simple scaling infeasible for real-time systems.

To balance accuracy with delay and resource use, model
complexity must be optimized. Key methods include pruning
(reducing h by removing redundant neurons), quantization
(lowering bit-widths), low-rank decomposition (splitting
large matrices), and hierarchical candidate screening
(limiting m early). Combining these model-level and systemlevel (e.g., heterogeneous acceleration) strategies keeps
complexity (O(mLh²), O(Lh²)) manageable, ensuring
efficient, stable large-scale recommender service.

III. M ODEL -L EVEL A CCELERATION T ECHNIQUES

_A._ _Lightweight Network Architecture Design_

Fig. 2. Illustration of feature weighting of user behavior sequence based

on self-attention

Deep recommendation models use self-attention to


capture temporal and contextual dependencies. As in Figure
2, standard self-attention on a sequence of length L and
hidden size d has time complexity O(L²d) and space
complexity O(L²). When L or d increase, latency and memory
grow quadratically, making real-time inference impractical.

[17-20] .

To address this bottleneck, we propose the following
lightweighting strategies: Replace the original fullyconnected projections with a grouped linear transformation:
split the d-dimensional feature into k groups and apply k
independent projections in parallel. This reduces the per

This encourages the student to learn the crucial attention
patterns using fewer layers and roughly 30% fewer
parameters, reducing inference cost by approximately
40%.We compute full attention over a local window of size
w≪Lw \ll L to model short-term dependencies, and apply
random or fixed sparse sampling for the remaining positions.
This reduces the nonzero attention ratio from O(L [!] ) to
O(Lw) or O(LlogL), cutting overall compute to roughly as
shown in Formula 4.

O(Lwd + LlogLd) (4)

Quantize both weights and activations from 32-bit to 8bit, reducing memory bandwidth and storage by about
4×4\times. Using dynamic-range-aware quantization along
the Value branch in Figure 2, we enable zero-copy integer
inference on hardware accelerators. This further cuts latency
by ~45% and nearly doubles throughput under concurrency.
Together, these methods—grouped/depthwise projections,
low-rank head factorization, distillation, hybrid sparsity, and
quantization—compress both compute and memory
overhead of the self-attention module in Figure 2 without
degrading recommendation quality, laying a solid foundation
for subsequent heterogeneous acceleration and scheduling [[21-]

24] .

_B._ _Model Pruning and Weight Quantization_

To further shrink model size and reduce latency in strict
real-time scenarios, we design a closed-loop pruning–
quantization workflow driven by dynamic thresholds, as
illustrated in Figure 3. The process first applies controllable
binary masks to iteratively prune weights, then performs
dynamic-range quantization on the resulting sparse network,


" [!]


layer compute from O(d) [!] to O ~~(~~


layer compute from O(d) to O ~~(~~ # [)][. Further substituting a ]

depthwise-separable mapping (depthwise convolution
followed by pointwise projection) lowers the cost O(d) [!] to


O(


O( #$" [, k) ] significantly reducing multiply–accumulate

operations.On the premise of ensuring the diversity of the
head, we do low-rank decomposition on the projection matrix

" "

of each head, so that its rank is reduced from [ to ] [r ≪] [, ]


" [!]


"


% [ to ] [r ≪]


"


% [, ]


and the overall calculation amount is about O(HL [!] r), r ≪


"


% [.]
while preserving sufficient head diversity.During training,
the large “teacher” model’s attention maps

'()&'"*+&)

S & supervise the smaller “student” model by


'()&'"*+&)

S & supervise the smaller “student” model by

minimizing the KL divergence as shown in Formula 3.


'(&*/01*2)

& ∥S &


L -. = KL(S &


'()&'"*+&)
) (3)

achieving dual compression of compute and storage with
minimal accuracy loss.

Fig. 3. Schematic diagram of the stepwise threshold driven neuron

pruning with weight quantization process

Specifically, let the weight matrix of a layer be W ∈
R [+×4], and the elements be denoted w 56 . The process is
divided into two main stages.

Stage 1: Dynamic-Threshold Pruning
Threshold Computation: Given a target pruning ratio p,
sort {∣w 56 ∣} in ascending order and choose the initial
threshold θ [(7)] as shown in Formula 5.

8{(5,6):∣= "# ∣>? [(%)] }8

+4 = p (5)

Mask Generation: Define a binary mask as shown in
Formula 6.


(#) [∣≥θ] [(#)]

56 = P0, ∣w [1, ∣w] [56] 56 ∣< θ [(#)]


accumulate operations by ≈60%, reduces latency by ≈50%,
and boosts concurrent throughput by over 2.5×. The “prune
→ fine-tune → quantize → QAT” pipeline in Figure 3 fully
leverages structural sparsity and low-precision compute,
providing a practical path for deploying deep
recommendation models in high-concurrency, real-time
environments.

IV. S YSTEM -L EVEL O PTIMIZATION S TRATEGIES

_A._ _Heterogeneous Compute Platform and Acceleration_

_Library Integration_

To deploy a lightweight deep recommendation model at
scale, it is crucial to leverage heterogeneous compute
resources and high-performance inference libraries as shown
in Figure 4. First, the distilled student model is exported
using ONNX, allowing it to be mapped to various hardware
backends like GPUs, CPUs, or accelerators (e.g., NPU, TPU,
FPGA). For GPUs, NVIDIA TensorRT performs layer fusion
and optimizes with FP16 or INT8 for maximum throughput
and reduced latency. On CPUs, Intel OpenVINO and AMD
ROCm MIOpen apply operator fusion and vectorization for
core operations, supporting multi-core concurrent inference.

Fig. 4. Deep Recommendation Model Training and Deployment

Architecture Based on Weighted Knowledge Distillation

For mobile and edge deployment, models run on
TensorFlow Lite or SNPE, targeting NPU/DSP for efficiency.
In the cloud, models use asynchronous microservices with
Kubernetes/Kubeflow, supporting dynamic replica scaling.
Mixed-precision training and auto-tuning ensure low latency
and high throughput. Containerized inference components
enable grey releases and rapid rollback. CI/CD pipelines
automate packaging, testing, and deployment, ensuring
seamless scaling and real-time performance during traffic

surges.

_B._ _Elastic Inference Scheduling and Load Balancing_

To handle traffic spikes in real-time recommendation
systems, we adopt elastic inference scheduling and load
balancing . The student model is deployed with a unified
interface (e.g., gRPC), and a hybrid rate limiter adjusts traffic
by user tier, priority, and system metrics.

Requests are routed to the least-loaded backend; highpriority ones bypass batching for low latency, while others
use asynchronous batching for efficiency.

A warm pool of pre-initialized instances reduces cold
starts. Kubernetes autoscaling and geo-aware edge routing
further optimize resource use. An end-to-end monitoring
system ensures SLO compliance through real-time metrics
and alerts [[25-30]] .


M
56



[(6) ]
0, ∣w 56 ∣< θ [(#)]


and prune weights as shown in Formula 7.

W [(#)] = W [(#AB)] ⊙M [(#)], k = 1,2, …, K (7)

where ⊙ denotes element-wise multiplication. After each
pruning iteration, fine-tune the pruned network on the
original training set by minimizing the task loss L &/)# (W [(#)] ).
Empirically, after K=3 rounds, we reduce total parameters by
≈40% while keeping Top-N accuracy loss under 1%.

Stage 2: Dynamic-Range Quantization
Step Size Determination: For the nonzero weights in
W [(-)], let the quantization bit-width be bb. Compute the step
size as shown in Formula 8.


4/CD [(')] A45+D [(')]

s = ! [()*] AB (8)


Weight Mapping: Quantize each weight via as shown in
Formula 9.

wZ 56 = clip(round(w 56(-) /s) × s, minW (-), maxW (-) ) (9)

Quantization-Aware Training (QAT): Insert fakequantization nodes in the forward pass to simulate integer
behavior while preserving full-precision gradients in the
backward pass. After iterative QAT, the final sparsequantized model can perform zero-copy integer inference
without floating-point support. On real-world hardware, this
pruning–quantization loop achieves outstanding results:
compared to the original 32-bit model, the sparse-quantized
version uses only ≈15% of the storage, cuts multiply–

V. E XPERIMENT AND E VALUATION

_A._ _Experimental Setup and Benchmark Selection_

To To validate our optimization strategies in a realistic
scenario, we used the Alibaba Taobao User Behavior Dataset,
which includes 50M logs from 1M users and 200K products.
We truncated each user’s behavior sequence to the latest 100
entries and set the candidate set to 50, simulating typical ecommerce recommendations.

Experiments were conducted on NVIDIA V100 GPUs
and Intel Xeon CPUs using PyTorch 1.10, ONNX Runtime
1.9, TensorRT 8.0, and OpenVINO 2021.4 .

We evaluated five models:
(1) **Baseline** – original FP32 model with self-attention;
(2) **Quantized** – 8-bit weights [33];
(3) **Pruned** – 40% dynamic pruning;
(4) **Pruned + Quantized** – combined;
(5) **Distilled + RT (FP16)** – student model with TensorRT
acceleration .

Table I summarizes model size, parameter count, latency,
and throughput across platforms. [ [31-34]]

TABLE I. P ERFORMANCE C OMPARISON OF D IFFERENT M ODELS ON



Fig. 5. Inference Performance Comparison

Figure 5 shows pruning and quantization reduce GPU
latency by up to 43% and boost throughput over 70%. The
Distilled + RT model achieves the best GPU performance:
21.5 ms latency and 460 req/s throughput, 2.4× baseline.
Similar gains appear on CPU.

Fig. 6. Accuracy Comparison

As shown in Figure 6, applying quantization and pruning
separately results in a drop of approximately 1.0% and 2.0%
in Hit Rate, respectively. After combining pruning and
quantization, accuracy slightly decreases to 97.3% of the
Baseline. The distilled model with FP16 optimization not
only preserves the lightweight advantages but also maintains
Hit Rate and NDCG close to the original level (a decrease of
less than 0.6%), with MRR decreasing by less than 0.8%,
indicating that the distillation strategy preserves the model's
performance effectively.

Fig. 7. Resource Consumption Comparison

Figure 7 shows the Quantized model cuts model size to
25% and memory usage to 79% of the Baseline; Pruning
reduces peak memory by 62%. Combining both brings









|Col1|Col2|THE TA|AOBAO DA|ATASET|Col6|Col7|
|---|---|---|---|---|---|---|
|Method|Paramet<br>ers (M)|Mod<br>el<br>Size<br>(MB)|Latenc<br>y (ms)<br>[V100<br>]|Throughp<br>ut (req/s)<br>[V100]|Latenc<br>y (ms)<br>[CPU]|Throughp<br>ut (req/s)<br>[CPU]|
|Baseline|32.0|128.0|52.4|190|120.7|80|
|Quantized|32.0|32.0|44.1|225|102.3|95|
|Pruned|19.2|76.8|36.7|260|88.5|110|
|Pruned +<br>Quantized|19.2|19.2|29.8|325|74.2|140|
|Distilled<br>+ RT<br>(FP16)|6.4|12.8|21.5|460|54.8|180|


From Table 1, it is evident that applying quantization
alone (Quantized) reduces GPU latency by about 15.8% and
CPU latency by 15.3%. Pruning alone (Pruned) further
reduces GPU latency to 36.7 ms, which is a 30% reduction
from the Baseline, while throughput increases by about 37%.
Combining pruning and quantization (Pruned + Quantized)
reduces GPU latency to 29.8 ms, only 57% of the Baseline,
with a throughput increase of nearly 71%. The distilled
model with TensorRT FP16 acceleration (Distilled + RT)
achieves the best performance, with a GPU latency of 21.5
ms (41% of Baseline) and a throughput increase of over 2.4x.
On the CPU platform, similar trends are observed, with the
combined optimization significantly reducing latency and
improving concurrent handling capability.

_B._ _Performance Metrics and Accuracy Comparison_

In the e-commerce recommendation scenario, we
comprehensively compare the optimized and non-optimized
models across three dimensions: inference performance,
recommendation accuracy, and resource consumption.
Figure 5 shows the average latency and maximum throughput
on GPU (V100) and CPU platforms for each model. Table 53 presents the online recommendation quality metrics,
including Hit Rate@50, NDCG@50, and MRR, evaluated
using the Taobao User Behavior Dataset. Table 5-4
summarizes the parameter count, model size, and average
memory usage for each model, providing valuable insights
for resource budgeting in system design.

memory usage down to 46%. The distilled model with FP16
acceleration shrinks model size to 10% and memory usage
below 30%, freeing significant hardware resources. Overall,
pruning, quantization, distillation, and system-level FP16
acceleration reduce latency to 21.5 ms and boost throughput
beyond 460 req/s, with less than 1% accuracy loss and
resource use under 30%. This offers a robust solution for
large-scale real-time recommendation deployment. [ [35-36]]

VI. C ONCLUSION

We propose a joint model–system optimization
framework for real-time recommendation. Techniques
include model compression (pruning, quantization,
distillation) and system-level acceleration (elastic scheduling,
load balancing). Results show <1% accuracy loss, 60%
latency reduction, and 2× throughput improvement. The
approach enables scalable, efficient deployment, with future
work on cross-model adaptation and auto-tuning.

R EFERENCES

[1] Su, Pei-Chiang, et al. "A Mixed-Heuristic Quantum-Inspired

Simplified Swarm Optimization Algorithm for scheduling of real-time
tasks in the multiprocessor system." Applied Soft Computing 131
(2022): 109807.

[2] Sun S, Yuan J, Yang Y. Research on Effectiveness Evaluation and

Optimization of Baseball Teaching Method Based on Machine
Learning[J]. arXiv preprint arXiv:2411.15721, 2024.

[3] Duan, Chenming, et al. "Real-Time Prediction for Athletes'

Psychological States Using BERT-XGBoost: Enhancing HumanComputer Interaction." _arXiv preprint arXiv:2412.05816_ (2024).

[4] Shen J, Wu W, Xu Q. Accurate Prediction of Temperature Indicators

in Eastern China Using a Multi-Scale CNN-LSTM-Attention model[J].
arXiv preprint arXiv:2412.07997, 2024.

[5] Wang S, Jiang R, Wang Z, et al. Deep learning-based anomaly

detection and log analysis for computer networks[J]. arXiv preprint
arXiv:2407.05639, 2024.

[6] Zhang T, Zhang B, Zhao F, et al. COVID-19 localization and

recognition on chest radiographs based on Yolov5 and
EfficientNet[C]//2022 7th International Conference on Intelligent
Computing and Signal Processing (ICSP). IEEE, 2022: 1827-1830.

[7] Gao Z, Tian Y, Lin S C, et al. A ct image classification network

framework for lung tumors based on pre-trained mobilenetv2 model
and transfer learning, and its application and market analysis in the
medical field[J]. arXiv preprint arXiv:2501.04996, 2025.

[8] Liu J, Huang T, Xiong H, et al. Analysis of collective response reveals

that covid-19-related activities start from the end of 2019 in mainland
china[J]. medRxiv, 2020: 2020.10. 14.20202531.

[9] Zhao C, Li Y, Jian Y, et al. II-NVM: Enhancing Map Accuracy and

Consistency with Normal Vector-Assisted Mapping[J]. IEEE Robotics
and Automation Letters, 2025.

[10] Wang Y, Jia P, Shu Z, et al. Multidimensional precipitation index

prediction based on CNN-LSTM hybrid framework[J]. arXiv preprint
arXiv:2504.20442, 2025.

[11] Lv K. CCi-YOLOv8n: Enhanced Fire Detection with CARAFE and

Context-Guided Modules[J]. arXiv preprint arXiv:2411.11011, 2024.

[12] Zhang L, Liang R. Avocado Price Prediction Using a Hybrid Deep

Learning Model: TCN-MLP-Attention Architecture[J]. arXiv preprint
arXiv:2505.09907, 2025.

[13] Zheng Z, Wu S, Ding W. CTLformer: A Hybrid Denoising Model

Combining Convolutional Layers and Self-Attention for Enhanced CT
Image Reconstruction[J]. arXiv preprint arXiv:2505.12203, 2025.

[14] Freedman H, Young N, Schaefer D, et al. Construction and Analysis

of Collaborative Educational Networks based on Student Concept
Maps[J]. Proceedings of the ACM on Human-Computer Interaction,
2024, 8(CSCW1): 1-22.



[15] Hu J, Zeng H, Tian Z. Applications and Effect Evaluation of

Generative Adversarial Networks in Semi-Supervised Learning[J].
arXiv preprint arXiv:2505.19522, 2025.

[16] Song Z, Liu Z, Li H. Research on feature fusion and multimodal patent

text based on graph attention network[J]. arXiv preprint
arXiv:2505.20188, 2025.

[17] ~~Xiang~~, ~~A~~ ., ~~Zhang~~, ~~J~~ ., ~~Yang~~, ~~Q~~ ., ~~Wang~~, ~~L~~ ., ~~&~~ ~~Cheng~~, ~~Y~~ . ~~(2024)~~ .

~~Research~~ ~~on~~ ~~splicing~~ ~~image~~ ~~detection~~ ~~algorithms~~ ~~based~~ ~~on~~ ~~natural~~
~~image~~ ~~statistical~~ ~~characteristics~~ . ~~_arXiv_~~ ~~_preprint_~~ ~~_arXiv:2404_~~ _._ ~~_16296_~~ . ~~[~~ ~~x~~ a ]

[18] ~~Xiang~~, ~~A~~ ., ~~Qi~~, ~~Z~~ ., ~~Wang~~, ~~H~~ ., ~~Yang~~, ~~Q~~ ., ~~&~~ ~~Ma~~, ~~D~~ . ~~(2024~~, ~~August)~~ . ~~A~~

~~multimodal~~ ~~fusion~~ ~~network~~ ~~for~~ ~~student~~ ~~emotion~~ ~~recognition~~ ~~based~~ ~~on~~
~~transformer~~ ~~and~~ ~~tensor~~ ~~product~~ . ~~In~~ ~~_2024_~~ ~~_IEEE_~~ ~~_2nd_~~ ~~_International_~~
~~_Conference_~~ ~~_on_~~ ~~_Sensors_~~ _,_ ~~_Electronics_~~ ~~_and_~~ ~~_Computer_~~ ~~_Engineering_~~
~~_(ICSECE)_~~ ~~(pp~~ . ~~1~~   - ~~4)~~ . ~~IEEE~~ .

[19] Yang H, Fu L, Lu Q, et al. Research on the Design of a Short Video

Recommendation System Based on Multimodal Information and
Differential Privacy[J]. arXiv preprint arXiv:2504.08751, 2025.

[20] Lin X, Cheng Z, Yun L, et al. Enhanced Recommendation Combining

Collaborative Filtering and Large Language Models[J]. arXiv preprint
arXiv:2412.18713, 2024.

[21] Ji C, Luo H. Cloud-Based AI Systems: Leveraging Large Language

Models for Intelligent Fault Detection and Autonomous SelfHealing[J]. arXiv preprint arXiv:2505.11743, 2025.

[22] Yang Q, Ji C, Luo H, et al. Data Augmentation Through Random Style

Replacement[J]. arXiv preprint arXiv:2504.10563, 2025.

[23] Mao, Y., Tao, D., Zhang, S., Qi, T., & Li, K. (2025). Research and

Design on Intelligent Recognition of Unordered Targets for Robots
Based on Reinforcement Learning. arXiv preprint arXiv:2503.07340.

[24] Yi, Q., He, Y., Wang, J., Song, X., Qian, S., Zhang, M., ... & Shi, T.

(2025). SCORE: Story Coherence and Retrieval Enhancement for AI
Narratives. arXiv preprint arXiv:2503.23512.

[25] Qiu, S., Wang, Y., Ke, Z., Shen, Q., Li, Z., Zhang, R., & Ouyang, K.

(2025). A Generative Adversarial Network-Based Investor Sentiment
Indicator: Superior Predictability for the Stock Market. Mathematics,
13(9), 1476.

[26] Ouyang, K., Fu, S., & Ke, Z. (2024). Graph Neural Networks Are

Evolutionary Algorithms. arXiv preprint arXiv:2412.17629.

[27] Wang J, Zhang Z, He Y, et al. Enhancing Code LLMs with

Reinforcement Learning in Code Generation[J]. arXiv preprint
arXiv:2412.20367, 2024.

[28] Tan C, Zhang W, Qi Z, et al. Generating Multimodal Images with GAN:

Integrating Text, Image, and Style[J]. arXiv preprint
arXiv:2501.02167, 2025.

[29] Tan C, Li X, Wang X, et al. Real-time Video Target Tracking

Algorithm Utilizing Convolutional Neural Networks (CNN)[C]//2024
4th International Conference on Electronic Information Engineering
and Computer (EIECT). IEEE, 2024: 847-851.

[30] Zhang Z, Luo Y, Chen Y, et al. Automated Parking Trajectory

Generation Using Deep Reinforcement Learning[J]. arXiv preprint
arXiv:2504.21071, 2025.

[31] Zhao H, Ma Z, Liu L, et al. Optimized path planning for logistics

robots using ant colony algorithm under multiple constraints[J]. arXiv
preprint arXiv:2504.05339, 2025.

[32] Wang Z, Zhang Q, Cheng Z. Application of AI in Real-time Credit

Risk Detection[J]. 2025.

[33] Wu S, Huang X. Psychological Health Prediction Based on the Fusion

of Structured and Unstructured Data in EHR: a Case Study of LowIncome Populations[J]. 2025.

[34] Lu D, Wu S, Huang X. Research on Personalized Medical Intervention

Strategy Generation System based on Group Relative Policy
Optimization and Time-Series Data Fusion[J]. arXiv preprint
arXiv:2504.18631, 2025.

[35] Feng H, Dai Y, Gao Y. Personalized Risks and Regulatory Strategies

of Large Language Models in Digital Advertising[J]. arXiv preprint
arXiv:2505.04665, 2025.

[36] Zhao P, Wu J, Liu Z, et al. Contextual bandits for unbounded context

distributions[J]. arXiv preprint arXiv:2408.09655, 2024.

