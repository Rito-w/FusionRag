# Paper1.json è®ºæ–‡ç»¼åˆåˆ†ææŠ¥å‘Š

åˆ†ææ—¶é—´: 2025-06-21 00:23:34
åˆ†æè®ºæ–‡æ•°é‡: 10

## ğŸ“Š è®ºæ–‡æ¦‚è§ˆ

| æ’å | arXiv ID | æ ‡é¢˜ | è¯„åˆ† | ä¸»è¦åˆ›æ–°ç‚¹ |
|------|----------|------|------|------------|
| 1 | 2502.18139 | LevelRAG: Enhancing Retrieval-Augmented ... | 0.9949 | However, the tight coupling of query rewriting to ... |
| 2 | 2404.07220 | Blended RAG: Improving RAG (Retriever-Au... | 0.9944 | In this paper, we propose
the â€™Blended RAGâ€™ method... |
| 3 | 2503.23013 | DAT: Dynamic Alpha Tuning for Hybrid Ret... | 0.9940 | tw
Abstract
Hybrid retrieval techniques in Retriev... |
| 4 | 2406.00638 | COS-Mix: Cosine Similarity and Distance ... | 0.9909 | COS-Mix: Cosine Similarity and Distance Fusion for... |
| 5 | 2005.11401 | Retrieval-Augmented Generation for Knowl... | 0.9901 | New York University;
plewis@fb... |
| 6 | 2504.05324 | Hybrid Retrieval for Hallucination Mitig... | 0.9897 | Results show that the hybrid retriever has a bette... |
| 7 | 2412.16311 | HybGRAG: Hybrid Retrieval-Augmented Gene... | 0.9871 | In experiments on
theSTARKbenchmark, HYBGRAG achie... |
| 8 | 2408.04948 | HybridRAG: Integrating Knowledge Graphs ... | 0.9861 | New York, NY, USADhagash Mehta
dhagash... |
| 9 | 2308.04215 | Hybrid Retrieval-Augmented Generation fo... | 0.9829 | com
Abstract
Large language models (LLMs) enhanced... |
| 10 | 2409.09046 | HyPA-RAG: A Hybrid Parameter Adaptive Re... | 0.9805 | , 2023a,b;Meta,
2024) have advanced question answe... |

## ğŸ“‹ è¯¦ç»†åˆ†æ

### 1. LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic Planning over Rewriting Augmented Searchers

**arXiv ID**: 2502.18139 | **è¯„åˆ†**: 0.9949

#### ğŸ¯ è¦è§£å†³çš„é—®é¢˜
challenge, we introduce a high-level searcher that de-
 challenges associated with current query rewriting tech-
 However, the tight coupling of query rewriting to the dense


#### ğŸ’¡ ä¸»è¦åˆ›æ–°ç‚¹
1. However, the tight coupling of query rewriting to the dense
retriever limits its compatibility with hybrid retrieval, im-
peding further RAG performance improvements
2. To address
this challenge, we introduce a high-level searcher that de-
composes complex queries into atomic queries, independent
of any retriever-specific optimizations
3. Additionally, to har-
ness the strengths of sparse retrievers for precise keyword re-
trieval, we have developed a new sparse searcher that employs
Lucene syntax to enhance retrieval accuracy

#### ğŸ”¬ å®éªŒè®¾è®¡
experiments
conducted on five datasets, encompassing both single-hop
and multi-hop question answering tasks, demonstrate the su-
perior performance of LevelRAG compared to existing RAG
methods. Notably, LevelRAG outperforms the state-of-the-art
proprietary model, GPT4o, underscoring its effectivenes...

#### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- datasets, encompassing
- Wikipedia
- dataset. However
- datasets. Based
- corpus and

#### ğŸ“ æ‘˜è¦
Retrieval-Augmented Generation (RAG) is a crucial method
for mitigating hallucinations in Large Language Models
(LLMs) and integrating external knowledge into their re-
sponses. Existing RAG methods typically employ query
rewriting to clarify the user intent and manage multi-hop
logic, while using hybrid retrieval to expand search scope.
However, the tight coupling of query rewriting to the dense
retriever limits its compatibility with hybrid retrieval, im-
peding further RAG performance improve...

---

### 2. Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers

**arXiv ID**: 2404.07220 | **è¯„åˆ†**: 0.9944

#### ğŸ¯ è¦è§£å†³çš„é—®é¢˜
challenge often faced
 Limitations in the current RAG system
 However, RAG accuracy becomes


#### ğŸ’¡ ä¸»è¦åˆ›æ–°ç‚¹
1. In this paper, we propose
the â€™Blended RAGâ€™ method of leveraging semantic search tech-
niques, such as Dense Vector indexes and Sparse Encoder indexes,
blended with hybrid query strategies
2. Our study achieves better
retrieval results and sets new benchmarks for IR (Information
Retrieval) datasets like NQ and TREC-COVID datasets
3. Their ability to transform text into vector space models,
where semantic similarities can be quantitatively assessed,
marks a significant advancement over traditional keyword-
based approaches

#### ğŸ”¬ å®éªŒè®¾è®¡
results and sets new benchmarks for IR (Information
Retrieval) datasets like NQ and TREC-COVID datasets. We
further extend such a â€™Blended Retrieverâ€™ to the RAG system
to demonstrate far superior results on Generative Q&A datasets
like SQUAD, even surpassing fine-tuning performance.
Index Terms â€”RAG...

#### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- dataset-specific
- dataset are
- corpus occupied
- dataset,â€ ArXiv
- dataset; for

#### ğŸ“ æ‘˜è¦
Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q\&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the 'Blended RAG' method of...

---

### 3. DAT: Dynamic Alpha Tuning for Hybrid Retrieval in Retrieval-Augmented Generation

**arXiv ID**: 2503.23013 | **è¯„åˆ†**: 0.9940

#### ğŸ¯ è¦è§£å†³çš„é—®é¢˜
limitation include approaches that assign different Î±values
 challenge to hybrid retrieval optimization. However, existing approaches struggle


#### ğŸ’¡ ä¸»è¦åˆ›æ–°ç‚¹
1. tw
Abstract
Hybrid retrieval techniques in Retrieval-Augmented Generation (RAG)
systems enhance information retrieval by combining dense and sparse (e
2. To address this, we propose DAT (Dynamic Alpha Tuning), a novel
hybrid retrieval framework that dynamically balances dense retrieval and
BM25 for each query
3. Empirical results show that DAT consistently significantly outperforms
fixed-weighting hybrid retrieval methods across various evaluation metrics

#### ğŸ”¬ å®éªŒè®¾è®¡
results from both retrieval methods,
assigning an effectiveness score to each. It then calibrates the optimal
weighting factor through effectiveness score normalization, ensuring a
more adaptive and query-aware weighting between the two approaches.
Empirical results show that DAT consistently signif...

#### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- Dataset Evaluation
- SQuAD
- Dataset Articles
- datasets. Despite
- corpus Peval

#### ğŸ“ æ‘˜è¦
Hybrid retrieval techniques in Retrieval-Augmented Generation (RAG)
systems enhance information retrieval by combining dense and sparse (e.g.,
BM25-based) retrieval methods. However, existing approaches struggle
with adaptability, as fixed weighting schemes fail to adjust to different
queries. To address this, we propose DAT (Dynamic Alpha Tuning), a novel
hybrid retrieval framework that dynamically balances dense retrieval and
BM25 for each query. DAT leverages a large language model (LLM) to
e...

---

### 4. COS-Mix: Cosine Similarity and Distance Fusion for Improved Information Retrieval

**arXiv ID**: 2406.00638 | **è¯„åˆ†**: 0.9909

#### ğŸ¯ è¦è§£å†³çš„é—®é¢˜
limitation, we incorporate cosine distance
 issue in evaluating RAG
 However, it has been shown that this measure can


#### ğŸ’¡ ä¸»è¦åˆ›æ–°ç‚¹
1. COS-Mix: Cosine Similarity and Distance Fusion for
Improved Information Retrieval
Kush Juvekar1and Anupam Purwar2âˆ—
1https://gihub
2. The proposed method demonstrates enhanced retrieval performance and provides a
more comprehensive understanding of the semantic relationships between documents or items
3. In this study, we propose a hybrid
1arXiv:2406

#### ğŸ”¬ å®éªŒè®¾è®¡
results in certain scenarios. To address this limitation, we incorporate cosine distance
measures to provide a complementary perspective by quantifying the dissimilarity between vectors.
Our approach is experimented on proprietary data, unlike recent publications that have used open-
source datasets...

#### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- Wikipedia
- dataset from
- Corpus Scifact
- corpus T, we
- datasets like

#### ğŸ“ æ‘˜è¦
This study proposes a novel hybrid retrieval strategy for Retrieval-Augmented Generation (RAG)
that integrates cosine similarity and cosine distance measures to improve retrieval performance, par-
ticularly for sparse data. The traditional cosine similarity measure is widely used to capture the sim-
ilarity between vectors in high-dimensional spaces. However, it has been shown that this measure can
yield arbitrary results in certain scenarios. To address this limitation, we incorporate cosine di...

---

### 5. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

**arXiv ID**: 2005.11401 | **è¯„åˆ†**: 0.9901

#### ğŸ¯ è¦è§£å†³çš„é—®é¢˜
problems. issues because knowledge can be directly revised and expanded, and accessed knowledge can be
 However, their ability to access and precisely manipulate knowl-


#### ğŸ’¡ ä¸»è¦åˆ›æ–°ç‚¹
1. New York University;
plewis@fb
2. We introduce RAG models where the parametric
memory is a pre-trained seq2seq model and the non-parametric memory is a dense
vector index of Wikipedia, accessed with a pre-trained neural retriever
3. REALM [ 20] and ORQA [ 31], two recently introduced models that
combine masked language models [ 8] with a differentiable retriever, have shown promising results,arXiv:2005

#### ğŸ”¬ å®éªŒè®¾è®¡
results when ï¬ne-tuned on down-
stream NLP tasks. However, their ability to access and precisely manipulate knowl-
edge is still limited, and hence on knowledge-intensive tasks, their performance
lags behind task-speciï¬c architectures. Additionally, providing provenance for their
decisions and updat...

#### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- Wikipedia
- Trec
- MS MARCO
- SQuAD
- Dataset. arXiv

#### ğŸ“ æ‘˜è¦
Large pre-trained language models have been shown to store factual knowledge
in their parameters, and achieve state-of-the-art results when ï¬ne-tuned on down-
stream NLP tasks. However, their ability to access and precisely manipulate knowl-
edge is still limited, and hence on knowledge-intensive tasks, their performance
lags behind task-speciï¬c architectures. Additionally, providing provenance for their
decisions and updating their world knowledge remain open research problems. Pre-
trained mod...

---

### 6. Hybrid Retrieval for Hallucination Mitigation in Large Language Models: A Comparative Analysis

**arXiv ID**: 2504.05324 | **è¯„åˆ†**: 0.9897

#### ğŸ¯ è¦è§£å†³çš„é—®é¢˜
limitations of standalone LLMs [2, 3], in-
 issue of hallucina-
 However, for instances labeled as FAIL , the ground truth answer


#### ğŸ’¡ ä¸»è¦åˆ›æ–°ç‚¹
1. Results show that the hybrid retriever has a better relevance score outperforming
both sparse and dense retrievers
2. RAG is an approach that enhances LLMs by integrating retrieval mech-
anisms to improve response accuracy and reduce hallucinations [1]
3. To the best of our knowledge, this is the first study that evaluates the hybrid retrieval
performance in mitigating hallucinations

#### ğŸ”¬ å®éªŒè®¾è®¡
results of sparse and dense retrievers through a dynamically-weighted Reciprocal
Rank Fusion (RRF) score. Using the HaluBench dataset, a benchmark for halluci-
nations in Question Answering tasks, we assess retrieval performance with MAP
and NDCG metrics, focusing on the relevance of the top-3 retri...

#### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- dataset is
- benchmark consisting
- benchmark comparisons
- dataset, a benchmark
- dataset diversity

#### ğŸ“ æ‘˜è¦
Large Language Models (LLMs) excel in language comprehension and generation but are prone to hallucinations, producing factually incorrect or unsupported outputs. Retrieval Augmented Generation (RAG) systems address this issue by grounding LLM responses with external knowledge. This study evaluates the relationship between retriever effectiveness and hallucination reduction in LLMs using three retrieval approaches: sparse retrieval based on BM25 keyword search, dense retrieval using semantic sea...

---

### 7. HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases

**arXiv ID**: 2412.16311 | **è¯„åˆ†**: 0.9871

#### ğŸ¯ è¦è§£å†³çš„é—®é¢˜
problem concentrates on
 problems. However,


#### ğŸ’¡ ä¸»è¦åˆ›æ–°ç‚¹
1. In experiments on
theSTARKbenchmark, HYBGRAG achieves
significant performance gains, with an average
relative improvement in Hit@ 1of51%
2. First, they fo-
cus solely on retrieving either textual or relational
information
3. âœ” âœ” âœ”
21%
Higher10%
Higher
Figure 2: HYBGRAG wins inSTARK, outperforming
baselines by up to 21% in Hit@ 1

#### ğŸ”¬ å®éªŒè®¾è®¡
experiments on
theSTARKbenchmark, HYBGRAG achieves
significant performance gains, with an average
relative improvement in Hit@ 1of51%.
1 Introduction
Retrieval-Augmented Generation (RAG) (Lewis
et al., 2020; Guu et al., 2020) enables large lan-
guage models (LLMs) to access the informa-
tion from an...

#### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- datasets in
- benchmark. arXiv
- Benchmarking
- Benchmarks We
- benchmark STARK

#### ğŸ“ æ‘˜è¦
Given a semi-structured knowledge base
(SKB), where text documents are intercon-
nected by relations, how can we effectively re-
trieve relevant information to answer user ques-
tions? Retrieval-Augmented Generation (RAG)
retrieves documents to assist large language
models (LLMs) in question answering; while
Graph RAG (GRAG) uses structured knowl-
edge bases as its knowledge source. However,
many questions require both textual and rela-
tional information from SKB â€” referred to as
â€œhybridâ€ quest...

---

### 8. HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction

**arXiv ID**: 2408.04948 | **è¯„åˆ†**: 0.9861

#### ğŸ¯ è¦è§£å†³çš„é—®é¢˜
challenges to large language
 challenges such as domain specific terminology and complex
 However, traditional


#### ğŸ’¡ ä¸»è¦åˆ›æ–°ç‚¹
1. New York, NY, USADhagash Mehta
dhagash
2. The proposed
technique has applications beyond the financial domain
3. Financial KGs
integrate various financial data sources such as market data, fi-
nancial reports, and news articles, creating a comprehensive view
of financial entities and their relationships

#### ğŸ”¬ å®éªŒè®¾è®¡
experiments on a set of financial earning call transcripts
documents which come in the form of Q&A format, and hence
provide a natural set of pairs of ground-truth Q&As, we show that
HybridRAG which retrieves context from both vector database and
KG outperforms both traditional VectorRAG and GraphRA...

#### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- dataset of
- Datasets like
- dataset, we
- benchmark dataset
- benchmark for

#### ğŸ“ æ‘˜è¦
Extraction and interpretation of intricate information from unstruc-
tured text data arising in financial applications, such as earnings
call transcripts, present substantial challenges to large language
models (LLMs) even using the current best practices to use Re-
trieval Augmented Generation (RAG) (referred to as VectorRAG
techniques which utilize vector databases for information retrieval)
due to challenges such as domain specific terminology and complex
formats of the documents. We introduc...

---

### 9. Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance

**arXiv ID**: 2308.04215 | **è¯„åˆ†**: 0.9829

#### ğŸ¯ è¦è§£å†³çš„é—®é¢˜
challenge when applying them to real-time
 challenges of latency and model per-
 However, the


#### ğŸ’¡ ä¸»è¦åˆ›æ–°ç‚¹
1. com
Abstract
Large language models (LLMs) enhanced with
retrieval augmentation has shown great perfor-
mance in many applications
2. Meanwhile, via a novel
asynchronous memory update mechanism, the
client model can deliver real-time completions
to user inputs without the need to wait for re-
sponses from the cloud
3. However, the large size of these
models and the additional retrieval step introduce
significant computational overhead

#### ğŸ”¬ å®éªŒè®¾è®¡
experiments on
five datasets demonstrate that Hybrid-RACA
offers strong performance while maintaining
low latency.
1 Introduction
Large language models have become powerful
tools in language processing and they are widely
adopted across applications. When augmented with
retrieved documents (Lewis et...

#### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- datasets and
- CorpusAugmentation
- dataset of
- corpus D, retrieval
- Datasets and

#### ğŸ“ æ‘˜è¦
Large language models (LLMs) enhanced with
retrieval augmentation has shown great perfor-
mance in many applications. However, the
computational demands for these models pose
a challenge when applying them to real-time
tasks, such as composition assistance. To
address this, we propose Hybrid Retrieval-
Augmented Composition Assistance (Hybrid-
RACA), a novel system for real-time text pre-
diction that efficiently combines a cloud-based
LLM with a smaller client-side model through
retrieval augme...

---

### 10. HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications

**arXiv ID**: 2409.09046 | **è¯„åˆ†**: 0.9805

#### ğŸ¯ è¦è§£å†³çš„é—®é¢˜
issues by incorporating external knowl-
 challenges
 However, they face challenges


#### ğŸ’¡ ä¸»è¦åˆ›æ–°ç‚¹
1. , 2023a,b;Meta,
2024) have advanced question answering across
domains(Brownetal
2. Advanced techniques like
query rewriters and LLM-based quality checks im-
prove quality but increase token usage and costs
3. These compo-
nentsaddresscommonRAGfailuresandenhance
AI applications in legal and policy domains

#### ğŸ”¬ å®éªŒè®¾è®¡
evaluation (AI, 2023; Saad-Falcon et al.,2023).
3 System Design
TheHybridParameter-AdaptiveRAG(HyPA-RAG)
system, shown in Figure 1, integrates vector-based
textchunksandaknowledgegraphofentitiesand
relationshipstoimproveretrievalaccuracy. Item-
ploysahybridretrievalprocessthatcombinessparse
(BM25)an...

#### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- benchmark. arXiv
- Dataset Generation
- benchmark for
- trec
- dataset,customquestion

#### ğŸ“ æ‘˜è¦
Large Language Models (LLMs) face limita-
tions in AI legal and policy applications due
to outdated knowledge, hallucinations, and
poor reasoning in complex contexts. Retrieval-
Augmented Generation (RAG) systems address
these issues by incorporating external knowl-
edge, but suffer from retrieval errors, ineffec-
tive context integration, and high operational
costs. ThispaperpresentstheHybridParameter-
Adaptive RAG (HyPA-RAG) system, designed
fortheAIlegaldomain,withNYCLocalLaw
144 (LL144) as t...

---

## ğŸ¯ æ€»ä½“è¶‹åŠ¿åˆ†æ

### ğŸ“Š çƒ­é—¨æ•°æ®é›†
- **Wikipedia**: 3 ç¯‡è®ºæ–‡
- **SQuAD**: 3 ç¯‡è®ºæ–‡
- **datasets are**: 2 ç¯‡è®ºæ–‡
- **Trec**: 2 ç¯‡è®ºæ–‡
- **dataset, we**: 2 ç¯‡è®ºæ–‡
- **benchmark. arXiv**: 2 ç¯‡è®ºæ–‡
- **dataset of**: 2 ç¯‡è®ºæ–‡
- **benchmark for**: 2 ç¯‡è®ºæ–‡
- **dataset for**: 2 ç¯‡è®ºæ–‡
- **datasets, encompassing**: 1 ç¯‡è®ºæ–‡

### ğŸ”¥ ä¸»è¦æŠ€æœ¯è¶‹åŠ¿
1. **æ··åˆæ£€ç´¢æ–¹æ³•**: ç»“åˆç¨ å¯†å’Œç¨€ç–æ£€ç´¢çš„ä¼˜åŠ¿
2. **åŠ¨æ€æƒé‡è°ƒæ•´**: æ ¹æ®æŸ¥è¯¢ç‰¹å¾åŠ¨æ€è°ƒæ•´æ£€ç´¢ç­–ç•¥
3. **çŸ¥è¯†å›¾è°±é›†æˆ**: å°†ç»“æ„åŒ–çŸ¥è¯†ä¸å‘é‡æ£€ç´¢ç»“åˆ
4. **è‡ªé€‚åº”RAG**: æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦é€‰æ‹©ä¸åŒçš„æ£€ç´¢ç­–ç•¥
5. **å¤šè·³æ¨ç†**: æ”¯æŒå¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡

