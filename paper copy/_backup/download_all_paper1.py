#!/usr/bin/env python3
"""
ä¸‹è½½paper1.jsonä¸­çš„æ‰€æœ‰53ç¯‡è®ºæ–‡
"""
import json
import os
import requests
from datetime import datetime
import re
import time

def extract_arxiv_id(link):
    """ä»é“¾æ¥ä¸­æå–arXiv ID"""
    patterns = [
        r'arxiv\.org/abs/(\d+\.\d+v?\d*)',
        r'arxiv\.org/pdf/(\d+\.\d+v?\d*)',
        r'(\d{4}\.\d{4,5}v?\d*)'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, link)
        if match:
            return match.group(1)
    return None

def download_paper_safe(arxiv_id, title, download_dir="all_paper1_downloads"):
    """å®‰å…¨ä¸‹è½½è®ºæ–‡PDFï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    os.makedirs(download_dir, exist_ok=True)
    
    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
    safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()
    safe_title = safe_title[:50]  # é™åˆ¶é•¿åº¦
    filename = f"{arxiv_id}_{safe_title}.pdf"
    filepath = os.path.join(download_dir, filename)
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
    if os.path.exists(filepath):
        print(f"  âœ… æ–‡ä»¶å·²å­˜åœ¨: {filename}")
        return filepath
    
    # å°è¯•ä¸‹è½½ï¼Œæœ€å¤šé‡è¯•3æ¬¡
    for attempt in range(3):
        try:
            # ç›´æ¥æ„é€ PDFä¸‹è½½é“¾æ¥
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
            
            # ä¸‹è½½PDF
            response = requests.get(pdf_url, stream=True, timeout=30)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"  âœ… ä¸‹è½½å®Œæˆ: {filename}")
            return filepath
            
        except Exception as e:
            print(f"  âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥ {arxiv_id}: {e}")
            if attempt < 2:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
            else:
                print(f"  âŒ æœ€ç»ˆä¸‹è½½å¤±è´¥ {arxiv_id}")
                return None

def categorize_papers_by_keywords(papers):
    """æ ¹æ®å…³é”®è¯å¯¹è®ºæ–‡è¿›è¡Œåˆ†ç±»"""
    categories = {
        'hybrid_retrieval': [],
        'adaptive_rag': [],
        'query_processing': [],
        'knowledge_fusion': [],
        'multimodal_rag': [],
        'domain_specific': [],
        'evaluation_methods': [],
        'others': []
    }
    
    # å…³é”®è¯æ˜ å°„
    keywords_map = {
        'hybrid_retrieval': ['hybrid', 'blended', 'fusion', 'combine', 'sparse', 'dense'],
        'adaptive_rag': ['adaptive', 'dynamic', 'self-rag', 'corrective', 'tuning'],
        'query_processing': ['query', 'question', 'rewriting', 'complexity', 'planning'],
        'knowledge_fusion': ['knowledge graph', 'multi-hop', 'reasoning', 'graph'],
        'multimodal_rag': ['multimodal', 'vision', 'visual', 'image', 'multi-modal'],
        'domain_specific': ['legal', 'medical', 'financial', 'regulatory', 'domain'],
        'evaluation_methods': ['evaluation', 'benchmark', 'metric', 'assessment', 'survey']
    }
    
    for paper in papers:
        text = (paper['title'] + ' ' + paper.get('abstract', '')).lower()
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åŒ¹é…åˆ†æ•°
        category_scores = {}
        for category, keywords in keywords_map.items():
            score = sum(1 for keyword in keywords if keyword in text)
            category_scores[category] = score
        
        # åˆ†é…åˆ°å¾—åˆ†æœ€é«˜çš„ç±»åˆ«
        best_category = max(category_scores, key=category_scores.get)
        if category_scores[best_category] > 0:
            categories[best_category].append(paper)
        else:
            categories['others'].append(paper)
    
    return categories

def main():
    print("ğŸš€ å¼€å§‹ä¸‹è½½paper1.jsonä¸­çš„æ‰€æœ‰è®ºæ–‡...")
    
    # åŠ è½½è®ºæ–‡åˆ—è¡¨
    with open('paper1.json', 'r', encoding='utf-8') as f:
        papers = json.load(f)
    
    print(f"ğŸ“š æ€»å…±æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    
    # æŒ‰è¯„åˆ†æ’åº
    sorted_papers = sorted(papers, key=lambda x: x.get('score', 0), reverse=True)
    
    downloaded_papers = []
    failed_papers = []
    
    print(f"ğŸ”„ å¼€å§‹æ‰¹é‡ä¸‹è½½...")
    
    for i, paper in enumerate(sorted_papers, 1):
        print(f"\nğŸ“„ [{i}/{len(papers)}] {paper['title'][:60]}...")
        print(f"   è¯„åˆ†: {paper.get('score', 0):.4f}")
        
        # æå–arXiv ID
        arxiv_id = extract_arxiv_id(paper['link'])
        if not arxiv_id:
            print(f"  âŒ æ— æ³•æå–arXiv ID: {paper['link']}")
            failed_papers.append(paper)
            continue
        
        print(f"   arXiv ID: {arxiv_id}")
        
        # ä¸‹è½½è®ºæ–‡
        filepath = download_paper_safe(arxiv_id, paper['title'])
        if filepath:
            downloaded_papers.append({
                'arxiv_id': arxiv_id,
                'title': paper['title'],
                'filepath': filepath,
                'score': paper.get('score', 0),
                'abstract': paper.get('abstract', ''),
                'authors': paper.get('authors', []),
                'publish_time': paper.get('publish_time', ''),
                'link': paper['link']
            })
        else:
            failed_papers.append(paper)
        
        # æ¯ä¸‹è½½10ç¯‡è®ºæ–‡åæš‚åœä¸€ä¸‹ï¼Œé¿å…è¢«é™åˆ¶
        if i % 10 == 0:
            print(f"  ğŸ’¤ å·²ä¸‹è½½{i}ç¯‡ï¼Œæš‚åœ3ç§’...")
            time.sleep(3)
    
    print(f"\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
    print(f"âœ… æˆåŠŸä¸‹è½½: {len(downloaded_papers)} ç¯‡")
    print(f"âŒ ä¸‹è½½å¤±è´¥: {len(failed_papers)} ç¯‡")
    
    # ä¿å­˜ä¸‹è½½è®°å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜æˆåŠŸä¸‹è½½çš„è®ºæ–‡
    with open(f'all_paper1_download_log_{timestamp}.json', 'w', encoding='utf-8') as f:
        json.dump(downloaded_papers, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜å¤±è´¥çš„è®ºæ–‡
    if failed_papers:
        with open(f'failed_downloads_{timestamp}.json', 'w', encoding='utf-8') as f:
            json.dump(failed_papers, f, indent=2, ensure_ascii=False)
    
    # è®ºæ–‡åˆ†ç±»
    categories = categorize_papers_by_keywords(downloaded_papers)
    
    print(f"\nğŸ“‚ è®ºæ–‡åˆ†ç±»ç»“æœ:")
    for category, papers_in_cat in categories.items():
        if papers_in_cat:
            print(f"  {category}: {len(papers_in_cat)} ç¯‡")
    
    # ä¿å­˜åˆ†ç±»ç»“æœ
    with open(f'all_paper1_categories_{timestamp}.json', 'w', encoding='utf-8') as f:
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_categories = {}
        for cat, papers_list in categories.items():
            serializable_categories[cat] = [
                {
                    'arxiv_id': p['arxiv_id'],
                    'title': p['title'],
                    'score': p['score'],
                    'filepath': p['filepath']
                } for p in papers_list
            ]
        json.dump(serializable_categories, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ ä¸‹è½½ç›®å½•: all_paper1_downloads/")
    print(f"ğŸ“„ ä¸‹è½½è®°å½•: all_paper1_download_log_{timestamp}.json")
    print(f"ğŸ“Š åˆ†ç±»ç»“æœ: all_paper1_categories_{timestamp}.json")
    if failed_papers:
        print(f"âŒ å¤±è´¥è®°å½•: failed_downloads_{timestamp}.json")
    
    return downloaded_papers, categories

if __name__ == "__main__":
    main()
