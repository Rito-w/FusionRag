#!/usr/bin/env python3
"""
ä¸‹è½½å…³é”®çš„æœ€æ–°è®ºæ–‡æ¥è¡¥å……åˆ†æ
"""
import arxiv
import os
import requests
from datetime import datetime

def download_paper(paper_info, download_dir):
    """ä¸‹è½½è®ºæ–‡PDF"""
    os.makedirs(download_dir, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶å
    title = paper_info['title']
    safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()
    safe_title = safe_title[:60]  # ç¼©çŸ­æ–‡ä»¶å
    
    # ä»entry_idæå–arxiv ID
    arxiv_id = paper_info['entry_id'].split('/')[-1]
    filename = f"{arxiv_id}_{safe_title}.pdf"
    filepath = os.path.join(download_dir, filename)
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
    if os.path.exists(filepath):
        print(f"  æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
        return filepath
    
    # ä¸‹è½½PDF
    pdf_url = paper_info['pdf_url']
    response = requests.get(pdf_url, stream=True)
    response.raise_for_status()
    
    with open(filepath, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    
    print(f"  âœ… ä¸‹è½½å®Œæˆ: {filename}")
    return filepath

def search_specific_papers():
    """æœç´¢ç‰¹å®šçš„é‡è¦è®ºæ–‡"""
    client = arxiv.Client()
    
    # é‡ç‚¹æœç´¢æŸ¥è¯¢
    key_searches = [
        {
            'query': 'query-aware retrieval fusion',
            'category': '06_recent_downloads',
            'description': 'æŸ¥è¯¢æ„ŸçŸ¥æ£€ç´¢èåˆ'
        },
        {
            'query': 'adaptive weight learning retrieval',
            'category': '06_recent_downloads', 
            'description': 'è‡ªé€‚åº”æƒé‡å­¦ä¹ æ£€ç´¢'
        },
        {
            'query': 'cross-modal attention retrieval',
            'category': '02_multimodal_retrieval',
            'description': 'è·¨æ¨¡æ€æ³¨æ„åŠ›æ£€ç´¢'
        },
        {
            'query': 'incremental vector index update',
            'category': '03_vector_indexing',
            'description': 'å¢é‡å‘é‡ç´¢å¼•æ›´æ–°'
        },
        {
            'query': 'dynamic retrieval reranking',
            'category': '05_reranking',
            'description': 'åŠ¨æ€æ£€ç´¢é‡æ’åº'
        }
    ]
    
    downloaded_papers = []
    
    for search_info in key_searches:
        print(f"\nğŸ” æœç´¢: {search_info['description']}")
        print(f"   æŸ¥è¯¢: {search_info['query']}")
        
        search = arxiv.Search(
            query=search_info['query'],
            max_results=3,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        
        for result in client.results(search):
            paper_info = {
                'title': result.title,
                'authors': [author.name for author in result.authors],
                'summary': result.summary,
                'published': str(result.published),
                'updated': str(result.updated),
                'entry_id': result.entry_id,
                'pdf_url': result.pdf_url,
                'categories': result.categories,
                'primary_category': result.primary_category
            }
            
            print(f"  ğŸ“„ {paper_info['title'][:60]}...")
            print(f"     ä½œè€…: {', '.join(paper_info['authors'][:2])}")
            print(f"     å‘å¸ƒ: {paper_info['published'][:10]}")
            
            try:
                filepath = download_paper(paper_info, search_info['category'])
                downloaded_papers.append({
                    'filepath': filepath,
                    'paper_info': paper_info,
                    'category': search_info['category']
                })
            except Exception as e:
                print(f"  âŒ ä¸‹è½½å¤±è´¥: {e}")
    
    return downloaded_papers

def download_specific_arxiv_papers():
    """ä¸‹è½½ç‰¹å®šçš„arXivè®ºæ–‡"""
    # ä¸€äº›é‡è¦çš„arXiv ID
    specific_papers = [
        {
            'arxiv_id': '2312.10997',  # æœ€æ–°çš„hybrid retrievalå·¥ä½œ
            'category': '01_hybrid_retrieval'
        },
        {
            'arxiv_id': '2401.08808',  # å¤šæ¨¡æ€æ£€ç´¢æ–°æ–¹æ³•
            'category': '02_multimodal_retrieval'
        },
        {
            'arxiv_id': '2311.09476',  # æŸ¥è¯¢ç†è§£ç›¸å…³
            'category': '04_query_understanding'
        }
    ]
    
    client = arxiv.Client()
    downloaded_papers = []
    
    for paper_info in specific_papers:
        print(f"\nğŸ“¥ ä¸‹è½½ç‰¹å®šè®ºæ–‡: {paper_info['arxiv_id']}")
        
        try:
            search = arxiv.Search(id_list=[paper_info['arxiv_id']])
            result = next(client.results(search))
            
            paper_data = {
                'title': result.title,
                'authors': [author.name for author in result.authors],
                'summary': result.summary,
                'published': str(result.published),
                'updated': str(result.updated),
                'entry_id': result.entry_id,
                'pdf_url': result.pdf_url,
                'categories': result.categories,
                'primary_category': result.primary_category
            }
            
            print(f"  ğŸ“„ {paper_data['title']}")
            
            filepath = download_paper(paper_data, paper_info['category'])
            downloaded_papers.append({
                'filepath': filepath,
                'paper_info': paper_data,
                'category': paper_info['category']
            })
            
        except Exception as e:
            print(f"  âŒ ä¸‹è½½å¤±è´¥: {e}")
    
    return downloaded_papers

def main():
    print("ğŸš€ å¼€å§‹ä¸‹è½½å…³é”®è®ºæ–‡...")
    
    # åˆ›å»ºç›®å½•
    os.makedirs("06_recent_downloads", exist_ok=True)
    
    # æœç´¢å¹¶ä¸‹è½½è®ºæ–‡
    search_papers = search_specific_papers()
    specific_papers = download_specific_arxiv_papers()
    
    all_papers = search_papers + specific_papers
    
    print(f"\nğŸ“Š ä¸‹è½½æ€»ç»“:")
    print(f"æ€»è®¡ä¸‹è½½: {len(all_papers)} ç¯‡è®ºæ–‡")
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    category_count = {}
    for paper in all_papers:
        cat = paper['category']
        category_count[cat] = category_count.get(cat, 0) + 1
    
    for cat, count in category_count.items():
        print(f"  {cat}: {count} ç¯‡")
    
    # ä¿å­˜ä¸‹è½½è®°å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    with open(f"06_recent_downloads/download_log_{timestamp}.txt", 'w', encoding='utf-8') as f:
        f.write("ä¸‹è½½çš„å…³é”®è®ºæ–‡åˆ—è¡¨\n")
        f.write("="*50 + "\n\n")
        
        for paper in all_papers:
            f.write(f"æ–‡ä»¶: {paper['filepath']}\n")
            f.write(f"æ ‡é¢˜: {paper['paper_info']['title']}\n")
            f.write(f"ä½œè€…: {', '.join(paper['paper_info']['authors'])}\n")
            f.write(f"å‘å¸ƒ: {paper['paper_info']['published'][:10]}\n")
            f.write(f"ç±»åˆ«: {paper['category']}\n")
            f.write(f"æ‘˜è¦: {paper['paper_info']['summary'][:200]}...\n")
            f.write("-" * 50 + "\n\n")
    
    print(f"\nâœ… ä¸‹è½½å®Œæˆï¼è¯¦ç»†è®°å½•ä¿å­˜åœ¨ 06_recent_downloads/download_log_{timestamp}.txt")
    
    return all_papers

if __name__ == "__main__":
    main()
