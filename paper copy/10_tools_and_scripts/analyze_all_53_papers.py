#!/usr/bin/env python3
"""
åˆ†ææ‰€æœ‰53ç¯‡è®ºæ–‡ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Š
"""
import json
import PyPDF2
import re
from pathlib import Path
from datetime import datetime
import os

def extract_text_from_pdf(pdf_path, max_pages=8):
    """ä»PDFä¸­æå–æ–‡æœ¬ï¼Œé™åˆ¶é¡µæ•°ä»¥æé«˜æ•ˆç‡"""
    try:
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ""
            # åªè¯»å–å‰å‡ é¡µï¼ŒåŒ…å«æ‘˜è¦ã€ä»‹ç»ã€æ–¹æ³•ç­‰å…³é”®ä¿¡æ¯
            for page_num in range(min(max_pages, len(reader.pages))):
                text += reader.pages[page_num].extract_text() + "\n"
        return text
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–PDF {pdf_path}: {e}")
        return ""

def extract_key_information(text, paper_info):
    """æå–è®ºæ–‡çš„å…³é”®ä¿¡æ¯"""
    analysis = {
        'arxiv_id': paper_info['arxiv_id'],
        'title': paper_info['title'],
        'score': paper_info['score'],
        'problem_statement': '',
        'innovation_points': [],
        'limitations': [],
        'experiments': '',
        'datasets': [],
        'methods': '',
        'contributions': [],
        'abstract': ''
    }
    
    # æå–æ‘˜è¦
    abstract_patterns = [
        r'Abstract\s*\n(.*?)\n\s*(?:1\s+Introduction|Introduction|Keywords)',
        r'ABSTRACT\s*\n(.*?)\n\s*(?:1\s+Introduction|Introduction|Keywords)',
        r'Abstract\s*[:\-]?\s*(.*?)\n\s*(?:1\.|Introduction|Keywords)'
    ]
    
    for pattern in abstract_patterns:
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            analysis['abstract'] = match.group(1).strip()[:600] + "..."
            break
    
    if not analysis['abstract']:
        analysis['abstract'] = paper_info.get('abstract', '')[:600] + "..."
    
    # æŸ¥æ‰¾é—®é¢˜é™ˆè¿°
    problem_keywords = [
        'problem', 'challenge', 'issue', 'limitation', 'difficulty',
        'However', 'Nevertheless', 'Unfortunately', 'struggle', 'suffer'
    ]
    
    problem_sentences = []
    sentences = re.split(r'[.!?]', text)
    for sentence in sentences[:150]:  # æ£€æŸ¥å‰150å¥
        if any(keyword.lower() in sentence.lower() for keyword in problem_keywords):
            if 20 < len(sentence.strip()) < 200:
                problem_sentences.append(sentence.strip())
    
    analysis['problem_statement'] = ' '.join(problem_sentences[:3])
    
    # æŸ¥æ‰¾åˆ›æ–°ç‚¹
    innovation_keywords = [
        'novel', 'new', 'propose', 'introduce', 'first', 'innovative',
        'breakthrough', 'advance', 'improve', 'enhance', 'outperform',
        'contribution', 'key insight', 'main idea'
    ]
    
    innovation_sentences = []
    for sentence in sentences[:200]:  # æ£€æŸ¥å‰200å¥
        if any(keyword.lower() in sentence.lower() for keyword in innovation_keywords):
            if 30 < len(sentence.strip()) < 250:
                innovation_sentences.append(sentence.strip())
    
    analysis['innovation_points'] = innovation_sentences[:5]
    
    # æŸ¥æ‰¾å±€é™æ€§
    limitation_keywords = [
        'limitation', 'drawback', 'weakness', 'shortcoming', 'constraint',
        'future work', 'not address', 'cannot handle', 'fails to'
    ]
    
    limitation_sentences = []
    for sentence in sentences:
        if any(keyword.lower() in sentence.lower() for keyword in limitation_keywords):
            if 20 < len(sentence.strip()) < 200:
                limitation_sentences.append(sentence.strip())
    
    analysis['limitations'] = limitation_sentences[:3]
    
    # æŸ¥æ‰¾æ•°æ®é›†
    dataset_patterns = [
        r'(?:MS MARCO|Natural Questions|SQuAD|TREC|BEIR|HotpotQA|FiQA)',
        r'(?:Wikipedia|Common Crawl|OpenQA|WebQA|KILT)',
        r'(?:dataset|benchmark|corpus).*?([A-Z][A-Z0-9-]+)',
        r'(?:evaluate|experiment).*?on.*?([A-Z][A-Za-z0-9-]+)'
    ]
    
    datasets = set()
    for pattern in dataset_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for match in matches:
            if isinstance(match, str) and 2 < len(match) < 30:
                datasets.add(match.strip())
    
    analysis['datasets'] = list(datasets)[:8]
    
    # æŸ¥æ‰¾å®éªŒä¿¡æ¯
    exp_patterns = [
        r'(?:Experiment|Evaluation|Results).*?(?:show|demonstrate|achieve).*?[.!?]',
        r'(?:performance|accuracy|improvement).*?(?:\d+\.?\d*%|\d+\.?\d*)',
        r'(?:outperform|better than|superior to).*?[.!?]'
    ]
    
    exp_sentences = []
    for pattern in exp_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
        exp_sentences.extend(matches[:2])
    
    analysis['experiments'] = ' '.join(exp_sentences[:3])
    
    return analysis

def analyze_all_papers():
    """åˆ†ææ‰€æœ‰ä¸‹è½½çš„è®ºæ–‡"""
    
    # åŠ è½½ä¸‹è½½è®°å½•
    download_files = [f for f in os.listdir('.') if f.startswith('all_paper1_download_log_')]
    if not download_files:
        print("âŒ æœªæ‰¾åˆ°ä¸‹è½½è®°å½•æ–‡ä»¶")
        return
    
    latest_file = max(download_files)
    print(f"ğŸ“„ åŠ è½½ä¸‹è½½è®°å½•: {latest_file}")
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        downloaded_papers = json.load(f)
    
    print(f"ğŸ” å¼€å§‹åˆ†æ {len(downloaded_papers)} ç¯‡è®ºæ–‡...")
    
    all_analyses = []
    
    for i, paper in enumerate(downloaded_papers, 1):
        print(f"\nğŸ“„ [{i}/{len(downloaded_papers)}] åˆ†æ: {paper['title'][:50]}...")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(paper['filepath']):
            print(f"    âŒ æ–‡ä»¶ä¸å­˜åœ¨: {paper['filepath']}")
            continue
        
        # æå–PDFæ–‡æœ¬
        text = extract_text_from_pdf(paper['filepath'])
        
        if text:
            # åˆ†æè®ºæ–‡å†…å®¹
            analysis = extract_key_information(text, paper)
            all_analyses.append(analysis)
            print(f"    âœ… åˆ†æå®Œæˆ - åˆ›æ–°ç‚¹: {len(analysis['innovation_points'])}, æ•°æ®é›†: {len(analysis['datasets'])}")
        else:
            print(f"    âŒ æ— æ³•æå–æ–‡æœ¬")
    
    print(f"\nğŸ“Š æˆåŠŸåˆ†æäº† {len(all_analyses)} ç¯‡è®ºæ–‡")
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    generate_comprehensive_report(all_analyses)
    
    return all_analyses

def generate_comprehensive_report(analyses):
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"comprehensive_53_papers_analysis_{timestamp}.md"
    
    print(f"ğŸ“ ç”Ÿæˆç»¼åˆæŠ¥å‘Š: {report_file}")
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# 53ç¯‡RAGç›¸å…³è®ºæ–‡ç»¼åˆåˆ†ææŠ¥å‘Š\n\n")
        f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"åˆ†æè®ºæ–‡æ•°é‡: {len(analyses)}\n\n")
        
        # æŒ‰è¯„åˆ†æ’åº
        sorted_analyses = sorted(analyses, key=lambda x: x['score'], reverse=True)
        
        # ç”Ÿæˆæ¦‚è§ˆè¡¨æ ¼
        f.write("## ğŸ“Š è®ºæ–‡æ¦‚è§ˆ (æŒ‰è¯„åˆ†æ’åº)\n\n")
        f.write("| æ’å | arXiv ID | æ ‡é¢˜ | è¯„åˆ† | ä¸»è¦åˆ›æ–° |\n")
        f.write("|------|----------|------|------|----------|\n")
        
        for i, analysis in enumerate(sorted_analyses[:20], 1):  # åªæ˜¾ç¤ºå‰20ç¯‡
            title_short = analysis['title'][:35] + "..." if len(analysis['title']) > 35 else analysis['title']
            innovation_short = analysis['innovation_points'][0][:40] + "..." if analysis['innovation_points'] else "N/A"
            f.write(f"| {i} | {analysis['arxiv_id']} | {title_short} | {analysis['score']:.4f} | {innovation_short} |\n")
        
        # æŠ€æœ¯è¶‹åŠ¿åˆ†æ
        f.write("\n## ğŸ”¥ æŠ€æœ¯è¶‹åŠ¿åˆ†æ\n\n")
        
        # ç»Ÿè®¡çƒ­é—¨æ•°æ®é›†
        all_datasets = []
        for analysis in analyses:
            all_datasets.extend(analysis['datasets'])
        
        dataset_counts = {}
        for dataset in all_datasets:
            if len(dataset) > 2:  # è¿‡æ»¤å¤ªçŸ­çš„
                dataset_counts[dataset] = dataset_counts.get(dataset, 0) + 1
        
        f.write("### ğŸ“Š çƒ­é—¨æ•°æ®é›†\n")
        sorted_datasets = sorted(dataset_counts.items(), key=lambda x: x[1], reverse=True)
        for dataset, count in sorted_datasets[:15]:
            f.write(f"- **{dataset}**: {count} ç¯‡è®ºæ–‡\n")
        
        # åˆ›æ–°ç‚¹åˆ†æ
        f.write("\n### ğŸ’¡ ä¸»è¦åˆ›æ–°æ–¹å‘\n")
        
        innovation_keywords = {}
        for analysis in analyses:
            for innovation in analysis['innovation_points']:
                words = re.findall(r'\b\w+\b', innovation.lower())
                for word in words:
                    if len(word) > 4:  # åªç»Ÿè®¡é•¿è¯
                        innovation_keywords[word] = innovation_keywords.get(word, 0) + 1
        
        sorted_keywords = sorted(innovation_keywords.items(), key=lambda x: x[1], reverse=True)
        f.write("**çƒ­é—¨æŠ€æœ¯å…³é”®è¯**:\n")
        for keyword, count in sorted_keywords[:20]:
            f.write(f"- {keyword}: {count} æ¬¡\n")
        
        # è¯¦ç»†åˆ†æå‰10ç¯‡é«˜åˆ†è®ºæ–‡
        f.write("\n## ğŸ“‹ é«˜åˆ†è®ºæ–‡è¯¦ç»†åˆ†æ (Top 10)\n\n")
        
        for i, analysis in enumerate(sorted_analyses[:10], 1):
            f.write(f"### {i}. {analysis['title']}\n\n")
            f.write(f"**arXiv ID**: {analysis['arxiv_id']} | **è¯„åˆ†**: {analysis['score']:.4f}\n\n")
            
            f.write("#### ğŸ¯ è¦è§£å†³çš„é—®é¢˜\n")
            if analysis['problem_statement']:
                f.write(f"{analysis['problem_statement'][:400]}...\n\n")
            else:
                f.write("æœªæ˜ç¡®æå–åˆ°é—®é¢˜é™ˆè¿°\n\n")
            
            f.write("#### ğŸ’¡ ä¸»è¦åˆ›æ–°ç‚¹\n")
            if analysis['innovation_points']:
                for j, innovation in enumerate(analysis['innovation_points'][:3], 1):
                    f.write(f"{j}. {innovation[:200]}...\n")
            else:
                f.write("æœªæ˜ç¡®æå–åˆ°åˆ›æ–°ç‚¹\n")
            f.write("\n")
            
            f.write("#### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†\n")
            if analysis['datasets']:
                for dataset in analysis['datasets'][:6]:
                    f.write(f"- {dataset}\n")
            else:
                f.write("- æœªæ˜ç¡®æå–åˆ°æ•°æ®é›†ä¿¡æ¯\n")
            f.write("\n")
            
            f.write("#### ğŸ”¬ å®éªŒç»“æœ\n")
            if analysis['experiments']:
                f.write(f"{analysis['experiments'][:300]}...\n\n")
            else:
                f.write("æœªæå–åˆ°å®éªŒä¿¡æ¯\n\n")
            
            f.write("#### âš ï¸ æŠ€æœ¯å±€é™æ€§\n")
            if analysis['limitations']:
                for limitation in analysis['limitations'][:2]:
                    f.write(f"- {limitation[:150]}...\n")
            else:
                f.write("- æœªæ˜ç¡®æå–åˆ°å±€é™æ€§ä¿¡æ¯\n")
            f.write("\n")
            
            f.write("---\n\n")
        
        # æ€»ç»“å’Œå¯ç¤º
        f.write("## ğŸ¯ å¯¹æˆ‘ä»¬ç ”ç©¶çš„å…³é”®å¯ç¤º\n\n")
        f.write("### 1. æŠ€æœ¯å‘å±•è¶‹åŠ¿\n")
        f.write("- **æ··åˆæ£€ç´¢æˆä¸ºä¸»æµ**: å¤§å¤šæ•°è®ºæ–‡éƒ½é‡‡ç”¨æŸç§å½¢å¼çš„æ··åˆæ£€ç´¢\n")
        f.write("- **åŠ¨æ€é€‚åº”æ˜¯å…³é”®**: ä»å›ºå®šç­–ç•¥å‘è‡ªé€‚åº”ç­–ç•¥å‘å±•\n")
        f.write("- **æŸ¥è¯¢ç†è§£é‡è¦æ€§**: è¶Šæ¥è¶Šå¤šçš„å·¥ä½œå…³æ³¨æŸ¥è¯¢ç†è§£å’Œå¤„ç†\n")
        f.write("- **é¢†åŸŸç‰¹åŒ–è¶‹åŠ¿**: é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„RAGç³»ç»Ÿè¶Šæ¥è¶Šå¤š\n\n")
        
        f.write("### 2. åˆ›æ–°ç©ºé—´è¯†åˆ«\n")
        f.write("- **æŸ¥è¯¢æ„å›¾åˆ†ç±»**: è™½ç„¶æœ‰æŸ¥è¯¢å¤æ‚åº¦åˆ†æï¼Œä½†ç»†ç²’åº¦æ„å›¾åˆ†ç±»ä»æœ‰ç©ºé—´\n")
        f.write("- **è½»é‡çº§å®ç°**: å¤§å¤šæ•°æ–¹æ³•è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ï¼Œè½»é‡çº§æ–¹æ¡ˆæœ‰éœ€æ±‚\n")
        f.write("- **ç­–ç•¥çº§åˆ›æ–°**: ä»æƒé‡è°ƒæ•´å‡çº§åˆ°ç­–ç•¥é€‰æ‹©çš„åˆ›æ–°ç©ºé—´å¾ˆå¤§\n")
        f.write("- **å®æ—¶æ€§ä¼˜åŒ–**: å®æ—¶åº”ç”¨åœºæ™¯çš„ä¼˜åŒ–éœ€æ±‚æ˜æ˜¾\n\n")
        
        f.write("### 3. æˆ‘ä»¬æ–¹æ¡ˆçš„ä¼˜åŠ¿\n")
        f.write("- **å¡«è¡¥ç©ºç™½**: æŸ¥è¯¢æ„å›¾æ„ŸçŸ¥çš„è‡ªé€‚åº”æ£€ç´¢ç­–ç•¥å¡«è¡¥äº†é‡è¦ç©ºç™½\n")
        f.write("- **æŠ€æœ¯å¯è¡Œ**: åŸºäºç°æœ‰æŠ€æœ¯æ ˆï¼Œå®ç°éš¾åº¦é€‚ä¸­\n")
        f.write("- **æ€§èƒ½ä¼˜åŠ¿**: é¢„æœŸèƒ½å¤Ÿæ˜¾è‘—æå‡ä¸åŒç±»å‹æŸ¥è¯¢çš„æ€§èƒ½\n")
        f.write("- **å®ç”¨ä»·å€¼**: å¯ä»¥ç›´æ¥åº”ç”¨äºç°æœ‰æ£€ç´¢ç³»ç»Ÿ\n\n")
        
        f.write("### 4. å®éªŒè®¾è®¡å‚è€ƒ\n")
        f.write("- **æ ‡å‡†æ•°æ®é›†**: MS MARCO, SQuAD, Natural Questionsæ˜¯å¿…é€‰\n")
        f.write("- **è¯„ä¼°æŒ‡æ ‡**: NDCG@10, MRR, MAPæ˜¯æ ‡å‡†æŒ‡æ ‡\n")
        f.write("- **é‡è¦åŸºçº¿**: DAT, Adaptive-RAG, Self-RAGç­‰æ˜¯é‡è¦å¯¹æ¯”å¯¹è±¡\n")
        f.write("- **æ¶ˆèç ”ç©¶**: éœ€è¦è¯¦ç»†çš„ç»„ä»¶è´¡çŒ®åº¦åˆ†æ\n\n")
    
    print(f"âœ… ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    return report_file

if __name__ == "__main__":
    analyze_all_papers()
