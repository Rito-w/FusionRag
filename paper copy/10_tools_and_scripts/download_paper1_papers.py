#!/usr/bin/env python3
"""
ä¸‹è½½paper1.jsonä¸­çš„é‡è¦è®ºæ–‡å¹¶è¿›è¡Œåˆ†æ
"""
import json
import arxiv
import os
import requests
from datetime import datetime
import re

def extract_arxiv_id(link):
    """ä»é“¾æ¥ä¸­æå–arXiv ID"""
    match = re.search(r'arxiv\.org/abs/(\d+\.\d+)', link)
    if match:
        return match.group(1)
    return None

def download_paper(arxiv_id, title, download_dir="paper1_downloads"):
    """ä¸‹è½½è®ºæ–‡PDF"""
    os.makedirs(download_dir, exist_ok=True)
    
    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
    safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()
    safe_title = safe_title[:80]  # é™åˆ¶é•¿åº¦
    filename = f"{arxiv_id}_{safe_title}.pdf"
    filepath = os.path.join(download_dir, filename)
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
    if os.path.exists(filepath):
        print(f"  æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
        return filepath
    
    try:
        # ä½¿ç”¨arXiv APIä¸‹è½½
        client = arxiv.Client()
        search = arxiv.Search(id_list=[arxiv_id])
        result = next(client.results(search))
        
        # ä¸‹è½½PDF
        pdf_url = result.pdf_url
        response = requests.get(pdf_url, stream=True)
        response.raise_for_status()
        
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"  âœ… ä¸‹è½½å®Œæˆ: {filename}")
        return filepath
        
    except Exception as e:
        print(f"  âŒ ä¸‹è½½å¤±è´¥ {arxiv_id}: {e}")
        return None

def load_paper1_json():
    """åŠ è½½paper1.jsonæ–‡ä»¶"""
    with open('paper1.json', 'r', encoding='utf-8') as f:
        papers = json.load(f)
    return papers

def download_top_papers(papers, top_n=15):
    """ä¸‹è½½è¯„åˆ†æœ€é«˜çš„å‰Nç¯‡è®ºæ–‡"""
    # æŒ‰è¯„åˆ†æ’åº
    sorted_papers = sorted(papers, key=lambda x: x.get('score', 0), reverse=True)
    
    downloaded_papers = []
    
    print(f"ğŸš€ å¼€å§‹ä¸‹è½½å‰{top_n}ç¯‡é«˜åˆ†è®ºæ–‡...")
    
    for i, paper in enumerate(sorted_papers[:top_n], 1):
        print(f"\nğŸ“„ [{i}/{top_n}] {paper['title'][:60]}...")
        print(f"   è¯„åˆ†: {paper.get('score', 0):.4f}")
        print(f"   å‘å¸ƒ: {paper.get('publish_time', 'Unknown')}")
        print(f"   ä½œè€…: {', '.join(paper.get('authors', [])[:2])}")
        
        # æå–arXiv ID
        arxiv_id = extract_arxiv_id(paper['link'])
        if not arxiv_id:
            print(f"  âŒ æ— æ³•æå–arXiv ID: {paper['link']}")
            continue
        
        # ä¸‹è½½è®ºæ–‡
        filepath = download_paper(arxiv_id, paper['title'])
        if filepath:
            downloaded_papers.append({
                'arxiv_id': arxiv_id,
                'title': paper['title'],
                'filepath': filepath,
                'score': paper.get('score', 0),
                'abstract': paper.get('abstract', ''),
                'authors': paper.get('authors', []),
                'publish_time': paper.get('publish_time', '')
            })
    
    return downloaded_papers

def create_analysis_summary(downloaded_papers):
    """åˆ›å»ºè®ºæ–‡åˆ†ææ€»ç»“"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    summary_file = f"paper1_analysis_summary_{timestamp}.md"
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("# Paper1.json é‡è¦è®ºæ–‡åˆ†ææ€»ç»“\n\n")
        f.write(f"ä¸‹è½½æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»è®¡ä¸‹è½½: {len(downloaded_papers)} ç¯‡è®ºæ–‡\n\n")
        
        f.write("## ğŸ“Š è®ºæ–‡æ¦‚è§ˆ\n\n")
        f.write("| æ’å | arXiv ID | æ ‡é¢˜ | è¯„åˆ† | å‘å¸ƒæ—¶é—´ |\n")
        f.write("|------|----------|------|------|----------|\n")
        
        for i, paper in enumerate(downloaded_papers, 1):
            title_short = paper['title'][:50] + "..." if len(paper['title']) > 50 else paper['title']
            f.write(f"| {i} | {paper['arxiv_id']} | {title_short} | {paper['score']:.4f} | {paper['publish_time']} |\n")
        
        f.write("\n## ğŸ“‹ è¯¦ç»†ä¿¡æ¯\n\n")
        
        for i, paper in enumerate(downloaded_papers, 1):
            f.write(f"### {i}. {paper['title']}\n\n")
            f.write(f"- **arXiv ID**: {paper['arxiv_id']}\n")
            f.write(f"- **è¯„åˆ†**: {paper['score']:.4f}\n")
            f.write(f"- **ä½œè€…**: {', '.join(paper['authors'])}\n")
            f.write(f"- **å‘å¸ƒæ—¶é—´**: {paper['publish_time']}\n")
            f.write(f"- **æ–‡ä»¶è·¯å¾„**: {paper['filepath']}\n\n")
            f.write(f"**æ‘˜è¦**:\n{paper['abstract'][:300]}...\n\n")
            f.write("---\n\n")
    
    print(f"\nğŸ“„ åˆ†ææ€»ç»“å·²ä¿å­˜åˆ°: {summary_file}")
    return summary_file

def categorize_papers(downloaded_papers):
    """æ ¹æ®è®ºæ–‡å†…å®¹è¿›è¡Œåˆ†ç±»"""
    categories = {
        'hybrid_retrieval': [],
        'adaptive_rag': [],
        'query_understanding': [],
        'knowledge_fusion': [],
        'evaluation_methods': [],
        'domain_specific': []
    }
    
    # å…³é”®è¯åˆ†ç±»
    keywords_map = {
        'hybrid_retrieval': ['hybrid', 'blended', 'fusion', 'combine', 'sparse', 'dense'],
        'adaptive_rag': ['adaptive', 'dynamic', 'self-rag', 'corrective', 'tuning'],
        'query_understanding': ['query', 'question', 'complexity', 'intent', 'rewriting'],
        'knowledge_fusion': ['knowledge graph', 'multi-hop', 'reasoning', 'graph'],
        'evaluation_methods': ['evaluation', 'benchmark', 'metric', 'assessment'],
        'domain_specific': ['legal', 'medical', 'financial', 'regulatory', 'domain']
    }
    
    for paper in downloaded_papers:
        text = (paper['title'] + ' ' + paper['abstract']).lower()
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åŒ¹é…åˆ†æ•°
        category_scores = {}
        for category, keywords in keywords_map.items():
            score = sum(1 for keyword in keywords if keyword in text)
            category_scores[category] = score
        
        # åˆ†é…åˆ°å¾—åˆ†æœ€é«˜çš„ç±»åˆ«
        best_category = max(category_scores, key=category_scores.get)
        if category_scores[best_category] > 0:
            categories[best_category].append(paper)
        else:
            categories['hybrid_retrieval'].append(paper)  # é»˜è®¤åˆ†ç±»
    
    return categories

def main():
    print("ğŸ” å¼€å§‹å¤„ç†paper1.jsonä¸­çš„è®ºæ–‡...")
    
    # åŠ è½½è®ºæ–‡åˆ—è¡¨
    papers = load_paper1_json()
    print(f"ğŸ“š æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    
    # ä¸‹è½½å‰15ç¯‡é«˜åˆ†è®ºæ–‡
    downloaded_papers = download_top_papers(papers, top_n=15)
    
    # åˆ›å»ºåˆ†ææ€»ç»“
    summary_file = create_analysis_summary(downloaded_papers)
    
    # è®ºæ–‡åˆ†ç±»
    categories = categorize_papers(downloaded_papers)
    
    print(f"\nğŸ“Š è®ºæ–‡åˆ†ç±»ç»“æœ:")
    for category, papers_in_cat in categories.items():
        if papers_in_cat:
            print(f"  {category}: {len(papers_in_cat)} ç¯‡")
    
    # ä¿å­˜åˆ†ç±»ç»“æœ
    with open('paper1_categories.json', 'w', encoding='utf-8') as f:
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_categories = {}
        for cat, papers_list in categories.items():
            serializable_categories[cat] = [
                {
                    'arxiv_id': p['arxiv_id'],
                    'title': p['title'],
                    'score': p['score']
                } for p in papers_list
            ]
        json.dump(serializable_categories, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ ä¸‹è½½ç›®å½•: paper1_downloads/")
    print(f"ğŸ“„ åˆ†ææ€»ç»“: {summary_file}")
    print(f"ğŸ“Š åˆ†ç±»ç»“æœ: paper1_categories.json")
    
    return downloaded_papers, categories

if __name__ == "__main__":
    main()
