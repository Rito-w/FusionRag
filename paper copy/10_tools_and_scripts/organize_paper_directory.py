#!/usr/bin/env python3
"""
Paperç›®å½•æ•´ç†å·¥å…·

ä½¿ç”¨MCPå¸®åŠ©æ•´ç†æ··ä¹±çš„paperç›®å½•ç»“æ„
"""

import os
import shutil
import json
from pathlib import Path
from typing import Dict, List, Set, Tuple
import hashlib
import re
from collections import defaultdict

class PaperDirectoryOrganizer:
    """Paperç›®å½•æ•´ç†å™¨"""
    
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.backup_dir = self.base_dir / "_backup"
        
        # å®šä¹‰ç›®å½•ç»“æ„
        self.categories = {
            "01_hybrid_retrieval": ["hybrid", "blend", "fusion", "mix", "combine"],
            "02_multimodal_retrieval": ["multimodal", "vision", "image", "visual", "mm-"],
            "03_vector_indexing": ["vector", "embedding", "index", "faiss", "dense"],
            "04_query_understanding": ["query", "question", "understanding", "rewrite"],
            "05_reranking": ["rerank", "rank", "scoring", "relevance"],
            "06_evaluation": ["evaluation", "benchmark", "metric", "assess"],
            "07_core_papers": ["rag", "retrieval-augmented", "realm", "retro"],
            "08_surveys": ["survey", "review", "comprehensive", "overview"],
            "09_applications": ["application", "domain", "specific", "bio", "legal"],
            "10_tools_and_scripts": [],  # Pythonè„šæœ¬
            "11_analysis_results": [],   # åˆ†æç»“æœ
            "12_logs_and_configs": []    # æ—¥å¿—å’Œé…ç½®
        }
        
        # æ–‡ä»¶ç±»å‹æ˜ å°„
        self.file_type_mapping = {
            ".pdf": "papers",
            ".py": "10_tools_and_scripts",
            ".md": "11_analysis_results",
            ".json": "12_logs_and_configs",
            ".txt": "12_logs_and_configs"
        }
    
    def create_backup(self):
        """åˆ›å»ºå¤‡ä»½"""
        if self.backup_dir.exists():
            shutil.rmtree(self.backup_dir)
        
        print("ğŸ“¦ åˆ›å»ºå¤‡ä»½...")
        self.backup_dir.mkdir()
        
        # å¤‡ä»½æ ¹ç›®å½•çš„é‡è¦æ–‡ä»¶
        for item in self.base_dir.iterdir():
            if item.name.startswith("_"):
                continue
            if item.is_file():
                shutil.copy2(item, self.backup_dir / item.name)
            elif item.is_dir() and not item.name.startswith(("01_", "02_", "03_")):
                shutil.copytree(item, self.backup_dir / item.name)
        
        print(f"âœ… å¤‡ä»½å®Œæˆ: {self.backup_dir}")
    
    def analyze_current_structure(self) -> Dict:
        """åˆ†æå½“å‰ç›®å½•ç»“æ„"""
        print("ğŸ” åˆ†æå½“å‰ç›®å½•ç»“æ„...")
        
        analysis = {
            "total_files": 0,
            "pdf_files": 0,
            "duplicate_files": [],
            "misplaced_files": [],
            "file_distribution": defaultdict(int),
            "file_hashes": defaultdict(list)
        }
        
        # éå†æ‰€æœ‰æ–‡ä»¶
        for root, dirs, files in os.walk(self.base_dir):
            root_path = Path(root)
            
            # è·³è¿‡å¤‡ä»½ç›®å½•
            if "_backup" in root_path.parts:
                continue
            
            for file in files:
                file_path = root_path / file
                analysis["total_files"] += 1
                
                # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
                suffix = file_path.suffix.lower()
                analysis["file_distribution"][suffix] += 1
                
                if suffix == ".pdf":
                    analysis["pdf_files"] += 1
                
                # æ£€æŸ¥é‡å¤æ–‡ä»¶
                try:
                    file_hash = self.get_file_hash(file_path)
                    analysis["file_hashes"][file_hash].append(str(file_path))
                except:
                    pass
        
        # æ‰¾å‡ºé‡å¤æ–‡ä»¶
        for file_hash, paths in analysis["file_hashes"].items():
            if len(paths) > 1:
                analysis["duplicate_files"].append(paths)
        
        return analysis
    
    def get_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except:
            return ""
    
    def classify_paper(self, filename: str) -> str:
        """æ ¹æ®æ–‡ä»¶ååˆ†ç±»è®ºæ–‡"""
        filename_lower = filename.lower()
        
        # æ£€æŸ¥æ¯ä¸ªåˆ†ç±»çš„å…³é”®è¯
        for category, keywords in self.categories.items():
            if category.startswith("1"):  # è·³è¿‡å·¥å…·ç›®å½•
                continue
            
            for keyword in keywords:
                if keyword in filename_lower:
                    return category
        
        # é»˜è®¤åˆ†ç±»
        if "rag" in filename_lower or "retrieval" in filename_lower:
            return "07_core_papers"
        else:
            return "09_applications"
    
    def clean_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶å"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        cleaned = re.sub(r'[<>:"/\\|?*]', '_', filename)
        
        # é™åˆ¶é•¿åº¦
        name, ext = os.path.splitext(cleaned)
        if len(name) > 100:
            name = name[:100]
        
        return name + ext
    
    def create_directory_structure(self):
        """åˆ›å»ºç›®å½•ç»“æ„"""
        print("ğŸ“ åˆ›å»ºç›®å½•ç»“æ„...")
        
        for category in self.categories.keys():
            category_dir = self.base_dir / category
            category_dir.mkdir(exist_ok=True)
            
            # åˆ›å»ºREADMEæ–‡ä»¶
            readme_file = category_dir / "README.md"
            if not readme_file.exists():
                with open(readme_file, 'w', encoding='utf-8') as f:
                    f.write(f"# {category}\n\n")
                    if category == "10_tools_and_scripts":
                        f.write("Pythonè„šæœ¬å’Œå·¥å…·\n")
                    elif category == "11_analysis_results":
                        f.write("åˆ†æç»“æœå’Œæ€»ç»“æ–‡æ¡£\n")
                    elif category == "12_logs_and_configs":
                        f.write("æ—¥å¿—æ–‡ä»¶å’Œé…ç½®æ–‡ä»¶\n")
                    else:
                        f.write(f"ç›¸å…³è®ºæ–‡å’Œæ–‡æ¡£\n")
        
        print("âœ… ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ")
    
    def organize_files(self, dry_run: bool = True):
        """æ•´ç†æ–‡ä»¶"""
        print(f"ğŸ—‚ï¸ å¼€å§‹æ•´ç†æ–‡ä»¶ (dry_run={dry_run})...")
        
        moved_files = []
        
        # å¤„ç†æ ¹ç›®å½•çš„æ–‡ä»¶
        for item in self.base_dir.iterdir():
            if item.name.startswith(("_", "01_", "02_", "03_", "04_", "05_", "06_", "07_", "08_", "09_", "10_", "11_", "12_")):
                continue
            
            if item.is_file():
                target_category = self.determine_target_category(item)
                target_dir = self.base_dir / target_category
                
                # æ¸…ç†æ–‡ä»¶å
                clean_name = self.clean_filename(item.name)
                target_path = target_dir / clean_name
                
                # é¿å…è¦†ç›–
                counter = 1
                while target_path.exists():
                    name, ext = os.path.splitext(clean_name)
                    target_path = target_dir / f"{name}_{counter}{ext}"
                    counter += 1
                
                moved_files.append({
                    "source": str(item),
                    "target": str(target_path),
                    "category": target_category
                })
                
                if not dry_run:
                    shutil.move(str(item), str(target_path))
                    print(f"   ç§»åŠ¨: {item.name} -> {target_category}/")
        
        return moved_files
    
    def determine_target_category(self, file_path: Path) -> str:
        """ç¡®å®šæ–‡ä»¶çš„ç›®æ ‡åˆ†ç±»"""
        suffix = file_path.suffix.lower()
        filename = file_path.name
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹
        if suffix == ".py":
            return "10_tools_and_scripts"
        elif suffix == ".md" and any(word in filename.lower() for word in ["analysis", "summary", "report", "result"]):
            return "11_analysis_results"
        elif suffix in [".json", ".txt", ".log"]:
            return "12_logs_and_configs"
        elif suffix == ".pdf":
            return self.classify_paper(filename)
        else:
            return "12_logs_and_configs"
    
    def remove_duplicates(self, dry_run: bool = True):
        """ç§»é™¤é‡å¤æ–‡ä»¶"""
        print(f"ğŸ”„ ç§»é™¤é‡å¤æ–‡ä»¶ (dry_run={dry_run})...")
        
        analysis = self.analyze_current_structure()
        removed_files = []
        
        for duplicate_group in analysis["duplicate_files"]:
            if len(duplicate_group) <= 1:
                continue
            
            # ä¿ç•™æœ€æ–°çš„æ–‡ä»¶
            files_with_mtime = []
            for file_path in duplicate_group:
                try:
                    mtime = os.path.getmtime(file_path)
                    files_with_mtime.append((file_path, mtime))
                except:
                    continue
            
            if len(files_with_mtime) <= 1:
                continue
            
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„
            files_with_mtime.sort(key=lambda x: x[1], reverse=True)
            keep_file = files_with_mtime[0][0]
            
            for file_path, _ in files_with_mtime[1:]:
                removed_files.append(file_path)
                if not dry_run:
                    os.remove(file_path)
                    print(f"   åˆ é™¤é‡å¤æ–‡ä»¶: {file_path}")
        
        return removed_files
    
    def generate_report(self, moved_files: List, removed_files: List):
        """ç”Ÿæˆæ•´ç†æŠ¥å‘Š"""
        report = {
            "timestamp": str(Path().cwd()),
            "moved_files": len(moved_files),
            "removed_duplicates": len(removed_files),
            "file_movements": moved_files,
            "removed_files": removed_files,
            "category_distribution": defaultdict(int)
        }
        
        # ç»Ÿè®¡åˆ†ç±»åˆ†å¸ƒ
        for move in moved_files:
            report["category_distribution"][move["category"]] += 1
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.base_dir / "organization_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“Š æ•´ç†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report
    
    def run_organization(self, dry_run: bool = True):
        """è¿è¡Œå®Œæ•´çš„æ•´ç†æµç¨‹"""
        print("ğŸš€ å¼€å§‹Paperç›®å½•æ•´ç†")
        print("="*50)
        
        # 1. åˆ›å»ºå¤‡ä»½
        if not dry_run:
            self.create_backup()
        
        # 2. åˆ†æå½“å‰ç»“æ„
        analysis = self.analyze_current_structure()
        print(f"ğŸ“Š å½“å‰ç»Ÿè®¡:")
        print(f"   æ€»æ–‡ä»¶æ•°: {analysis['total_files']}")
        print(f"   PDFæ–‡ä»¶: {analysis['pdf_files']}")
        print(f"   é‡å¤æ–‡ä»¶ç»„: {len(analysis['duplicate_files'])}")
        
        # 3. åˆ›å»ºç›®å½•ç»“æ„
        self.create_directory_structure()
        
        # 4. æ•´ç†æ–‡ä»¶
        moved_files = self.organize_files(dry_run=dry_run)
        
        # 5. ç§»é™¤é‡å¤æ–‡ä»¶
        removed_files = self.remove_duplicates(dry_run=dry_run)
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(moved_files, removed_files)
        
        print(f"\nâœ… æ•´ç†å®Œæˆ!")
        print(f"   ç§»åŠ¨æ–‡ä»¶: {len(moved_files)}")
        print(f"   åˆ é™¤é‡å¤: {len(removed_files)}")
        
        if dry_run:
            print(f"\nâš ï¸ è¿™æ˜¯é¢„è§ˆæ¨¡å¼ï¼Œæ²¡æœ‰å®é™…ç§»åŠ¨æ–‡ä»¶")
            print(f"   è¦æ‰§è¡Œå®é™…æ•´ç†ï¼Œè¯·è¿è¡Œ: organizer.run_organization(dry_run=False)")
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    paper_dir = "/Users/wrt/PycharmProjects/grid-retrieval-system/paper"
    organizer = PaperDirectoryOrganizer(paper_dir)
    
    # å…ˆè¿è¡Œé¢„è§ˆ
    print("ğŸ” é¢„è§ˆæ¨¡å¼ - æŸ¥çœ‹å°†è¦è¿›è¡Œçš„æ“ä½œ")
    report = organizer.run_organization(dry_run=True)
    
    # æ‰§è¡Œå®é™…æ•´ç†
    print(f"\nğŸš€ æ‰§è¡Œå®é™…æ•´ç†...")
    organizer.run_organization(dry_run=False)
    
    return report

if __name__ == "__main__":
    main()
