#!/usr/bin/env python3
"""
æå–è¯„åˆ†æœ€é«˜çš„è®ºæ–‡æ–‡æœ¬ï¼Œä¾¿äºæ‰‹å·¥é˜…è¯»åˆ†æ
"""
import os
import json
import PyPDF2
from datetime import datetime

def extract_pdf_text(pdf_path, max_pages=15):
    """æå–PDFæ–‡æœ¬ï¼Œä¿ç•™æ›´å¤šé¡µé¢ä»¥è·å¾—å®Œæ•´ä¿¡æ¯"""
    try:
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ""
            total_pages = len(reader.pages)
            pages_to_read = min(max_pages, total_pages)
            
            for page_num in range(pages_to_read):
                page_text = reader.pages[page_num].extract_text()
                text += f"\n--- Page {page_num + 1} ---\n"
                text += page_text + "\n"
            
            return text, total_pages
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–PDF {pdf_path}: {e}")
        return "", 0

def extract_top_papers():
    """æå–è¯„åˆ†æœ€é«˜çš„å‰10ç¯‡è®ºæ–‡çš„æ–‡æœ¬"""
    
    # åŠ è½½ä¸‹è½½è®°å½•
    with open('all_paper1_download_log_20250621_003032.json', 'r', encoding='utf-8') as f:
        papers = json.load(f)
    
    # æŒ‰è¯„åˆ†æ’åºï¼Œå–å‰10ç¯‡
    top_papers = sorted(papers, key=lambda x: x['score'], reverse=True)[:10]
    
    print(f"ğŸ“š å¼€å§‹æå–è¯„åˆ†æœ€é«˜çš„10ç¯‡è®ºæ–‡æ–‡æœ¬...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = 'top_papers_text'
    os.makedirs(output_dir, exist_ok=True)
    
    extracted_papers = []
    
    for i, paper in enumerate(top_papers, 1):
        print(f"\nğŸ“„ [{i}/10] æå–: {paper['title'][:50]}...")
        print(f"   è¯„åˆ†: {paper['score']:.4f}")
        print(f"   arXiv ID: {paper['arxiv_id']}")
        
        try:
            # æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            pdf_path = paper['filepath']
            if not os.path.exists(pdf_path):
                print(f"   âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
                continue
            
            # æå–æ–‡æœ¬
            text, total_pages = extract_pdf_text(pdf_path)
            
            if text:
                # ä¿å­˜æ–‡æœ¬åˆ°æ–‡ä»¶
                text_filename = f"{paper['arxiv_id']}_{paper['title'][:30].replace('/', '_').replace(':', '_')}.txt"
                text_path = os.path.join(output_dir, text_filename)
                
                with open(text_path, 'w', encoding='utf-8') as f:
                    f.write(f"Title: {paper['title']}\n")
                    f.write(f"arXiv ID: {paper['arxiv_id']}\n")
                    f.write(f"Score: {paper['score']:.4f}\n")
                    f.write(f"Total Pages: {total_pages}\n")
                    f.write(f"Extraction Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(text)
                
                extracted_papers.append({
                    'arxiv_id': paper['arxiv_id'],
                    'title': paper['title'],
                    'score': paper['score'],
                    'original_pdf': pdf_path,
                    'text_file': text_path,
                    'total_pages': total_pages
                })
                
                print(f"   âœ… æå–å®Œæˆ: {text_filename} ({total_pages} é¡µ)")
            else:
                print(f"   âŒ æ–‡æœ¬æå–å¤±è´¥")
                
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
    
    # ä¿å­˜æå–è®°å½•
    with open('extracted_papers_log.json', 'w', encoding='utf-8') as f:
        json.dump(extracted_papers, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… æ–‡æœ¬æå–å®Œæˆï¼æˆåŠŸæå– {len(extracted_papers)} ç¯‡è®ºæ–‡")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“„ æå–è®°å½•: extracted_papers_log.json")
    
    return extracted_papers

if __name__ == "__main__":
    extract_top_papers()
