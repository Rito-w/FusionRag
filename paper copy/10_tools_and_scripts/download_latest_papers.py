#!/usr/bin/env python3
"""
ä¸‹è½½æœ€æ–°çš„ç›¸å…³è®ºæ–‡è¿›è¡Œæ·±å…¥åˆ†æ
"""
import arxiv
import os
import requests
import json
from datetime import datetime

def download_paper(paper_info, download_dir):
    """ä¸‹è½½è®ºæ–‡PDF"""
    os.makedirs(download_dir, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶å
    title = paper_info['title']
    safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()
    safe_title = safe_title[:80]  # ç¼©çŸ­æ–‡ä»¶å
    
    # ä»entry_idæå–arxiv ID
    arxiv_id = paper_info['entry_id'].split('/')[-1]
    filename = f"{arxiv_id}_{safe_title}.pdf"
    filepath = os.path.join(download_dir, filename)
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
    if os.path.exists(filepath):
        print(f"  æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
        return filepath
    
    # ä¸‹è½½PDF
    pdf_url = paper_info['pdf_url']
    response = requests.get(pdf_url, stream=True)
    response.raise_for_status()
    
    with open(filepath, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    
    print(f"  âœ… ä¸‹è½½å®Œæˆ: {filename}")
    return filepath

def search_and_download(keywords, category_dir, max_results=3):
    """æœç´¢å¹¶ä¸‹è½½è®ºæ–‡"""
    client = arxiv.Client()
    search = arxiv.Search(
        query=keywords,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )
    
    papers = []
    downloaded_files = []
    
    print(f"\nğŸ” æœç´¢: {keywords}")
    
    for result in client.results(search):
        paper_info = {
            'title': result.title,
            'authors': [author.name for author in result.authors],
            'summary': result.summary,
            'published': str(result.published),
            'updated': str(result.updated),
            'entry_id': result.entry_id,
            'pdf_url': result.pdf_url,
            'categories': result.categories,
            'primary_category': result.primary_category
        }
        papers.append(paper_info)
        
        # ä¸‹è½½è®ºæ–‡
        try:
            filepath = download_paper(paper_info, category_dir)
            downloaded_files.append(filepath)
        except Exception as e:
            print(f"  âŒ ä¸‹è½½å¤±è´¥: {e}")
    
    return papers, downloaded_files

def main():
    # å®šä¹‰é‡ç‚¹æœç´¢æŸ¥è¯¢
    search_queries = [
        ("hybrid retrieval dense sparse", "01_hybrid_retrieval"),
        ("dynamic weight fusion retrieval", "01_hybrid_retrieval"), 
        ("multimodal retrieval attention", "02_multimodal_retrieval"),
        ("vector index update incremental", "03_vector_indexing"),
        ("query understanding expansion", "04_query_understanding"),
        ("retrieval reranking learning", "05_reranking")
    ]
    
    all_results = {}
    
    for query, category in search_queries:
        try:
            papers, files = search_and_download(query, category, max_results=2)
            all_results[query] = {
                'papers': papers,
                'downloaded_files': files,
                'category': category
            }
            print(f"âœ… {query}: ä¸‹è½½äº† {len(files)} ç¯‡è®ºæ–‡")
        except Exception as e:
            print(f"âŒ {query}: æœç´¢å¤±è´¥ - {e}")
            all_results[query] = {'papers': [], 'downloaded_files': [], 'category': category}
    
    # ä¿å­˜æœç´¢ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_file = f"06_recent_downloads/download_results_{timestamp}.json"
    
    os.makedirs("06_recent_downloads", exist_ok=True)
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ æœç´¢ç»“æœä¿å­˜è‡³: {result_file}")
    
    # ç»Ÿè®¡ä¸‹è½½æƒ…å†µ
    total_downloaded = sum(len(result['downloaded_files']) for result in all_results.values())
    print(f"\nğŸ“Š æ€»è®¡ä¸‹è½½äº† {total_downloaded} ç¯‡æœ€æ–°è®ºæ–‡")
    
    return result_file

if __name__ == "__main__":
    main()
