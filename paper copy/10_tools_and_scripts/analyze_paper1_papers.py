#!/usr/bin/env python3
"""
åˆ†æpaper1.jsonä¸‹è½½çš„è®ºæ–‡ï¼Œæå–å…³é”®ä¿¡æ¯
"""
import json
import PyPDF2
import re
from pathlib import Path

def extract_text_from_pdf(pdf_path):
    """ä»PDFä¸­æå–æ–‡æœ¬"""
    try:
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ""
            # åªè¯»å–å‰10é¡µï¼Œé¿å…å¤„ç†æ—¶é—´è¿‡é•¿
            for page_num in range(min(10, len(reader.pages))):
                text += reader.pages[page_num].extract_text() + "\n"
        return text
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–PDF {pdf_path}: {e}")
        return ""

def analyze_paper_content(text, paper_info):
    """åˆ†æè®ºæ–‡å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯"""
    analysis = {
        'arxiv_id': paper_info['arxiv_id'],
        'title': paper_info['title'],
        'score': paper_info['score'],
        'problem_statement': '',
        'innovation_points': [],
        'limitations': '',
        'experiments': '',
        'datasets': [],
        'methods': '',
        'contributions': []
    }
    
    # æå–æ‘˜è¦
    abstract_match = re.search(r'Abstract\s*\n(.*?)\n\s*(?:1\s+Introduction|Introduction)', text, re.DOTALL | re.IGNORECASE)
    if abstract_match:
        analysis['abstract'] = abstract_match.group(1).strip()[:500] + "..."
    else:
        analysis['abstract'] = paper_info.get('abstract', '')[:500] + "..."
    
    # æŸ¥æ‰¾é—®é¢˜é™ˆè¿°
    problem_patterns = [
        r'(?:problem|challenge|issue|limitation).*?(?:\.|;|\n)',
        r'(?:However|Nevertheless|Unfortunately).*?(?:\.|;|\n)',
        r'(?:struggle|suffer|fail).*?(?:\.|;|\n)'
    ]
    
    problems = []
    for pattern in problem_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        problems.extend(matches[:2])  # æœ€å¤šå–2ä¸ª
    
    analysis['problem_statement'] = ' '.join(problems[:3])
    
    # æŸ¥æ‰¾åˆ›æ–°ç‚¹
    innovation_keywords = [
        'novel', 'new', 'propose', 'introduce', 'first', 'innovative',
        'breakthrough', 'advance', 'improve', 'enhance', 'outperform'
    ]
    
    innovation_sentences = []
    sentences = re.split(r'[.!?]', text)
    for sentence in sentences[:100]:  # åªæ£€æŸ¥å‰100å¥
        if any(keyword in sentence.lower() for keyword in innovation_keywords):
            if len(sentence.strip()) > 20 and len(sentence.strip()) < 200:
                innovation_sentences.append(sentence.strip())
    
    analysis['innovation_points'] = innovation_sentences[:5]
    
    # æŸ¥æ‰¾æ•°æ®é›†
    dataset_patterns = [
        r'(?:dataset|benchmark|corpus).*?(?:[A-Z][A-Z0-9-]+)',
        r'(?:MS MARCO|TREC|Natural Questions|SQuAD|BEIR|HotpotQA|FiQA)',
        r'(?:Wikipedia|Common Crawl|OpenQA|WebQA)'
    ]
    
    datasets = set()
    for pattern in dataset_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for match in matches:
            if isinstance(match, str) and len(match) > 2:
                datasets.add(match.strip())
    
    analysis['datasets'] = list(datasets)[:10]
    
    # æŸ¥æ‰¾å®éªŒéƒ¨åˆ†
    exp_match = re.search(r'(?:Experiment|Evaluation|Results).*?(?:Conclusion|Discussion|Future)', text, re.DOTALL | re.IGNORECASE)
    if exp_match:
        analysis['experiments'] = exp_match.group(0)[:300] + "..."
    
    # æŸ¥æ‰¾æ–¹æ³•
    method_match = re.search(r'(?:Method|Approach|Framework|Architecture).*?(?:Experiment|Evaluation)', text, re.DOTALL | re.IGNORECASE)
    if method_match:
        analysis['methods'] = method_match.group(0)[:300] + "..."
    
    return analysis

def create_comprehensive_analysis():
    """åˆ›å»ºç»¼åˆåˆ†ææŠ¥å‘Š"""
    
    # åŠ è½½ä¸‹è½½è®°å½•
    with open('paper1_download_log.json', 'r', encoding='utf-8') as f:
        downloaded_papers = json.load(f)
    
    print("ğŸ” å¼€å§‹åˆ†æä¸‹è½½çš„è®ºæ–‡...")
    
    all_analyses = []
    
    for i, paper in enumerate(downloaded_papers, 1):
        print(f"\nğŸ“„ [{i}/{len(downloaded_papers)}] åˆ†æ: {paper['title'][:50]}...")
        
        # æå–PDFæ–‡æœ¬
        pdf_path = paper['filepath']
        text = extract_text_from_pdf(pdf_path)
        
        if text:
            # åˆ†æè®ºæ–‡å†…å®¹
            analysis = analyze_paper_content(text, paper)
            all_analyses.append(analysis)
            print(f"    âœ… åˆ†æå®Œæˆ")
        else:
            print(f"    âŒ æ— æ³•æå–æ–‡æœ¬")
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    report_content = generate_analysis_report(all_analyses)
    
    # ä¿å­˜æŠ¥å‘Š
    with open('paper1_comprehensive_analysis.md', 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"\nğŸ“Š ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: paper1_comprehensive_analysis.md")
    
    return all_analyses

def generate_analysis_report(analyses):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    
    report = "# Paper1.json è®ºæ–‡ç»¼åˆåˆ†ææŠ¥å‘Š\n\n"
    report += f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    report += f"åˆ†æè®ºæ–‡æ•°é‡: {len(analyses)}\n\n"
    
    report += "## ğŸ“Š è®ºæ–‡æ¦‚è§ˆ\n\n"
    report += "| æ’å | arXiv ID | æ ‡é¢˜ | è¯„åˆ† | ä¸»è¦åˆ›æ–°ç‚¹ |\n"
    report += "|------|----------|------|------|------------|\n"
    
    for i, analysis in enumerate(analyses, 1):
        title_short = analysis['title'][:40] + "..." if len(analysis['title']) > 40 else analysis['title']
        innovation_short = analysis['innovation_points'][0][:50] + "..." if analysis['innovation_points'] else "N/A"
        report += f"| {i} | {analysis['arxiv_id']} | {title_short} | {analysis['score']:.4f} | {innovation_short} |\n"
    
    report += "\n## ğŸ“‹ è¯¦ç»†åˆ†æ\n\n"
    
    for i, analysis in enumerate(analyses, 1):
        report += f"### {i}. {analysis['title']}\n\n"
        report += f"**arXiv ID**: {analysis['arxiv_id']} | **è¯„åˆ†**: {analysis['score']:.4f}\n\n"
        
        report += "#### ğŸ¯ è¦è§£å†³çš„é—®é¢˜\n"
        if analysis['problem_statement']:
            report += f"{analysis['problem_statement']}\n\n"
        else:
            report += "æœªæ˜ç¡®æå–åˆ°é—®é¢˜é™ˆè¿°\n\n"
        
        report += "#### ğŸ’¡ ä¸»è¦åˆ›æ–°ç‚¹\n"
        if analysis['innovation_points']:
            for j, innovation in enumerate(analysis['innovation_points'][:3], 1):
                report += f"{j}. {innovation}\n"
        else:
            report += "æœªæ˜ç¡®æå–åˆ°åˆ›æ–°ç‚¹\n"
        report += "\n"
        
        report += "#### ğŸ”¬ å®éªŒè®¾è®¡\n"
        if analysis['experiments']:
            report += f"{analysis['experiments']}\n\n"
        else:
            report += "æœªæå–åˆ°å®éªŒä¿¡æ¯\n\n"
        
        report += "#### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†\n"
        if analysis['datasets']:
            for dataset in analysis['datasets'][:5]:
                report += f"- {dataset}\n"
        else:
            report += "- æœªæ˜ç¡®æå–åˆ°æ•°æ®é›†ä¿¡æ¯\n"
        report += "\n"
        
        report += "#### ğŸ“ æ‘˜è¦\n"
        report += f"{analysis['abstract']}\n\n"
        
        report += "---\n\n"
    
    # æ·»åŠ æ€»ç»“åˆ†æ
    report += "## ğŸ¯ æ€»ä½“è¶‹åŠ¿åˆ†æ\n\n"
    
    # ç»Ÿè®¡å¸¸è§æ•°æ®é›†
    all_datasets = []
    for analysis in analyses:
        all_datasets.extend(analysis['datasets'])
    
    dataset_counts = {}
    for dataset in all_datasets:
        dataset_counts[dataset] = dataset_counts.get(dataset, 0) + 1
    
    report += "### ğŸ“Š çƒ­é—¨æ•°æ®é›†\n"
    sorted_datasets = sorted(dataset_counts.items(), key=lambda x: x[1], reverse=True)
    for dataset, count in sorted_datasets[:10]:
        report += f"- **{dataset}**: {count} ç¯‡è®ºæ–‡\n"
    
    report += "\n### ğŸ”¥ ä¸»è¦æŠ€æœ¯è¶‹åŠ¿\n"
    report += "1. **æ··åˆæ£€ç´¢æ–¹æ³•**: ç»“åˆç¨ å¯†å’Œç¨€ç–æ£€ç´¢çš„ä¼˜åŠ¿\n"
    report += "2. **åŠ¨æ€æƒé‡è°ƒæ•´**: æ ¹æ®æŸ¥è¯¢ç‰¹å¾åŠ¨æ€è°ƒæ•´æ£€ç´¢ç­–ç•¥\n"
    report += "3. **çŸ¥è¯†å›¾è°±é›†æˆ**: å°†ç»“æ„åŒ–çŸ¥è¯†ä¸å‘é‡æ£€ç´¢ç»“åˆ\n"
    report += "4. **è‡ªé€‚åº”RAG**: æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦é€‰æ‹©ä¸åŒçš„æ£€ç´¢ç­–ç•¥\n"
    report += "5. **å¤šè·³æ¨ç†**: æ”¯æŒå¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡\n\n"
    
    return report

if __name__ == "__main__":
    from datetime import datetime
    create_comprehensive_analysis()
