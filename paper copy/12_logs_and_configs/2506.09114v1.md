## TRACE : Grounding Time Series in Context for **Multimodal Embedding and Retrieval**

**Jialin Chen** **[1]** _[âˆ—]_ **, Ziyu Zhao** **[2]** _[âˆ—]_ **, Gaukhar Nurbek** **[3]** **, Aosong Feng** **[1]** **,**
**Ali Maatouk** [1], **Leandros Tassiulas** [1], **Yifeng Gao** [3], **Rex Ying** [1]

1 Yale University, 2 McGill University, 3 University of Texas Rio Grande Valley
`{jialin.chen, aosong.feng, ali.maatouk, leandros.tassiulas, rex.ying}@yale.edu`,
`ziyu.zhao2@mail.mcgill.ca` ; `{gaukhar.nurbek01, yifeng.gao}@utrgv.edu`

**Abstract**

The ubiquity of dynamic data in domains such as weather, healthcare, and energy
underscores a growing need for effective interpretation and retrieval of time-series
data. These data are inherently tied to domain-specific contexts, such as clinical
notes or weather narratives, making cross-modal retrieval essential not only for
downstream tasks but also for developing robust time-series foundation models by
retrieval-augmented generation (RAG). Despite the increasing demand, time-series
retrieval remains largely underexplored. Existing methods often lack semantic
grounding, struggle to align heterogeneous modalities, and have limited capacity
for handling multi-channel signals. To address this gap, we propose `TRACE`, a
generic multimodal retriever that grounds time-series embeddings in aligned textual context. `TRACE` enables fine-grained channel-level alignment and employs hard
negative mining to facilitate semantically meaningful retrieval. It supports flexible
cross-modal retrieval modes, including Text-to-Timeseries and Timeseries-to-Text,
effectively linking linguistic descriptions with complex temporal patterns. By
retrieving semantically relevant pairs, `TRACE` enriches downstream models with
informative context, leading to improved predictive accuracy and interpretability. Beyond a static retrieval engine, `TRACE` also serves as a powerful standalone
encoder, with lightweight task-specific tuning that refines context-aware representations while maintaining strong cross-modal alignment. These representations
achieve state-of-the-art performance on downstream forecasting and classification
tasks. Extensive experiments across multiple domains highlight its dual utility,
as both an effective encoder for downstream applications and a general-purpose
retriever to enhance time-series models.

**1** **Introduction**

Time-series data is prevalent across critical domains such as healthcare, weather, and energy [ 1 â€“ 4 ].
Crucially, such data rarely exists in isolation in real-world applications. It is typically accompanied
by rich, domain-specific textual context, _e.g.,_ clinical notes and weather reports [ 5 â€“ 7 ]. This inherent multimodality necessitates a shift beyond unimodal time-series analysis towards multi-modal
frameworks that seamlessly integrate these heterogeneous data types.

Cross-modal retrieval between time series and text is not only natural but necessary. As shown
in Figure 1, given a flash flood report describing extreme rainfall and high wind gusts, retrieving
historical time series that exhibit similar patterns can support downstream tasks such as weather
forecasting and disaster warning. Such retrieval also enables the integration of semantically aligned

_âˆ—_ Both authors contributed equally to this paper.

Preprint. Under review.

~~**Top-1**~~ ~~**Retrieved**~~ ~~**Time**~~ ~~**Series**~~ ~~**(Label:**~~ ~~**Flash**~~ ~~**Flood)**~~ Cross-modal Retriever

Figure 1: A Use Case of Text-to-Timeseries Retrieval

external knowledge into time series foundation models [ 8 â€“ 10 ], guiding model attention to relevant
segments, and facilitating more generalizable inference via retrieval-augmented generation (RAG).

Despite the clear demand, time-series retrieval, particularly in a cross-modal context, remains
significantly underexplored. Existing approaches often fall short in several ways [ 11 â€“ 15 ]. They
overlook the rich textual context within time-series data and rely on shallow similarity measures
rather than contextual understanding, leading to a lack of effective cross-modal alignment between
time-series signals and their associated textual descriptions. Moreover, they struggle with the multichannel nature of real-world time series, where each channel can encode distinct yet interrelated
information [ 16 â€“ 18 ]. Importantly, prior work rarely explores retrieval-augmented generation (RAG)
for time series foundation models, restricting their utility in augmenting downstream models.

To address this gap, we introduce `TRACE`, a novel multimodal **T** ime-series **R** etriever with **A** ligned
**C** ontext **E** mbedding. As illustrated in Figure 2, `TRACE` adopts a two-stage training: a pre-training
stage for the time-series encoder, followed by a cross-modal alignment. To address the challenge
of modeling multivariate time series, we introduce Channel Identity Tokens (CITs) into a masked
autoencoder framework pre-trained at both the token level and channel level in Stage 1. CITs guide
the model to attend to unique channel behaviors and enable the learning of channel disentangled
representations, overcoming the limitation of conventional decoder-only foundation models which
often yield embeddings lacking discriminative power for retrieval and classification. In Stage 2, we
propose a novel component for effective cross-modal alignment between time-series embeddings
and their textual counterparts through a hierarchical hard negative mining strategy. At the channel
level, we identify distractor single-channel segments that exhibit misleadingly similar patterns. At
the sample level, we dynamically mine hard negatives by selecting highly similar text descriptions
but with divergent semantics. This dual-level contrastive learning encourages the model to learn both
local precision and global consistency, leading to strong generalization in downstream tasks.





Stage 1: Pre-training Stage 2: Alignment




token-level


channel-level sample-level








|CIT CIT CIT<br>Temperature ranges â€¦Precipitation is â€¦ Humidity fluctuates â€¦|Col2|MMTS<br>Dataset|
|---|---|---|
|CIT CIT CIT<br>Temperature ranges â€¦Precipitation is â€¦ Humidity fluctuates â€¦|idity fluctuates â€¦|idity fluctuates â€¦|


(1) TRACE Two-stage Training (2) TRACE Two-fold Utility

Figure 2: Overview of `TRACE` . CIT stands for Channel Identity Tokens, which serve as a key bridge
to connect two stages. MMTS denotes multimodal time series.

`TRACE` is designed with a two-fold utility. It acts as a general-purpose retriever, which provides
relevant information via a soft token interface. The soft token summarizes retrieved time-series
snippets into a latent vector, which is then prepended as a conditioning token, guiding a frozen
time-series foundation model towards more context-aware predictions. Moreover, `TRACE` serves as a
powerful standalone encoder, producing rich embeddings that achieve state-of-the-art performance on
downstream forecasting and classification tasks. Extensive experiments on both public benchmarks
and our curated multimodal dataset validate the effectiveness of `TRACE`, demonstrating superior
retrieval accuracy. The retrieved context substantially boosts downstream time-series models in
retrieval-augmented settings, with up to 4.56% increase in classification accuracy and 4.55% reduction
in forecasting error. In addition, `TRACE` produces high-quality time-series embeddings that achieve
state-of-the-art results on a wide range of forecasting and classification benchmarks.

2

The contributions of this paper are: (1) we propose the first multimodal retriever, `TRACE`, that
learns semantically grounded time-series embeddings through fine-grained dual-level alignment;
(2) we establish new benchmarks on cross-modal retrieval between time series and text, and (3)
extensive validation showcases that `TRACE` consistently delivers state-of-the-art performance both as
a general-purpose retriever for time-series models and a powerful encoder for time series analysis.

**2** **Related Work**

**Time Series Forecasting** . Recent work on time-series forecasting has led to a range of model
architectures, each emphasizing different inductive biases. Transformer-based models leverage selfattention to capture long-range dependencies and flexible temporal dynamics [ 19 â€“ 28 ]. Linear-based
models assume time-series signals can be effectively decomposed and modeled with simple linear
projections [ 29, 30 ]. Frequency-domain and mixing-based approaches aim to model periodicity and
multi-scale temporal structures using Fourier transforms or token mixers [ 31 ]. Recently, a variety
of time series foundation models have emerged. Timer-XL [ 9 ] leverages Kronecker attention and
is pre-trained with multivariate next-token prediction to enable unified, long-context forecasting.
Chronos [ 32 ] tokenizes time series via scaling and quantization, and trains a T5-style model for zeroshot probabilistic forecasting. Time-MoE [ 10 ] introduces a sparse mixture-of-experts architecture to
support variable horizons and input lengths. TimesFM [ 33 ] uses input patching and is pre-trained
on large-scale data for strong zero-shot performance. Moment [ 8 ] and Moirai [ 34 ] adopt masked
prediction pretraining to enable generalization across diverse multivariate forecasting tasks. While
these models perform well on forecasting tasks, they are generally unimodal and not designed for
retrieval or integration of external context, highlighting a gap addressed by our cross-modal retrieval
framework.

**Time Series Language Models** . Recently, several multimodal encoders have been proposed to
integrate time series and text [ 35 â€“ 40 ], which aim to leverage the generalization capabilities of
large language models by reprogramming time series into token-like representations or textual
prototypes. ChatTime[ 41 ] models time series as a foreign language by normalizing and discretizing
continuous signals into token sequences, which are then processed by a large language model (LLM).
ChatTS[ 42 ] supports both understanding and reasoning by fine-tuning on synthetic datasets generated
via attribute-based sampling. TimeXL[ 43 ] combines a prototype-based time series encoder with
a multimodal prediction framework to capture explainable temporal patterns guided by aligned
textual cues. However, they primarily treat text as global context and lack fine-grained alignment
between structured time series components and textual semantics, leading to suboptimal cross-modal
embedding or retrieval.

**Time Series Retrieval System** . Recent work has explored retrieval systems for time series data,
primarily within a unimodal setting [ 44, 13, 45, 15 ]. CTSR [ 11 ] supports content-based time-series
retrieval using contextual metadata. TimeRAF [ 12 ] integrates a trainable retriever with task-specific
time-series knowledge bases for downstream augmentation. TS-RAG [ 14 ] retrieves relevant time
series segments using pre-trained encoders and combines them via a mixture-of-experts module to
improve forecasting. However, all of these methods rely solely on time series embeddings and do not
incorporate textual signals, limiting their ability to support multimodal and context-aware retrieval.

**3** **Proposed Method**

As shown in Figure 3, `TRACE` learns robust time series representations through a masked reconstruction
objective with channel-biased attention in the pre-training stage (Sec. 3.2). Then, each time series
channel is aligned with its corresponding textual description via fine-grained contrastive learning
in the cross-modal alignment stage (Sec. 3.3). We further propose a novel retrieval-augmented
generation strategy for time series foundation models, where `TRACE` retrieves relevant context for
downstream tasks (Sec. 3.4). This modular design enables both strong standalone performance and
effective integration with existing time series foundation models.

**3.1** **Problem Definition**

**Multimodal Time-series** . Let **X** _âˆˆ_ R _[C][Ã—][T]_ denote a multivariate time series instance, where _C_ is the
number of channels (or variables) and _T_ is the number of time steps. We assume the availability of

3

|CLS|Col2|
|---|---|
|CLS||
|CIT<br>1|CIT<br>1|
|CIT<br>2|CIT<br>2|
|CIT<br>3|CIT<br>3|

|Col1|ğ³<br>!"#|
|---|---|
||ğ³<br>%|
||ğ³<br>&|
||ğ³<br>'|








|Reconstruction Forecasting Classification ğ³!"# $|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Event Report: A strong storm<br>system slowly movedâ€¦ The low-<br>level wind shear had become<br>strong near the warm front itself,<br>which supported the development<br>of a few tornadoesâ€¦.<br>Temperature ranges from a high of<br>21.42Â°C to a low of -9.24Â°C â€¦<br>Precipitation is generally low, with<br>several days of no rain, but â€¦<br>Humidity fluctuates significantly,<br>averaging around 75% â€¦|
|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|CIT<br>1|CIT<br>1|
|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|Head Head Head CLS ğ³ !"# Projection Encoder Sentence<br>C CC I II T TT 2 31 ğ³ ğ³ğ³ % & Linear<br>Encoder-only Transformer '<br>Channel-biased Attention Cross Attention<br>RoPE CIT 1 ğ³ % a rett pra ec lt<br>CIT 2 ğ³ & Patch Token<br>CLSCIT1 CIT2 CIT3 CIT 3 ğ³ ' Mask Token|CIT<br>2|CIT<br>2|
|CLS|CIT1||||CIT2||||CIT3||||CIT<br>3|CIT<br>3|


Figure 3: Illustration of `TRACE`, which encodes multivariate time series using channel-biased attention
and aligns token embeddings with its corresponding textual description ( _e.g.,_ **z** _i_ and **z** cxt ) through
cross-attention and dual-level contrastive learning. **z** _[â€²]_ cxt [indicates an in-batch hard negative sample.]

two types of textual information aligned with **X** . First, for each channel _c_ in an instance **X**, there
is a corresponding textual description _Ï„_ _c_ that summarizes the behavior or trend of **X** _c_ over the time
window [0 _, T_ ) . These descriptions are denoted as _T_ [ch] = _{Ï„_ _c_ _|c_ = 1 _, Â· Â· Â·, C}_ . Additionally, there is a
sample-level context _Ï„_ cxt summarizing the overall condition occurring during the same time window,
which could be weather reports or clinical narratives, depending on the application domain.

**Task Objectives** . The goal is to jointly embed the multivariate time series **X** and its corresponding
textual context _T_ = _T_ [ch] _âˆª{Ï„_ cxt _}_ into a shared space that supports multiple downstream tasks,
including: (1) forecasting future values **X** _T_ : _T_ + _H_ _âˆˆ_ R _[C][Ã—][H]_ for the next _H_ time steps; (2) classification,
where the model predicts a categorical label for each time series instance; and (3) cross-modal retrieval,
where the goal is to retrieve relevant time series **X** based on a text query _Ï„_ cxt or retrieve historical
relevant reports from _T_ given a time series query, etc.

**3.2** **Stage 1: Time Series Encoder Pre-training**

**Time Series Tokenization** . Given an input multivariate time series **X** _âˆˆ_ R _[C][Ã—][T]_, we divide the
temporal dimension into non-overlapping (or strided) patches of length _P_, resulting in _T_ [Ë†] = _âŒŠ_ _P_ _[T]_ _[âŒ‹]_

patches per channel. Each patch is flattened and linearly projected into a _d_ -dimensional embedding
space using a learnable linear projection. This converts each channel into a sequence of patch tokens
_X_ _c_ [patch] _âˆˆ_ R _[T]_ [Ë†] _[ Ã—][d]_, for _âˆ€c âˆˆ{_ 1 _, . . ., C}_ . To capture localized semantics within each channel, we
prepend a learnable channel identity token [ `CIT` ] _âˆˆ_ R [1] _[Ã—][d]_ to the patch token sequence of each channel.
These tokens serve as explicit representations of channel-level summaries. Each token is uniquely
indexed and not shared across channels, initialized from a standard Gaussian distribution, and trained
jointly with the model. This design allows the model to differentiate between channels and effectively
aggregate channel-wise patterns. We then concatenate all tokenized channels into a single sequence
and insert a global learnable `[CLS]` token at the beginning of the full sequence. The final token
sequence for a multivariate instance is structured as:

**H** = `[CLS]` ; `[CIT]` 1 ; _X_ 1 [patch] ; `[CIT]` 2 ; _X_ 2 [patch] ; _. . ._ ; `[CIT]` _C_ ; _X_ _C_ [patch] _âˆˆ_ R _[L][Ã—][d]_ _,_ (1)
ï¿½ ï¿½

where _L_ = _C_ ( _T_ [Ë†] + 1) + 1 is the total sequence length after flattening all channel in 1. This
tokenization strategy preserves both temporal and structural granularity: patchification encodes
token-level patterns; `[CIT]` summarizes intra-channel dynamics; and `[CLS]` provides a global and
sample-level embedding that can be used for downstream retrieval and classification tasks.

**Channel-biased Attention and Rotary PE** . To encode channel dependencies in multivariate time
series, we introduce a novel Channel-biased Attention ( `CbA` ) mechanism that incorporates both inductive bias for channel disentanglement and temporal order encoding via rotary positional embeddings
(RoPE) [ 46 ]. In our `CbA`, we design a biased attention mask _M âˆˆ{_ 0 _,_ 1 _}_ _[L][Ã—][L]_ to prevent unintended
semantic entanglement across heterogeneous variables. Specifically, for each channel identity token

`[CIT]` _c_ located at index _i_ _c_ in the flattened sequence, we define _M_ _i_ _c_ _,j_ = 0 if token _j /âˆˆ_ channel
_c_ and 1 otherwise, and _M_ _k,j_ = 1 if token _k_ is not a `[CIT]` . Let **Q** _,_ **K** _,_ **V** _âˆˆ_ R _[L][Ã—][d]_ be the learned
linear projections of the input token embedding **H** . We apply RoPE to the query ( **Q** ) and key ( **K** )
vectors before computing attention. RoPE is applied independently within each channel to the _T_ [Ë†]
temporal tokens, and is not applied to the channel identity tokens, which act as position-agnostic

4

aggregators. The attention weight between tokens _i_ and _j_ in a RoPE-enhanced attention is given by
_Î±_ _ij_ = softmax _j_ ï¿½ _Q_ _[âŠ¤]_ _i_ _[R]_ _[Î¸]_ âˆ† _tij_ _[K]_ _[j]_ _[/]_ _âˆšd_ + log _M_ _ij_ ï¿½, where _R_ _Î¸_ âˆ† _tij_ ( _Â·_ ) denotes a rotation by angle _Î¸_ âˆ† _t_ _ij_,

and âˆ† _t_ _ij_ is the relative time difference between tokens _i_ and _j_ in their original unflattened sequence.
This is crucial in the multichannel setting, as two tokens that are close in actual time may appear far
apart in the flattened sequence. Using âˆ† _t_ _ij_ ensures that the position encoding remains consistent
with the true temporal structure rather than the flattened channel order. _M_ _ij_ mask enforces channel
disentanglement, while still allowing rich token-level interactions across the full sequence.

**Pre-training Setup** . We adopt an encoder-only Transformer [ 47 ] with multi-head channel-based
attention layers in `TRACE` . We apply reversible instance normalization [ 48 ] to multivariate time series
before tokenizing and embedding. A fixed proportion of these tokens is randomly masked with a
mask ratio of _Î³_, and the model is pre-trained to reconstruct the missing values based on the unmasked
context. We use mean squared error (MSE) loss to supervise pre-training, encouraging the model to
capture cross-channel dependencies while learning transferable representations for downstream tasks.

**3.3** **Stage 2: Multimodal Alignment Learning**

**Motivation** . Standard contrastive learning methods typically rely on sample-level random negatives.
However, textual descriptions frequently reference specific variables ( _e.g.,_ temperature spikes, wind
gusts), which cannot be precisely aligned using a single global embedding. To address this, we
introduce channel-level alignment that explicitly models the interaction between individual timeseries channels and their corresponding textual context. This not only enhances semantic precision
but also promotes modularity in representation learning and enables variable-specific interactions.

**Cross-attention Between Modalities** . After pre-training the time-series encoder via masked reconstruction, we obtain hidden embedding **H** [out] _âˆˆ_ R _[L][Ã—][d]_ from the final transformer layer, where _L_ is the
full sequence length after flattening all channels. From this, we extract the `[CLS]` token embedding
**h** `[CLS]` _âˆˆ_ R _[d]_, and the set of channel identity token embeddings **H** `[CIT]` = [ **h** 1 _, . . .,_ **h** _C_ ] _âˆˆ_ R _[C][Ã—][d]_, each
corresponding to a `[CIT]` token and serving as fine-grained anchors that enable structured reasoning
at the channel level. Let _Ï„_ cxt and _Ï„_ _c_ denote the sample-level and the _c_ -th channel textual context for a
time series instance, respectively. The textual inputs are first encoded using a pre-trained language
model ( _e.g.,_ a frozen Sentence-Transformer [ 49 ]), followed by a learnable linear layer that projects
them into the same _d_ -dimensional embedding space as the time series representations, collectively
denoted as _f_ t ( _Â·_ ) . This yields semantic embeddings **z** cxt = _f_ t ( _Ï„_ cxt ) _âˆˆ_ R _[d]_ for the sample-level context
and **z** _c_ = _f_ t ( _Ï„_ _c_ ) _âˆˆ_ R _[d]_ for each channel-level description. We further apply a cross-attention between
**H** `[CIT]` _âˆˆ_ R _[C][Ã—][d]_ and channel text embeddings **Z** ch = [ **z** 1 _, . . .,_ **z** _C_ ] _âˆˆ_ R _[C][Ã—][d]_, allowing information
to be fused across aligned channels. This interaction allows the model to refine its channel-wise
time-series representations using semantically aligned textual information.

**Dual-level Hard Negative Mining** . To enhance the discriminative capacity of the model, we develop
a dual-level hard negative mining strategy that introduces fine-grained contrastive pressure at both the
sample and channel levels. This approach enables the model to distinguish not only between unrelated
time series and text, but also between subtly confusable pairs that share superficial temporal similarity
but diverge semantically. For each time series instance _i_, we mine negative candidates from all other
sample-level reports in the same batch based on embedding cosine similarity. For a certain channel,
we mine channel-level negatives from a broader candidate pool that includes both intra-instance
distractors (other channels within the same sample) and inter-instance distractors (same-indexed
channels across different samples). Specifically, for the _c_ -th channel of the _i_ -th instance, we define
the sample-level and channel-level negative candidate set as
_N_ cxt [(] _[i]_ [)] [= Top] _K_ ï¿½sim( **h** [(] `[CLS]` _[i]_ [)] _[,]_ **[ z]** cxt [(] _[j]_ [)] [)] _[ |][ j][ Ì¸]_ [=] _[ i]_ ï¿½ _, N_ ch [(] _[i,c]_ [)] = Top _K_ ï¿½sim( **h** [(] _c_ _[i]_ [)] _[,]_ **[ z]** [(] _c_ _[j]_ _[â€²]_ [ )] [)] _[ |][ c]_ _[â€²]_ _[ Ì¸]_ [=] _[ c]_ [ or] _[ j][ Ì¸]_ [=] _[ i]_ ï¿½ _,_

where _K_ is number of negative samples at each level. Symmetric negative sets are defined in the
reverse direction for **z** cxt [(] _[i]_ [)] [and] **[ z]** c [(] _[i]_ [)] by swapping the roles of time series and text. We then compute a
bidirectional InfoNCE loss at sample levels: _L_ global [text] _[â†’]_ [ts] [,] _[ L]_ [ts] global _[â†’]_ [text] [, and similarly for channel-level losses.]
The total alignment objective is the average of both directions (Formulations detailed in Appendix C):

_L_ align = 2 [1] ï¿½ _L_ [text] global _[ â†’]_ [ts] + _L_ [ts] global _[ â†’]_ [text] ï¿½ + _Î»_ ch _Â·_ 2 [1] ï¿½ _L_ [text] channel _[ â†’]_ [ts] + _L_ [ts] channel _[ â†’]_ [text] ï¿½ _,_ (2)

where _Î»_ ch controls the contribution of channel-level alignment. The entire alignment objective is
optimized jointly with the trainable parameters of the time series encoder in the pre-training stage
and the linear projection head in _f_ t, while keeping the backbone language model frozen.


_L_ align = [1]


2 [1] ï¿½ _L_ [text] global _[ â†’]_ [ts] + _L_ [ts] global _[ â†’]_ [text] ï¿½ + _Î»_ ch _Â·_ 2 [1]


5

**3.4** **Retrieval-augmented Generation with Time Series Foundation Models**

As shown in Figure 2, `TRACE` enables retrieval-augmented generation (RAG) for time series foundation
models, inspired by the success of RAG in NLP [ 50, 13 ]. Given a query time series, `TRACE` retrieves
semantically relevant time-seriesâ€“text pairs from a large multimodal corpus based on the embedding
similarity. The retrieved context is then encoded into a soft token, which is a trainable, dense vector
that serves as a continuous prompt to condition the downstream forecasting model. This design allows
the forecaster to incorporate external knowledge without architectural modification. Importantly, the
base time-series foundation model remains frozen during training, as the soft tokens are differentiable
and model-agnostic, improving efficiency and enabling plug-and-play integration across diverse
backbone architectures. In effect, `TRACE` acts as a structured, external memory, enriching the modelâ€™s
input with historically grounded and semantically aligned context.

**4** **Experiments**

We evaluate `TRACE` from three key perspectives: (1) its effectiveness in cross-modal retrieval compared to strong time series encoders (Sec. 4.2), (2) its utility as a retriever in retrieval-augmented
forecasting pipelines (Sec 4.3), and (3) its generalization ability as a standalone encoder for forecasting and classification (Sec. 4.4). Experiments are conducted on public benchmarks and our curated
multimodal dataset designed to assess cross-modal alignment and retrieval performance.

**4.1** **Experimental Setting**

**Dataset** . To support real-world multimodal time series applications, we construct a new dataset in the
weather domain with three aligned components: multivariate time series, sample-level event reports,
and synthetic channel-level descriptions, specifically for downstream forecasting and event-type
classification tasks. The event reports are sourced from the NOAA Events Database [ 51 ], while the
associated time series data are retrieved from the NOAA Global Historical Climatology Network
(GHCN) [ 52 ]. We focus on stations and time windows characterized by frequent severe weather events
and extract historical multivariate time-series segments at multiple temporal resolutions, anchored at
event onset. To enhance data diversity and model robustness, we also sample non-event ( _i.e.,_ typical)
periods from the same stations, as well as from geographically distinct locations. Each time-series
segment includes seven variables ( _e.g.,_ temperature, relative humidity, precipitation) and is annotated
with either a specific event type or a non-event label. To evaluate performance in the univariate
setting, we further incorporate the three largest subsetsâ€”Health, Energy, and Environmentâ€”from
TimeMMD [ 5 ], a multimodal benchmark designed for time series forecasting, where each singlevariate instance is aligned with a sample-level textual report ( _e.g.,_ clinical notes, incident logs). This
setting allows us to assess the modelâ€™s generalization across diverse domains and varying channel
configurations. Full dataset details and illustrative examples are provided in Appendix B.

**Baselines** . We evaluate against the state-of-the-art traditional time series models and recent time series
foundation models. Traditional baselines include DLinear [ 29 ], iTransformer [ 24 ], PatchTST [ 22 ],
TimesNet [ 53 ], TimeMixer [ 54 ], and multimodal model FSCA [ 37 ]. These models are trained from
scratch on each task. For foundation models, we include Chronos[ 32 ], TimesFM [ 55 ], Timer-XL [ 9 ],
Time-MoE [10], Moirai [34] and Moment [8]. We refer to Appendix D.1 for baseline details.

**Implementation Details** . The default `TRACE` consists of a 6-layer Transformer encoder with a hidden
dimension of 384 and 6 attention heads. We use the AdamW [ 56 ] optimizer with a linear warmup
followed by a cosine decay schedule. Pre-training is conducted with a mask ratio of 0 _._ 3, and runs for
up to 400 epochs. We take 32 in-batch negative samples at each level in the alignment stage and run
for up to 300 epochs. All experiments are conducted over five runs with different random seeds on
NVIDIA A100 40GB GPUs. We refer to Appendix D.2 for experiment configurations and details.

**4.2** **Cross-modal Retrieval**

**Alignment Setup** . To evaluate the modelâ€™s retrieval performance, we conduct a controlled comparison
by replacing the encoder in `TRACE` with several strong time series foundation models that produce
fixed-length embeddings. Each encoder is jointly fine-tuned end-to-end with a lightweight projection
layer following the sentence encoder, using a contrastive learning objective. While `TRACE` leverages

6

`[CLS]` and `[CIT]` embeddings for dual-level alignment, other baselines use mean pooling over the
sequence due to their architectural constraints.

**Evaluation Metrics** . `TRACE` supports flexible retrieval modes, including cross-modal (Text-to-TS
and TS-to-Text) and unimodal TS-to-TS retrieval. We provide results of TS-to-TS retrieval in
Appendix D.8. For cross-modal retrieval, a query in one modality is used to retrieve its corresponding
counterpart in the other modality based on embedding cosine similarity. The evaluation includes
several metrics:

- **Label Matching** uses P@ _k_ to measure the precision of correctly labeled items among the top- _k_
retrieval, and Mean Reciprocal Rank (MRR) to assess the rank of the first correct item.

- **Modality Matching** evaluates whether a query retrieves its paired instance from the opposite
modality, using P@ _k_ for top- _k_ precision and MRR for the rank of the true counterpart.

- **Text Similarity** uses ROUGE between the query text and the text paired with the top-1 retrieved
time series (for text-to-ts scenario), or between the top-1 retrieved text and the original text paired
with the query time series (for ts-to-text scenario).

- **Time Series Similarity** computes MAE and MSE between the time series linked to the query and
that of the top-1 retrieved pair, defined similarly to Text Similarity.

**Results** . As shown in Table 1, `TRACE` consistently achieves state-of-the-art performance in two
retrieval settings with approximately 90% top-1 label matching and 44% top-1 modality matching.
Notably, this retrieval precision surpasses the classification accuracy of all train-from-scratch models
reported in Table 3, highlighting the strength of alignment supervision in learning discriminative
representations. Among baselines, Moment outperforms other foundation models, suggesting that
encoder-only architectures are better suited for dense retrieval tasks. In contrast, `TRACE` provides
fine-grained embeddings for cross-modal alignment, enabling it to recover semantical counterparts
with high precision. `TRACE` supports flexible retrieval modes, such as TS-to-TS.

Table 1: Retrieval results on 2,000 bidirectional Textâ€“Timeseries query pairs. â€œRandomâ€ indicates a
non-informative retriever that ranks candidates uniformly at random.

|Retriever|Label Matching<br>P@1 (â†‘) P@5 (â†‘) MRR (â†‘)|Modality Matching<br>P@1 (â†‘) P@5 (â†‘) MRR (â†‘)|Text<br>ROUGE (â†‘)|Time Series<br>MAE (â†“) MSE (â†“)|
|---|---|---|---|---|
|Random|42.61 47.50 0.583|0.00 0.00 0.00|0.416|0.874 1.653|
|w/ Time-MoE TS-to-Text<br>w/ Timer-XL<br>w/ TS2Vec<br>w/ Moment<br>TRACE|46.46 43.98 0.612<br>36.34 38.16 0.543<br>50.47 48.72 0.651<br>55.73 53.18 0.691<br>90.08 77.60 0.940|1.79 5.93 0.052<br>4.29 12.61 0.090<br>4.37 14.57 0.112<br>7.78 21.68 0.154<br>44.10 70.24 0.560|0.482<br>0.482<br>0.503<br>0.515<br>0.717|0.837 1.607<br>0.793 1.493<br>0.784 1.462<br>0.747 1.415<br>0.403 0.771|
|w/ Time-MoE Text-to-TS<br>w/ Timer-XL<br>w/ TS2Vec<br>w/ Moment<br>TRACE|57.08 52.22 0.656<br>63.91 58.71 0.731<br>60.28 56.41 0.706<br>64.67 59.53 0.740<br>89.63 78.39 0.938|0.75 2.89 0.031<br>2.94 9.47 0.073<br>7.42 23.70 0.184<br>5.83 18.15 0.133<br>43.72 69.84 0.557|0.460<br>0.463<br>0.471<br>0.488<br>0.713|0.857 1.578<br>0.821 1.568<br>0.806 1.490<br>0.778 1.467<br>0.411 0.793|



**4.3** **Retrieval-augmented Time Series Forecasting**


Table 2: Forecasting performance on Weather dataset
for next 24 steps under different retrieval-augmented
generation settings.


**Setup.** We use `TRACE` to retrieve the Table 2: Forecasting performance on Weather dataset
most relevant timeseriesâ€“text pairs from for next 24 steps under different retrieval-augmented
the curated corpus based on time-series em- generation settings.

raw time series, denoted as _h_ ts ; for _TS+Text_,

|Setting|Timer-XL Time-MoE|Moment TRACE|
|---|---|---|
|Setting|MAE MSE MAE MSE|MAE MSE MAE MSE|
|w/o RAG<br>w/ TS-only<br>w/ TS+Text|0.729 1.055 0.635 0.903<br>0.720 1.009 0.621 0.801<br>0.712 0.984 0.611 0.787|0.645 0.816 0.576 0.718<br>0.628 0.797 0.556 0.698<br>0.631 0.801 0.555 0.696|

we concatenate _h_ ts and semantic embedding **z** cxt from the retrieved text to form the prompt. This
soft prompt is then prepended to the query for the downstream forecasting layer, without fine-tuning
the pre-trained model weights. We test two architecture families: (1) decoder-only models, including
Timer-XL and Time-MoE, where the prompt is prepended at every autoregressive generation step,
and (2) encoder-only models, Moment and `TRACE`, where the prompt is prepended to the encoderâ€™s
hidden states and followed by a trainable forecasting head. In all settings, only the linear projection
layers for prompt generation and the forecasting head (for encoder-only) are trained.




7

Table 4: Forecasting results (MAE and MSE) of full-shot models and time series foundation models
on multi-variate ( **M** ) and univariate ( **U** ) datasets. **Red** : the best, Blue: the 2nd best.




|Model|Weather (M)|Health (U)|Energy (U)|Environment (U)|#<br>1st|
|---|---|---|---|---|---|
|Model|H = 7 H = 24<br>MAE MSE MAE MSE|H = 12 H = 48<br>MAE MSE MAE MSE|H = 12 H = 48<br>MAE MSE MAE MSE|H = 48 H = 336<br>MAE MSE MAE MSE|H = 48 H = 336<br>MAE MSE MAE MSE|
|Chronos<br>Time-MoE Zero-shot<br>TimesFM<br>Timer-XL<br>Moirai<br>Moment|0.560 0.937 0.646 1.094<br>0.579 0.803 0.635 0.903<br>0.550 0.859 0.640 1.034<br>0.645 0.912 0.729 1.055<br>0.593 1.001 0.675 1.135<br>0.572 0.732 0.645 0.816|0.650 1.106 0.987 2.019<br>0.604 0.981 0.832 1.697<br>0.610 0.913 0.865 1.685<br>0.741 1.235 0.988 1.892<br>0.976 3.029 1.569 8.125<br>0.988 1.824 0.997 1.902|0.263 0.148 0.554 0.553<br>0.205 0.089 0.451 0.396<br>0.248 0.137 0.499 0.482<br>0.236 0.118 0.460 0.424<br>0.318 0.273 0.692 1.415<br>0.471 0.411 0.542 0.542|0.536 0.612 0.583 0.671<br>0.562 0.508 0.836 0.969<br>0.503 0.532 0.531 0.569<br>0.549 0.564 0.565 0.574<br>0.935 12.428 2.237 25.011<br>0.449 0.375 0.554 0.502|0<br>2<br>0<br>0<br>0<br>2|
|DLinear<br>iTransformer<br>PatchTST Full-shot<br>TimesNet<br>TimeMixer<br>FSCA<br>TRACE|0.593 0.778 0.691 0.884<br>0.518 0.707 0.591 0.814<br>0.529 0.723 0.599 0.826<br>0.497 0.654 0.581 0.786<br>0.501 0.667 0.585 0.787<br>0.496 0.642 0.780 0.762<br>0.501 0.623 0.576 0.718|1.178 2.421 1.132 2.256<br>0.676 1.072 0.911 1.747<br>0.656 1.034 0.902 1.708<br>0.820 1.376 0.969 1.903<br>1.091 2.215 1.126 2.250<br>0.756 1.240 0.969 1.904<br>0.547 0.768 0.827 1.435|0.410 0.273 0.546 0.512<br>0.267 0.124 0.487 0.399<br>0.263 0.121 0.489 0.407<br>0.270 0.127 0.496 0.398<br>0.376 0.246 0.538 0.491<br>0.278 0.136 0.520 0.466<br>0.230 0.113 0.448 0.389|0.561 0.515 0.581 0.534<br>0.486 0.425 0.511 0.458<br>0.493 0.462 0.525 0.511<br>0.520 0.486 0.489 0.430<br>0.558 0.553 0.559 0.568<br>0.497 0.462 0.511 0.496<br>0.455 0.403 0.475 0.413|0<br>0<br>0<br>0<br>0<br>1<br>11|


**Results.** Table 2 presents the forecasting results across decoder-only and encoder-only models under
different RAG settings, augmented by top- _R_ retrieved instances. We refer to Figure 4 (d) for ablation
on _R_ . The results reveal that retrieval augmentation consistently improves forecasting performance
across all models, and the _TS+Text_ setting leads to the most significant gains for decoder-only models
like Timer-XL and Time-MoE. Notably, `TRACE` shows marginal improvement when moving from
_TS-only_ to _TS+Text_ retrieval, which can be attributed to that its multimodal embedding space is already
aligned with textual descriptions. This alignment reduces the dependency on additional textual signals
and justifies `TRACE` â€™s design as a lightweight, general-purpose retriever for RAG pipelines. Moreover,
these results indicate decoder-only models are more sensitive to the richness of retrieved modalities,
whereas encoder-only models exhibit more stable and better capacity for internalizing and utilizing
structured representations. While our RAG design adopts a simple embedding concatenation strategy,
it primarily validates the general utility of retrieved content across different model families. We leave
optimizing augmentation architectures for future work.

**4.4** **Standalone Time Series Encoder**


**Setup.** To evaluate `TRACE` as a standalone encoder, we conduct
experiments on forecasting and classification tasks. We compare
`TRACE` against full-shot models all trained from scratch, and time
series foundation models. All foundation models are evaluated in
a zero-shot setting, except for Moment and `TRACE`, which are finetuned on the forecasting head following the official protocol for
forecasting. For classification, we evaluate on our curated weather
dataset and fine-tune all foundation models in the same setting to
ensure a fair comparison (detailed in Appendix D.6).


Table 3: Weather Event Classification Results.

**Model** **Accuracy F1**


_**Train-from-scratch Model**_
DLinear 82.37 65.78

iTransformer 84.99 68.29

PatchTST 84.78 69.13

TimesNet 86.09 68.97

TimeMixer 84.78 68.65

FSCA 85.62 69.41

_**Finetune a Pre-trained Model**_
Time-MoE large 59.09 19.74
Moment base 65.43 28.29
Timer-XL 72.38 33.45
Chronos tiny 74.79 40.21


**Results.** As shown in Table 4, `TRACE` outperforms baselines across FSCA 85.62 69.41
different datasets and showcases capability on longer forecasting _**Finetune a Pre-trained Model**_
horizons ( _H_ ), whereas the performance of baselines exhibits con- Time-MoE large 59.09 19.74
siderable variation. This observation justifies the cross-modal de- Moment base 65.43 28.29
sign behind `TRACE`, which equips the model with stronger semantic Timer-XL 72.38 33.45
grounding and context-aware forecasting. In the event-type clas- Chronos tiny 74.79 40.21
sification task (as shown in Table 3), we observe that fine-tuned ~~`TRACE`~~ ~~_w/o_~~ ~~RAG~~ ~~85~~ . ~~20~~ ~~69~~ . ~~98~~
foundation models underperform traditional train-from-scratch baselines, suggesting that their embeddings may be overgeneralized and
poorly adapted to domain-specific classification signals. In contrast, `TRACE` achieves significantly
higher accuracy and F1 without RAG, and benefits further from the retrieval-augmented setting. This
demonstrates `TRACE` â€™s ability to retain discriminative structure while maintaining broad semantic
alignment, which is essential for robust downstream deployment. Full results of other foundation
model variants are in Appendix D.4.


**5** **Ablation Studies**

**Hyper-parameter Sensitivity.** Figure 4 presents a comprehensive ablation study investigating the
effects of patch length _P_, positional embedding (PE) types, and the number of retrieved instances

8

_R_ used in our RAG setup. Rotary PE consistently outperforms Relative PE by achieving lower
reconstruction and forecasting MSEs as well as higher classification accuracy, particularly when
using a smaller model size ( _d_ = 384 ). Notably, increasing the model size to _d_ = 768 does not yield
significant improvements, especially for downstream forecasting and classification tasks, suggesting
that careful architectural design and PE choice may matter more than simply scaling parameters.
Across tasks, mid-range patch lengths ( _e.g.,_ _P_ = 6 ) offer the best trade-off between local and global
temporal resolution. In Figure 4 (d), we observe that time series foundation models are relatively
robust to the choice of _R_, and models augmented with aligned text generally outperform their TS-only
counterparts, highlighting the benefit of cross-modal retrieval in improving forecasting performance.

**(a)** **(b)** **(c)** **(d)**

Figure 4: Ablation studies on patch length, positional embedding, and hidden dimension for (a)
Reconstruction MSE, (b) Classification Accuracy (%), and (c) Average Forecasting MSE. (d) shows
ablation studies on the number of retrieved instances ( _R_ ) in the RAG pipeline.

**Attention Variants.** Table 5 assess the impact of key architectural choices in `TRACE`, including
channel identity token ( `CIT` ) and different attention mechanisms. Removing `CIT` results in a notable
increase in average MSE, indicating its importance for capturing fine-grained temporal dependencies.
We also replace the channel-biased attention ( `CbA` ) with two alternatives: full attention, similar to
a multivariate variant of Moment [ 8 ], and causal attention, analogous to decoder-only designs like
Timer-XL [ 9 ], Both alternatives yield degraded performance. These results highlight the effectiveness
of the architectural design in `TRACE`, particularly the synergy between `CIT` and `CbA` in achieving
outstanding performance. We refer to Appendix D.9 for runtime and efficiency evaluation.


**Cross-Attention and Hard Negative Sampling.** Figure 5
presents an ablation study on key components in `TRACE` for
retrieval precision under different numbers of negative samples ( _K_ ). â€œallâ€ indicates using the entire batch (excluding the
paired counterpart) as negatives. The default model, using
`nomic` text encoder [ 57 ], consistently achieves the highest performance, especially when _K_ is small, highlighting its efficacy
in low-computation settings. Removing the final cross-attention
module between time series and text leads to notable performance degradation under small _K_, suggesting that cross-modal
fusion becomes especially crucial when fewer negatives are
available. Similarly, eliminating channel-level alignment yields
a consistent drop, confirming the strength of the proposed duallevel contrastive mechanism. Substituting `nomic` with weaker
text encoders like `bge` or `MiniLM` results in worse performance,
implying that high-quality embeddings are necessary for discriminating harder negatives. Overall, these trends support the
effectiveness of our hard negative mining strategy and emphasize the importance of dual-level alignment in retrieval performance. We provide empirical case studies in Appendix D.7.

**6** **Conclusion and Future Work**


Table 5: Ablation study on attention
variants in pre-training architecture.

Avg. MSE Acc. (%)

`TRACE` **0.670** _Â±_ **0.013** **85.20** _Â±_ **0.13**

_w/o_ `CIT` 0.713 _Â±_ 0.016 85.04 _Â±_ 0.26

`CbA` _â‡’_ Full Attn 0.705 _Â±_ 0.013 84.18 _Â±_ 0.11

`CbA` _â‡’_ Causal Attn 0.682 _Â±_ 0.015 83.72 _Â±_ 0.13

Figure 5: Retrieval performance under varying numbers of negative
samples. The best is indicated by _â‹†_ .


We introduce `TRACE`, a multimodal framework that aligns time series with textual descriptions at
both channel and sample levels. Extensive experiments demonstrate that `TRACE` outperforms strong
baselines across retrieval, standalone encoding, retrieval-augmented settings, and generalizes well
across model families. One limitation is that `TRACE` relies on supervised textual alignment, which
may not be readily available in all domains. Future work includes extending to more real-world
domains and exploring more expressive integration mechanisms for time-series RAG. We refer to
Appendix E for detailed discussion on social impact and future works.

9

**References**

[1] Francisco Martinez Alvarez, Alicia Troncoso, Jose C Riquelme, and Jesus S Aguilar Ruiz.
Energy time series forecasting based on pattern sequence similarity. _IEEE Transactions on_
_Knowledge and Data Engineering_, 23(8):1230â€“1243, 2010.

[2] Irena Koprinska, Dengsong Wu, and Zheng Wang. Convolutional neural networks for energy
time series forecasting. In _2018 international joint conference on neural networks (IJCNN)_,
pages 1â€“8. IEEE, 2018.

[3] Rafal A Angryk, Petrus C Martens, Berkay Aydin, Dustin Kempton, Sushant S Mahajan, Sunitha
Basodi, Azim Ahmadzadeh, Xumin Cai, Soukaina Filali Boubrahimi, Shah Muhammad Hamdi,
et al. Multivariate time series dataset for space weather data analytics. _Scientific data_, 7(1):1â€“13,
2020.

[4] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato,
Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, et al. Learning
skillful medium-range global weather forecasting. _Science_, 382(6677):1416â€“1421, 2023.

[5] Haoxin Liu, Shangqing Xu, Zhiyuan Zhao, Lingkai Kong, Harshavardhan Kamarthi, Aditya B
Sasanur, Megha Sharma, Jiaming Cui, Qingsong Wen, Chao Zhang, et al. Time-mmd: A new
multi-domain multimodal dataset for time series analysis. _arXiv preprint arXiv:2406.08627_,
2024.

[6] Jialin Chen, Aosong Feng, Ziyu Zhao, Juan Garza, Gaukhar Nurbek, Cheng Qin, Ali Maatouk,
Leandros Tassiulas, Yifeng Gao, and Rex Ying. Mtbench: A multimodal time series benchmark
for temporal reasoning and question answering, 2025.

[7] Geon Lee, Wenchao Yu, Kijung Shin, Wei Cheng, and Haifeng Chen. Timecap: Learning to
contextualize, augment, and predict time series events with large language model agents, 2025.

[8] Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski.
Moment: A family of open time-series foundation models. In _Proceedings of the 41st Interna-_
_tional Conference on Machine Learning_, pages 16115â€“16152. PMLR, 2024.

[9] Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. Timer-xl: Longcontext transformers for unified time series forecasting. _arXiv preprint arXiv:2410.04803_,
2024.

[10] Xiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, and Ming Jin.
Time-moe: Billion-scale time series foundation models with mixture of experts. _arXiv preprint_
_arXiv:2409.16040_, 2024.

[11] Chin-Chia Michael Yeh, Huiyuan Chen, Xin Dai, Yan Zheng, Junpeng Wang, Vivian Lai, Yujie
Fan, Audrey Der, Zhongfang Zhuang, Liang Wang, et al. An efficient content-based time series
retrieval system. In _Proceedings of the 32nd ACM International Conference on Information and_
_Knowledge Management_, pages 4909â€“4915, 2023.

[12] Huanyu Zhang, Chang Xu, Yi-Fan Zhang, Zhang Zhang, Liang Wang, Jiang Bian, and Tieniu
Tan. Timeraf: Retrieval-augmented foundation model for zero-shot time series forecasting.
_arXiv preprint arXiv:2412.20810_, 2024.

[13] Jingwei Liu, Ling Yang, Hongyan Li, and Shenda Hong. Retrieval-augmented diffusion models
for time series forecasting. _Advances in Neural Information Processing Systems_, 37:2766â€“2786,
2024.

[14] Kanghui Ning, Zijie Pan, Yu Liu, Yushan Jiang, James Y Zhang, Kashif Rasul, Anderson
Schneider, Lintao Ma, Yuriy Nevmyvaka, and Dongjin Song. Ts-rag: Retrieval-augmented
generation based time series foundation models are stronger zero-shot forecaster. _arXiv preprint_
_arXiv:2503.07649_, 2025.

[15] Silin Yang, Dong Wang, Haoqi Zheng, and Ruochun Jin. Timerag: Boosting llm time series
forecasting via retrieval-augmented generation. In _ICASSP 2025-2025 IEEE International_
_Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1â€“5. IEEE, 2025.

10

[16] Lu Han, Han-Jia Ye, and De-Chuan Zhan. The capacity and robustness trade-off: Revisiting
the channel independent strategy for multivariate time series forecasting. _arXiv preprint_
_arXiv:2304.05206_, 2023.

[17] Jongseon Kim, Hyungjoon Kim, HyunGi Kim, Dongjun Lee, and Sungroh Yoon. A comprehensive survey of deep learning for time series forecasting: architectural diversity and open
challenges. _Artificial Intelligence Review_, 58(7):1â€“95, 2025.

[18] Jialin Chen, Jan Eric Lenssen, Aosong Feng, Weihua Hu, Matthias Fey, Leandros Tassiulas,
Jure Leskovec, and Rex Ying. From similarity to superiority: Channel clustering for time series
forecasting. _Advances in Neural Information Processing Systems_, 37:130635â€“130663, 2024.

[19] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention
with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.

[20] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers:
Rethinking the stationarity in time series forecasting. _arXiv preprint arXiv:2205.14415_, 2022.

[21] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. _Advances in Neural Information_
_Processing Systems_, 34:22419â€“22430, 2021.

[22] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is
worth 64 words: Long-term forecasting with transformers. In _International Conference on_
_Learning Representations_, 2023.

[23] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency
for multivariate time series forecasting. In _The Eleventh International Conference on Learning_
_Representations_, 2022.

[24] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng
Long. itransformer: Inverted transformers are effective for time series forecasting, 2024.

[25] Binh Tang and David S Matteson. Probabilistic transformer for time series analysis. _Advances_
_in Neural Information Processing Systems_, 34:23592â€“23608, 2021.

[26] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer:
Frequency enhanced decomposed transformer for long-term series forecasting. _arXiv preprint_
_arXiv:2201.12740_, 2022.

[27] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar.
Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and
forecasting. In _International Conference on Learning Representations_, 2021.

[28] Aosong Feng, Jialin Chen, Juan Garza, Brooklyn Berry, Francisco Salazar, Yifeng Gao, Rex
Ying, and Leandros Tassiulas. Efficient high-resolution time series classification via attention
kronecker decomposition. _arXiv preprint arXiv:2403.04882_, 2024.

[29] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series
forecasting? _arXiv preprint arXiv:2205.13504_, 2022.

[30] Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O Arik, and Tomas Pfister. Tsmixer: An
all-mlp architecture for time series forecasting. _arXiv preprint arXiv:2303.06053_, 2023.

[31] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y Zhang,
and Jun Zhou. Timemixer: Decomposable multiscale mixing for time series forecasting. _arXiv_
_preprint arXiv:2405.14616_, 2024.

[32] Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin
Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham
Kapoor, et al. Chronos: Learning the language of time series. _arXiv preprint arXiv:2403.07815_,
2024.

[33] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. A decoder-only foundation model
for time-series forecasting. In _Forty-first International Conference on Machine Learning_, 2024.

11

[34] Xu Liu, Juncheng Liu, Gerald Woo, Taha Aksu, Yuxuan Liang, Roger Zimmermann, Chenghao
Liu, Silvio Savarese, Caiming Xiong, and Doyen Sahoo. Moirai-moe: Empowering time series
foundation models with sparse mixture of experts. _arXiv preprint arXiv:2410.10469_, 2024.

[35] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu
Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by
reprogramming large language models. _arXiv preprint arXiv:2310.01728_, 2023.

[36] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. One fits all: Power general time series
analysis by pretrained lm. _Advances in neural information processing systems_, 36:43322â€“43355,
2023.

[37] Yuxiao Hu, Qian Li, Dongxiao Zhang, Jinyue Yan, and Yuntian Chen. Context-alignment:
Activating and enhancing llm capabilities in time series. _arXiv preprint arXiv:2501.03747_,
2025.

[38] Chenxi Liu, Qianxiong Xu, Hao Miao, Sun Yang, Lingzheng Zhang, Cheng Long, Ziyue Li,
and Rui Zhao. Timecma: Towards llm-empowered multivariate time series forecasting via
cross-modality alignment. _arXiv preprint arXiv:2406.01638_, 2024.

[39] Zijie Pan, Yushan Jiang, Sahil Garg, Anderson Schneider, Yuriy Nevmyvaka, and Dongjin Song.
_s_ [2] ip-llm: Semantic space informed prompt learning with llm for time series forecasting. In
_Forty-first International Conference on Machine Learning_, 2024.

[40] Taibiao Zhao, Xiaobing Chen, and Mingxuan Sun. Enhancing time series forecasting via
multi-level text alignment with llms. _arXiv preprint arXiv:2504.07360_, 2025.

[41] Chengsen Wang, Qi Qi, Jingyu Wang, Haifeng Sun, Zirui Zhuang, Jinming Wu, Lei Zhang, and
Jianxin Liao. Chattime: A unified multimodal time series foundation model bridging numerical
and textual data. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 39,
pages 12694â€“12702, 2025.

[42] Zhe Xie, Zeyan Li, Xiao He, Longlong Xu, Xidao Wen, Tieying Zhang, Jianjun Chen, Rui
Shi, and Dan Pei. Chatts: Aligning time series with llms via synthetic data for enhanced
understanding and reasoning. _arXiv preprint arXiv:2412.03104_, 2024.

[43] Yushan Jiang, Wenchao Yu, Geon Lee, Dongjin Song, Kijung Shin, Wei Cheng, Yanchi Liu,
and Haifeng Chen. Explainable multi-modal time series prediction with llm-in-the-loop. _arXiv_
_preprint arXiv:2503.01013_, 2025.

[44] Yanyan Yue, Xingbo Zhang, Jiancheng Lv, and Bo Du. Ts2vec: Towards universal representation
of time series. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36,
pages 8268â€“8276, 2022.

[45] Baoyu Jing, Si Zhang, Yada Zhu, Bin Peng, Kaiyu Guan, Andrew Margenot, and Hanghang
Tong. Retrieval based time series forecasting. _arXiv preprint arXiv:2209.13525_, 2022.

[46] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:
Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.

[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information_
_processing systems_, 30, 2017.

[48] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo.
Reversible instance normalization for accurate time-series forecasting against distribution shift.
In _International conference on learning representations_, 2021.

[49] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language_
_Processing_ . Association for Computational Linguistics, 11 2019.

[50] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun,
Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models:
A survey. _arXiv preprint arXiv:2312.10997_, 2:1, 2023.

12

[51] DOC/NOAA/NESDIS/NCDC and National Climatic Data Center, NESDIS, NOAA, U.S. Department of Commerce. Storm Events Database, 2023. Accessed: February 21, 2025.

[52] Matthew J. Menne, Simon Noone, Nancy W. Casey, Robert H. Dunn, Shelley McNeill, Diana
Kantor, Peter W. Thorne, Karen Orcutt, Sam Cunningham, and Nicholas Risavi. Global
Historical Climatology Network-Hourly (GHCNh), 2023.

[53] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet:
Temporal 2d-variation modeling for general time series analysis. In _International Conference_
_on Learning Representations_, 2023.

[54] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y. Zhang,
and Jun Zhou. Timemixer: Decomposable multiscale mixing for time series forecasting, 2024.

[55] Yuyang Wu, Haoran Zhang, Yong Liu, and Mingsheng Long. A decoder-only foundation model
for time-series forecasting. _arXiv preprint arXiv:2405.12345_, 2024.

[56] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint_
_arXiv:1711.05101_, 2017.

[57] Zach Nussbaum, John X Morris, Brandon Duderstadt, and Andriy Mulyar. Nomic embed:
Training a reproducible long context text embedder. _arXiv preprint arXiv:2402.01613_, 2024.

[58] Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Mingsheng Long, and Jianmin Wang.
Deep time series models: A comprehensive survey and benchmark. 2024.

[59] Pavel Senin and Sergey Malinchik. Sax-vsm: Interpretable time series classification using sax
and vector space model. In _2013 IEEE 13th international conference on data mining_, pages
1175â€“1180. IEEE, 2013.

13

## **Appendix**

**A** **Notations**

The main notations used throughout this paper are summarized in Table 6.

Table 6: Summary of the notations used in this paper.

|Notation|Description|
|---|---|
|||
|X âˆˆRCÃ—T<br>P<br>TË†<br>C<br>T<br>H<br>d<br>L<br>Xpatch<br>i<br>M âˆˆ{0, 1}LÃ—L<br>Hout âˆˆRLÃ—d<br>h âˆˆRd<br>[CLS]<br>H âˆˆRCÃ—d<br>[CIT]<br>Ï„<br>cxt<br>Ï„<br>c<br>z âˆˆRd<br>cxt<br>z âˆˆRd<br>c<br>N, N<br>cxt ch|Input multivariate time series with C channels and T time steps<br>Patch length for time series tokenization<br>Number of temporal patches per channel (âŒŠT/PâŒ‹)<br>Number of channels (variables)<br>Number of time steps in the original sequence<br>Forecasting horizon<br>Embedding dimension<br>Length of the flattened token sequence<br>Embedding of the i-th patch token<br>Biased attention mask for the flatten token sequence<br>Output token embeddings from the Transformer encoder<br>Embedding of the global [CLS] token<br>Embeddings of Channel Identity Tokens (CITs)<br>Sample-level textual context associated with a time series<br>Channel-level textual description for the c-th variable<br>Semantic embedding of the sample-level text<br>Semantic embedding of the channel-level text<br>Sample-level and channel-level hard negative candidate sets|



**B** **Dataset Curation**

We curate a new multimodal time series dataset in the weather domain by extending MTBench [ 6 ]. It
is built from two primary sources:

   - **Event reports** from the _NOAA Storm Events Database_ [ 51 ], which contains detailed narratives
of severe weather occurrences across the U.S.

   - **Weather Time Series (TS) data** from the _NOAA Global Historical Climatology Network -_
_Hourly (GHCN-h)_ [52], covering multiple meteorological variables.

When applying `TRACE` to our curated dataset, the sample-level context is event report, while the
channel-level description is synthetically generated by LLMs.

**B.1** **Station and Event Selection**

We begin by selecting over 100 U.S. locations frequently affected by severe weather events and
associated with long narrative reports. This yields approximately 5,000 event entries. For each event
location, we identify nearby GHCN-h weather stations and extract multivariate TS data anchored at
the start time of each event.

**B.2** **Time Series Sampling**

Each event is treated as an anchor point to extract TS data at three resolutions:

   - Hourly for 7 days

   - Every 4 hours for 28 days

14

   - Daily for 180 days

This results in approximately 15,000 TS samples from event-associated windows. To balance the
dataset, we sample an additional 30,000 TS sequences from the same stations at random non-event
times, ensuring no overlapping event narratives. To enhance weather diversity, we also sample 30,000
TS sequences from geographically distant stations without any event association, using randomly
selected anchor times. See Figure 6. All time series instances contain seven channels: temperature,
humidity, wind_u, wind_v, visibility, precipitation, and sky code. The curated weather dataset
contains a total of 74,337 time series instances, and the lengths have a mean of 169.25 and a median
of 168.0.

~~With~~ ~~Event~~ ~~Without~~ ~~Event~~

Figure 6: The red points are locations with event reports and the blue points are locations without
event reports




15

widespread minor nuisance flooding in the San Joaquin Valley and Sierra foothills with a few
rock slides noticed. Several roads were closed as a precaution and chain restrictions were
implemented on some roads in the Sierra Nevada. The storm also produced strong winds over
the West Side Hills as well as in the Grapevine and Tehachapi areas in Kern County. Several
stations in these areas measured wind gusts exceeding 50 mph with a few locations near the
Grapevine measuring brief gusts exceeding 70 mph. California Highway Patrol reported mud,
rock and dirt covering most of North Plano St. near Lynch Dr.",

**B.3** **Synthetic Description Generation**

We use ChatGPT to generate channel-level textual descriptions for selected TS samples, where all TS
samples linked to event reports are included. We also randomly select 50% of TS samples from both
the non-event windows at event-associated stations and the non-event-associated stations to generate
channel-level descriptions for event-label balance. The generated descriptions follow the style of
TimeCap[ 7 ], but each is additionally annotated with one or more keywords selected from the set as
auxiliary information: { Clear, Cloudy, Rainy, Snowy, Windy, Foggy, Hot, Cold, Humid, Stormy }.
We use a consistent meta-prompt to elicit both descriptive and label-aligned outputs. A full example
of the meta-prompt and a generated description is provided in B.4.

**B.4** **Prompt for Weather Description Generation and an example synthetic description**





16

**Code** **Meaning** **Sky Fraction Covered**
00 CLR (Clear) 0/8 or 0%
01 FEW 1/8 ( 12%)
02 FEW 2/8 - 3/8 (25%-37%)
03 SCT (Scattered) 4/8 ( 50%)
04 SCT 5/8 ( 62%)
05 BKN (Broken) 6/8 ( 75%)
06 BKN 7/8 - 8/8 (87%-100%)
07 BKN _âˆ¼_ 9/10
08 OVC (Overcast) 10/10 (100%)
09 VV (Vertical Visibility) Sky obscured
10 X (Unknown) Partially obscured

**Please summarize the data using the following format:**

   - **Date:** {sentence of date}

   - **Location:** {sentence of location}

   - **Temperature:** {sentence of temperature}

   - **Precipitation:** {sentence of precipitation}

   - **Relative Humidity:** {sentence of relative humidity}

   - **Visibility:** {sentence of visibility}

   - **Wind_V:** {sentence of wind_v}

   - **Wind_U:** {sentence of wind_u}

   - **Sky Cover:** {sentence of sky cover}

   - **Keywords:** {list of keywords from label set}

No additional explanation or commentary should be included in the output.




**B.5** **Dataset Details**

Our curated weather dataset contains a total of 74,337 time series instances. We allocate 9,561 of
these exclusively for the forecasting task, ensuring this subset is disjoint from the pretraining and
classification data to avoid any potential label leakage or information overlap. The classification task
is formulated as multi-class event prediction, where each time series instance is annotated by the
NOAA System with a corresponding weather event type from nine common severe weather events,
and one special category for non-events. The event labels are as follows: _Lightning (0), Debris_

17

Table 7: Dataset size for each task.

**Dataset Type** **Train** **Test** **Val** **Total**

_**Newly Curated Weather Dataset**_
Forecasting (H=7) 6,690 957 1,914 9,561
Pretraining & Classification 45,339 6,484 12,953 64,776

_**Public Dataset from TimeMMD [5]**_
Health (H=12) 929 266 129 1,324
Energy (H=12) 992 284 138 1,414
Environment (H=48) 7,628 2,173 1,064 10,865

_Flow (1), Flash Flood (2), Heavy Rain (3), Tornado (4), Funnel Cloud (5), Hail (6), Flood (7),_
_Thunderstorm Wind (8)_ . Instances that do not correspond to any specific event are labeled as `None` .
This setup ensures the model learns to distinguish between distinct event types while being robust
to trivial (non-event) data. We follow the original split to create train/test/val set for TimeMMD
forecasting tasks [5].

**C** **Alignment Objective: Full Formulation**

To fully capture the structured alignment between multivariate time series and text, we employ a
dual-level contrastive learning strategy, consisting of sample-level and channel-level hard negative
mining.

**C.1** **Hard Negative Candidate Sets**

Given a time series instance _i_ with _C_ channels, we define the following negative sets:

**Sample-level negative sets.** For aligning the global [CLS] embedding **h** [(] `[CLS]` _[i]_ [)] [of instance] _[ i]_ [ with its]
corresponding sample-level textual embedding **z** [(] cxt _[i]_ [)] [, we mine hard negatives from other samples in]
the batch. Specifically:

_N_ cxt [(] _[i]_ [)] [= Top] _K_ ï¿½sim( **h** [(] `[CLS]` _[i]_ [)] _[,]_ **[ z]** [(] cxt _[j]_ [)] [)] _[ |][ j][ Ì¸]_ [=] _[ i]_ ï¿½ _,_ (3)

and symmetrically,

_N_ cxt [(] _[i,]_ [text][)] = Top _K_ ï¿½sim( **z** [(] cxt _[i]_ [)] _[,]_ **[ h]** [(] `[CLS]` _[j]_ [)] [)] _[ |][ j][ Ì¸]_ [=] _[ i]_ ï¿½ _._ (4)

**Channel-level negative sets.** To align each channel-specific CIT embedding **h** [(] _c_ _[i]_ [)] with its corresponding channel-level text embedding **z** [(] _c_ _[i]_ [)] [, we mine two types of distractors:]

   - _Intra-instance negatives:_ embeddings from other channels within the same instance, i.e., **z** [(] _c_ _[i]_ _[â€²]_ [)]
where _c_ _[â€²]_ = _Ì¸_ _c_ ;

   - _Inter-instance negatives:_ same-indexed channel embeddings across different instances, i.e.,
**z** [(] _c_ _[j]_ [)] where _j Ì¸_ = _i_ .

Formally, the channel-level negative set is defined as:

_N_ ch [(] _[i,c]_ [)] = Top _K_ sim( **h** [(] _c_ _[i]_ [)] _[,]_ **[ z]** [(] _c_ _[j]_ _[â€²]_ [ )] [)] _[ |][ c]_ _[â€²]_ _[ Ì¸]_ [=] _[ c]_ [ or] _[ j][ Ì¸]_ [=] _[ i]_ _,_ (5)
ï¿½ ï¿½

and similarly in the reverse direction:

_N_ ch [(] _[i,c,]_ [text][)] = Top _K_ sim( **z** [(] _c_ _[i]_ [)] _[,]_ **[ h]** [(] _c_ _[j]_ _[â€²]_ [ )] [)] _[ |][ c]_ _[â€²]_ _[ Ì¸]_ [=] _[ c]_ [ or] _[ j][ Ì¸]_ [=] _[ i]_ _._ (6)
ï¿½ ï¿½

18

**C.2** **Contrastive Alignment Loss**

We adopt a bidirectional InfoNCE loss at both the sample and channel levels. For each alignment
direction, the objective maximizes the similarity between the positive pair and minimizes similarity
with hard negatives.

**Sample-level loss.**

exp(sim( **z** [(] cxt _[i]_ [)] _[,]_ **[ h]** [(] `[` _[i]_ `CLS` [)] `]` [)] _[/][Ï„]_ [)]
_L_ [text] global _[â†’]_ [ts] [=] _[ âˆ’]_ [log] (7)
ï¿½ exp(sim( **z** [(] cxt _[i]_ [)] _[,]_ **[ h]** [(] `[CLS]` _[j]_ [)] [)] _[/Ï„]_ [)]

_jâˆˆ{i}âˆªN_ cxt [(] _[i,]_ [text][)]

exp(sim( **h** [(] `[` _[i]_ `CLS` [)] `]` _[,]_ **[ z]** cxt [(] _[i]_ [)] [)] _[/][Ï„]_ [)]
_L_ [ts] global _[â†’]_ [text] [=] _[ âˆ’]_ [log] (8)
ï¿½ exp(sim( **h** [(] `[CLS]` _[i]_ [)] _[,]_ **[ z]** [(] cxt _[j]_ [)] [)] _[/Ï„]_ [)]

_jâˆˆ{i}âˆªN_ cxt [(] _[i]_ [)]

**Channel-level loss.**


(9)

ï¿½ exp(sim( **z** [(] _c_ _[i]_ [)] _[,]_ **[ h]** [(] _c_ _[j]_ _[â€²]_ [ )] [)] _[/Ï„]_ [)]

( _j,c_ _[â€²]_ ) _âˆˆ{_ ( _i,c_ ) _}âˆªN_ ch [(] _[i,c,]_ [text][)]


_L_ [text] channel _[â†’]_ [ts] [=] _C_ [1]

_L_ [ts] channel _[â†’]_ [text] [=] _C_ [1]

**C.3** **Total Loss Objective**


_C_
ï¿½


_C_

_âˆ’_ exp(sim( **z** [(] _c_ _[i]_ [)] _[,]_ **[ h]** [(] _c_ _[i]_ [)] [)] _[/][Ï„]_ [)]

ï¿½ log

_c_ =1 ï¿½ exp(sim( **z** [(] _c_ _[i]_


_C_
ï¿½


(10)

ï¿½ exp(sim( **h** [(] _c_ _[i]_ [)] _[,]_ **[ z]** [(] _c_ _[j]_ _[â€²]_ [ )] [)] _[/Ï„]_ [)]

( _j,c_ _[â€²]_ ) _âˆˆ{_ ( _i,c_ ) _}âˆªN_ ch [(] _[i,c]_ [)]


_C_

_âˆ’_ exp(sim( **h** [(] _c_ _[i]_ [)] _[,]_ **[ z]** [(] _c_ _[i]_ [)] [)] _[/][Ï„]_ [)]

ï¿½ log

_c_ =1 ï¿½ exp(sim( **h** [(] _c_ _[i]_ [)]


The total alignment loss is the average of both sample-level and channel-level contrastive losses:


2 [1] ï¿½ _L_ [text] global _[â†’]_ [ts] [+] _[ L]_ [ts] global _[â†’]_ [text] ï¿½ + _Î»_ ch _Â·_ 2 [1]


_L_ align = [1]


ï¿½ _L_ channel [text] _[â†’]_ [ts] [+] _[ L]_ [ts] channel _[â†’]_ [text] ï¿½ _,_ (11)
2


where _Ï„_ is the temperature hyperparameter, and _Î»_ ch is a hyperparameter, controlling the contribution
of channel-level alignment. We set _Î»_ ch = 1 _._ 0 as default in experiments.

**D** **Experiments**

**D.1** **Baselines**

**D.1.1** **Full-shot Time Series Models**

**DLinear** (Decomposition-Linear) [ 29 ]is a lightweight time-series forecasting model that decomposes
the input into trend and seasonal components, and applies simple linear layers to each component
separately. Despite its simplicity, DLinear has demonstrated strong performance on both long- and
short-term forecasting tasks by effectively capturing linear temporal patterns without relying on
complex neural architectures.

**PatchTST** [ 22 ] reformulates time-series forecasting as a patch-based sequence modeling problem.
It splits the input time series into non-overlapping patches and applies a Transformer encoder to
model inter-patch dependencies. The design removes positional encoding and avoids decoder layers,
making the model more suitable for forecasting tasks while benefiting from the global receptive field
of Transformers.

**iTransformer** [ 24 ] (Instance-aware Transformer) extends Transformer-based forecasting by modeling
instance-wise variations. It introduces a shared backbone Transformer and an instance-specific
modulation mechanism, enabling the model to better adapt to diverse temporal dynamics across
different time-series samples. This design improves generalization and robustness, particularly for
multivariate forecasting.

19

**TimesNet** [ 53 ] proposes a novel temporal block that captures multi-frequency patterns in time-series
data using learnable convolutions in the frequency domain. By combining time and frequency-domain
features, TimesNet achieves strong performance across a variety of datasets. It is particularly effective
at modeling both short-term and long-term temporal dependencies.

**TimeMixer** [ 54 ] employs a structured state-space-inspired architecture where time mixing and channel
mixing operations alternate. It replaces self-attention with parameter-efficient mixing blocks that
blend information across the temporal and feature dimensions. TimeMixer is designed for scalable
forecasting and excels in low-resource regimes due to its compact architecture and efficient training.

**FSCA** [ 37 ] introduces a new paradigm that aligns time series (TS) with a linguistic component in
the language environments familiar to LLMs to enable LLMs to contextualize and comprehend TS
data, thereby activating their capabilities. FSCA uses a Dual-Scale Context-Alignment Graph Neural
Networks (DSCA-GNNs) framework to achieve both structural and logical alignment, demonstrate
good performance in few-shot and zero-shot settings.

**D.1.2** **Time Series Foundation Model**

Table 8: Comparison of time-series foundation models.

|Method|Chronos Time-MoE TimesFM Moirai Moment Timer-XL|
|---|---|
|Architecture<br>(Max) Model Size<br>Input Token<br>Max Length<br>FFN<br>Cross-channel|Encoder-Decoder Decoder-Only Decoder-Only Encoder-Only Encoder-Only Decoder-only<br>710M 2.4B 200M 311M 385M 84M<br>Point Point Patch Patch Patch Patch<br>512 4096 512 5000 512 1024<br>Dense Sparse Dense Dense Dense Dense<br>âœ— âœ— âœ— âœ“ âœ— âœ“|



We test several recent time-series foundation models that have been pretrained on large-scale datasets
from relevant domains, including weather, healthcare, energy, and environment. These include
Chronos [ 32 ], Time-MoE [ 10 ], TimesFM [ 55 ], Moirai [ 34 ], Moment [ 8 ], and Timer-XL [ 9 ], which
offer strong generalization through large-scale pretraining. A comparison is given in Table 8. To
evaluate retrieval-augmented performance on diverse real-world domains, we integrate our retriever
with three publicly available time-series foundation models: Time-MoE, Timer-XL, and Moment,
which are selected based on the availability of stable, open-source implementations that support
customization and downstream fine-tuning. We leave the adaptation of our retriever to additional
proprietary or closed-source foundation models, as well as its integration into unified pretraining
pipelines, for future work.

**Comparison of** `TRACE` **with Time-series Foundation Models** . It is important to note that our model
is not itself a cross-domain foundation model, but rather a modular encoder-based retriever capable
of enhancing such models. Architecturally, our model adopts an encoder-only design with flexible
point- and patch-based tokenization, supports input sequences exceeding 2,048 tokens, and enables
effective cross-channel interactions through channel-biased attention mechanisms.

**D.2** **Experiment Configurations**

All models are implemented in PyTorch and trained on NVIDIA A100 40GB GPUs. For most time
series models, we adopt the implementation from TSLib[ 58 ] [2] . The sequence length is fixed at 96
for both prediction horizons of 7 and 24. We use mean squared error (MSE) as the loss function for
forecasting tasks, and accuracy for classification. Forecasting models are trained for 10 epochs, while
classification models are trained for up to 150 epochs with early stopping. We follow the official code
to implement other baselines [ 37 ] [3] . All other hyperparameters follow the default settings in TSLib,
except for those explicitly tuned to achieve the best performance, as reported in Tables 9. For our
model, the initial learning rate is tuned from _{_ 10 _[âˆ’]_ [4] _,_ 10 _[âˆ’]_ [3] _}_ . The number of attention layers is tuned
from _{_ 6 _,_ 12 _}_, and the hidden dimension is from _{_ 384 _,_ 768 _}_ with the number of heads in _{_ 6 _,_ 12 _}_ .

2 `[https://github.com/thuml/Time-Series-Library](https://github.com/thuml/Time-Series-Library)`
3 `[https://github.com/tokaka22/ICLR25-FSCA](https://github.com/tokaka22/ICLR25-FSCA)`

20

Table 9: Best hyperparameters per model

Model Name Learning Rate Encoder Layers Hidden Dimension

DLinear 0.0010 2 32

PatchTST 0.0050 4 64

TimeMixer 0.0100 4 64

TimesNet 0.0010 4 64

iTransformer 0.0100 4 64

FSCA 0.0001 4 256

**D.3** **Embedding Visualization**

Figure 7 presents the cosine similarity matrix between text and time series embeddings across
the test set. The diagonal dominance indicates that `TRACE` successfully aligns each time series
with its corresponding textual description, suggesting strong one-to-one semantic matching in the
shared embedding space. Off-diagonal similarities remain low, demonstrating the modelâ€™s ability to
distinguish unrelated instances. Figure 8 visualizes the joint embedding space using UMAP. Each
color represents a distinct event category, where circles ( _â—¦_ ) denote time series instances and crosses
( _Ã—_ ) denote their corresponding textual descriptions. A line connects each textâ€“time series pair. We
observe clear clustering by event type, with paired modalities positioned closely in the embedding
space. Notably, for some events ( _e.g.,_ "Flood" and "Debris Flow"), clusters partially overlap, reflecting
shared underlying dynamics. The tight alignment between paired points validates the effectiveness of
our dual-level alignment strategy, and the modality-mixing within clusters suggests successful fusion
of structured and unstructured signals.



8

6

4




Figure 7: Cosine Similarity Matrix Between Text
and Time Series Embeddings.

**D.4** **Classification Task**


2

0

2

2 4 6 8

Figure 8: Umap Visualization of Aligned Text
and Time Series Embeddings.


Table 10 reports the classification accuracy and F1 scores
of different size variants of time-series foundation models on the weather event classification task. We observe
that a larger model size does not necessarily lead to better performance. For example, Momentâ€™s base model
achieves a higher F1 score than the large model despite a
lower accuracy. In contrast, Chronos exhibits more stable
performance across scales, with the tiny and mini variants achieving the best F1 scores, outperforming even
the larger variants. These results suggest that, in domainspecific classification tasks with relatively limited supervision, scaling up foundation models may not always be
beneficial, and smaller models can offer a better balance
between accuracy and efficiency.

21


Table 10: Weather Event Classification
Accuracy and F1 Score (%).

**Model** **Size** **Accuracy** **F1**

small 56.27 16.56
Time-MoE
large 59.09 19.74

base 65.43 28.29
Moment
large 64.94 26.35


Chronos


tiny 74.79 40.21
mini 73.89 37.98

small 71.07 35.39

base 71.42 36.40
large 71.97 36.30

**D.5** **RAG Setting**

In our retrieval-augmented generation (RAG) framework, given a query time series **X** _q_, we compute
its `[CLS]` token embedding as **h** _q_ _âˆˆ_ R _[d]_ using the frozen encoder from `TRACE` . Based on cosine
_R_
similarity, we retrieve the top- _R_ most relevant multimodal pairs ( **X** _[i]_ _, Ï„_ cxt _[i]_ [)] _i_ =1 [from the corpus, where]
**X** _[i]_ is a historical multivariate time series and _Ï„_ cxt _[i]_ [is the associated sample-level context. Each retrieved]
pair is transformed into a soft prompt vector using a trainable linear projection layer. Specifically, the
time series component is encoded to **h** [(] ts _[i]_ [)] _[âˆˆ]_ [R] _[d]_ [, and the textual context] _[ Ï„]_ _[ i]_ cxt [is encoded to] **[ z]** [cxt] [(] _[i]_ [)] _[ âˆˆ]_ [R] _[d]_
using a frozen SentenceTransformer, followed by a shared projection. For the _TS+Text_ setting, we
concatenate each pair as **p** [(] _[i]_ [)] = [ _h_ [(] ts _[i]_ [)] [;] **[ z]** [(] cxt _[i]_ [)] []] _[ âˆˆ]_ [R] [2] _[d]_ [, and stack all] _[ R]_ [ vectors to form the final prompt:]

**P** = `Proj` [ **p** [(1)] ; _Â· Â· Â·_ ; **p** [(] _[R]_ [)] ] _âˆˆ_ R _[d]_ _[f]_ _,_
ï¿½ ï¿½

where `Proj` is a feedforward layer mapping from R [2] _[Rd]_ _â†’_ R _[d]_ _[f]_, and _d_ _f_ is the hidden dimension of
the downstream time series foundation model. For the _TS-only_ setting, we omit the text component
and instead concatenate [ _h_ [(1)] ts [;] _[ Â· Â· Â·]_ [ ;] _[ h]_ ts [(] _[R]_ [)] ] _âˆˆ_ R _[Rd]_ and project into R _[d]_ _[f]_ accordingly.

This dense prompt **P** is prepended to the query sequence during inference. For decoder-only models
( _e.g.,_ Timer-XL, Time-MoE), **P** is appended to the autoregressive context at each decoding step. For
encoder-only models ( _e.g.,_ Moment, `TRACE` ), **P** is inserted as a prefix to the encoder input, _i.e.,_

Ë†
_y_ = `Head` ([ **P** _|_ **H** _q_ ]) _,_

where **H** _q_ _âˆˆ_ R _[L][Ã—][d]_ _[f]_ is the encoded query and `Head` is a forecasting head trained from scratch. In
all configurations, only `Proj` and `Head` are updated during training in RAG framework, while the
backbone foundation model remains frozen.

**D.6** **Standalone Time Series Encoder**

To evaluate the classification capabilities of time series foundation models, we finetune a multi-layer
perceptron (MLP) classifier on top of each modelâ€™s final output representation, as most existing
time series foundation models do not support classification task by design, except Moment [ 8 ], The
MLP consists of four hidden layers with sizes [256 _,_ 128 _,_ 64 _,_ 32], followed by a softmax output layer
corresponding to 9 weather event categories. This architecture was selected based on empirical
tuning for optimal performance on our classification task. We include all available variants from
four foundation model families: Time-MoE, Timer-XL, Moment, and Chronos. All backbone
parameters of the time series foundation models are fully activated and updated during training to
ensure consistency and fair evaluation. Each model is finetuned for 100 epochs using the Adam
optimizer. The training batch size is set to 256 for small and mid-sized variants, and reduced to 128
for larger models to accommodate memory constraints.

For full-shot time series models, we train them from scratch using a unified training and evaluation
protocol with time series foundation models. Results are shown in Table 3 and Table 10.

**D.7** **Empirical Case Study**

Figure 9 illustrates the capability to align detailed textual context with corresponding multivariate
time series. The retrieval pool is constructed by excluding the queryâ€™s paired time series instance.
This setup ensures that retrieved results are non-trivial and reflect the modelâ€™s ability to identify
semantically similar yet distinct examples. `TRACE` leverages both high-level and fine-grained semantic
cues to retrieve the most relevant time series from the curated candidate pool. The top-1 retrieved
sequence closely reflects key patterns in the query text, which can serve as a valuable reference for
downstream forecasting, scenario simulation, or contextual explanation.

**D.8** **Timeseries-to-Timeseries Retrieval**

To assess the effectiveness of our model in time series retrieval, we conduct a TS-to-TS retrieval task
where each query is matched against all other time series to identify the most semantically similar
ones. The evaluation is performed using the label matching metrics (Sec. 4.2), including Precision@1,
Precision@5, and Mean Reciprocal Rank (MRR), alongside query time as a proxy for computational
efficiency.

22

**Top-1 Retrieved Time Series (Label: Flash Flood):**

Figure 9: A case study of text-to-timeseries retrieval of flash flood-related time series. The key textual
cues are highlighted in color for clarity.


**Baseline Setup.** We compare `TRACE`, against
several representative time series retrieval methods. Euclidean Distance (ED) serves as a simple
statistical baseline based on mean-pooled raw
time series. Dynamic Time Warping (DTW), a
classic elastic matching method, evaluates similarity by aligning sequences with potential shifts,
but at significant computational cost. SAXVSM [ 59 ] leverages symbolic aggregation and
vector space modeling to convert time series into
symbolic representations for efficient textual retrieval. CTSR [ 11 ] refers to a learned baseline
that uses contextual metadata to enhance retrieval.


Table 11: TS-to-TS Retrieval performance comparison. Evaluation is conducted over 1000 randomly
sampled time series queries.


**Method** **P@1** **P@5** **MRR** **Time (s)**

ED 0.548 0.762 0.644 0.083

DTW 0.380 0.770 0.543 2273.93

SAX-VSM 0.551 0.769 0.649 0.343

CTSR 0.682 0.893 0.802 0.057

`TRACE` **0.900** **0.986** **0.938** **0.045**


**Analysis** . The results shown in Table 11 demonstrate that `TRACE` substantially outperforms all
baselines across accuracy metrics while maintaining the lowest retrieval latency. Notably, despite
the design simplicity of SAX-VSM and its moderate performance gains over raw ED, it fails to
capture deep temporal or semantic patterns. CTSR, while benefiting from structured cues, struggles to
generalize as effectively in purely time-series scenarios. The results suggest that learned time-series
representations, when equipped with task-driven objectives and textual alignment-aware training,
provide not only superior retrieval quality but also enable scalable and efficient retrieval pipelines.
The combination of semantic precision and runtime efficiency highlights the efficacy of `TRACE` for
real-world applications where fast and accurate time series matching is critical.



























Figure 10: Visualization of Timeseries-to-Timeseries Retrieval by `TRACE`

23

**TS-to-TS Case Study** . Figure 10 illustrates a case study of TS-to-TS retrieval using `TRACE` . Given a
query time series (top row) labeled as `Flood`, the system retrieves the top-3 most similar samples from
the corpus based on embedding similarity in the shared representation space. The similarity score
for each retrieved sample is shown on the left, with per-channel similarity values annotated below
each plot. We observe that all retrieved samples have high overall similarity scores (approximately
0.79â€“0.81), reflecting strong semantic alignment. The top-2 retrievals are also labeled as `Flood`,
while the third belongs to a semantically related event, `Flash Flood`, suggesting that `TRACE` is
capable of retrieving contextually relevant samples even across closely related labels. Notably, `TRACE`
enables fine-grained channel-level similarity assessment by leveraging its Channel Identity Tokens
(CIT), which allow independent embedding of channel-specific signals.

However, we also find that high similarity in individual channels ( _e.g.,_ temperature or precipitation)
does not always guarantee high overall semantic alignment. For instance, the first retrieval shows
moderate similarity across channels but still achieves a high overall semantic score. This highlights
the benefit of `TRACE` â€™s structured aggregation over all channels to capture global semantics and
reveal the most semantically dominant channels that contribute most to the retrieval relevance. This
capability enables `TRACE` to go beyond surface-level similarity, retrieving samples that share latent
event signatures rather than merely matching patterns across all channels uniformly.

**D.9** **Complexity and Efficiency**

**D.9.1** **Computational Complexity**

We analyze the computational complexity of the main components in `TRACE`, including the encoder
stage, the dual-level contrastive alignment, and the retrieval-augmented generation (RAG) setup.

**1. Encoder Pre-training Complexity** . Let _X âˆˆ_ R _[C][Ã—][T]_ be the input multivariate time series with _C_
channels and _T_ time steps. The sequence is tokenized into _T_ [Ë†] = _âŒŠT/P_ _âŒ‹_ patches per channel, each
projected to a _d_ -dimensional embedding. The total token length after flattening is 1 + _C_ ( _T_ [Ë†] + 1) .
This includes one global `[CLS]` token, one `[CIT]` token per channel, and _T_ [Ë†] patch tokens per channel.
The encoder is a _N_ -layer Transformer with multi-head channel-biased attention. The complexity per
attention layer is _O_ ( _L_ [2] _d_ ) = _O_ ( _C_ [2] [ Ë†] _T_ [2] _d_ ) . Note that channel-biased attention applies a sparse mask
_M âˆˆ{_ 0 _,_ 1 _}_ _[L][Ã—][L]_ to restrict certain attention to within-channel interactions, which effectively reduces
the constant factors in practice but not the asymptotic complexity.

**2. Dual-level Contrastive Alignment** . Let _B_ be the batch size. For each time series, the alignment
stage computes:

 - Sample-level similarity: _O_ ( _B_ [2] _d_ ) for all **h** `[CLS]` â€“ **z** cxt pairs.

 - Channel-level similarity: For _C_ channels and _B_ instances, total cost is _O_ ( _B_ [2] _C_ [2] _d_ ) for **h** _c_ â€“ **z** _c_
pairs.

 - Negative mining selects top- _R_ hardest negatives per instance and per channel, which costs
_O_ ( _B_ log _R_ + _BC_ log _R_ ), and is negligible compared to similarity computation.

**3. Retrieval-Augmented Generation** . During inference, retrieval selects top- _R_ neighbors for a
query based on cosine similarity:

 - Retrieval cost: _O_ ( _Rd_ ) using approximate methods ( _e.g.,_ FAISS) from a database.

 - Prompt generation: if soft prompt dimension is _d_ _f_, and each retrieved pair contributes _d_ -dim
vector, this yields a projection cost of _O_ ( _Rdd_ _f_ ).

 - The forecasting model remains frozen; only the soft prompt (a single vector of shape [1 _, d_ _f_ ] ) is
appended, incurring no extra Transformer-layer cost.

**Summary** . Pre-training (Transformer encoder) yields _O_ ( _L_ [2] _d_ ) per layer. Alignment yields _O_ ( _B_ [2] _d_ +
_B_ [2] _C_ [2] _d_ ) and RAG inference yields _O_ ( _Rdd_ _f_ ) for retrieval and projection.

**D.9.2** **Empirical Runtime**

We report the model size and empirical runtime of `TRACE` and other baselines in Table 12, including
FSCA [ 37 ], which is the second-best train-from-scratch time series model, and time series foundation
models with the availability of open-source implementations. `TRACE` activates only 0.12M parameters

24

during finetuning with a lightweight linear head, which is nearly 200Ã— fewer than FSCA and over
700Ã— fewer than Time-MoE small . This lightweight design results in substantially faster training and
inference speed. Compared to Moment, `TRACE` achieves faster training time with significantly fewer
trainable parameters and better performance, which can be attributed to its multichannel modeling
with channel-biased attention. While slightly slower than Timer-XL, which is a decoder-only model
with causal attention, `TRACE` offers an acceptable overhead given its significantly stronger retrieval
performance and the high quality of embeddings it produces for cross-modal and TS-to-TS retrieval.
It is worth noting that for Timer-XL and Time-MoE, despite their strong generalizability, parameterefficient finetuning strategies are relatively underexplored, as all model parameters must be activated
and updated during finetuning for reasonable performance in domain-specific tasks.

Table 12: Comparisons of model efficiency. Activated Params indicates the number of parameters
activated during finetuning for 7-step forecasting on the weather dataset. Training and inference time
are seconds per epoch on the forecasting dataset. Device is a single A100 40GB GPU.

**Total Params** **Activated Params** **Training Time** **Inference Time**

**FSCA** 82.35M 22.68M 1249.701 1.589

`TRACE` 10.78M 0.12M 6.054 0.955

**Moment** base 109.87M 0.24M 11.706 1.691
**Timer-XL** base 84.44M 84.44M 3.392 0.685
**Time-MoE** small 113.49M 113.49M 106.308 15.545

**E** **Discussion**

**Limitation** . While `TRACE` demonstrates strong performance in multimodal retrieval and retrievalaugmented forecasting, it currently assumes the availability of aligned time seriesâ€“text pairs during
training. In some domains, such alignment may be noisy or incomplete. Additionally, although
channel-level alignment improves interpretability and fine-grained matching, it introduces a modest
increase in computational overhead during training. We believe these trade-offs are justified by the
performance gains but acknowledge that further optimization may enhance scalability.

**Future Work** . In future work, we plan to extend `TRACE` to support weakly supervised and semisupervised settings, where textual context is partially missing or noisy. Another promising direction
is integrating domain adaptation techniques to improve generalization across unseen domains and
sensor modalities ( _e.g.,_ image, video). Moreover, exploring autoregressive generation conditioned on
retrieved time seriesâ€“text pairs may further enhance understanding tasks in temporal modeling.

**Broader Impact** . `TRACE` offers a general framework for cross-modal reasoning in time series
applications, with potential benefits in domains such as healthcare monitoring, disaster forecasting,
and industrial diagnostics. By improving retrieval and interpretation of structured temporal data, our
approach may enhance decision support and model transparency. However, we encourage responsible
deployment and emphasize the importance of auditing training data and retrieval outputs to avoid
amplifying biases present in either modality.

25

