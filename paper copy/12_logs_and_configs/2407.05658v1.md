**Random Features Hopfield Networks generalize retrieval to previously unseen examples**

Silvio Kalaj [1], Clarissa Lauditi [2], Gabriele Perugini [3] _[,]_ [4], Carlo Lucibello [3] _[,]_ [4], Enrico M. Malatesta [3] _[,]_ [4], Matteo Negri [1] _[,]_ [5] _[∗]_

1 _University of Rome ‘La Sapienza’, Department of Physics, Piazzale Aldo Moro 5, 00185 Roma, Italy_
2 _Department of Applied Math, John A. Paulson School of Engineering and Applied Sciences,_
_Harvard University, 02138 Cambridge, MA, USA_
3 _Department of Computing Sciences, Bocconi University, 20136 Milano, Italy_
4 _Institute for Data Science and Analytics, Bocconi University, 20136 Milano, Italy and_
5 _CNR-NANOTEC, Institute of Nanotechnology,_
_Rome Unit, Piazzale Aldo Moro, 00185 Roma, Italy_
(Dated: July 9, 2024)

It has been recently shown that a learning transition happens when a Hopfield Network stores
examples generated as superpositions of random features, where new attractors corresponding to
such features appear in the model. In this work we reveal that the network also develops attractors
corresponding to previously unseen examples generated with the same set of features. We explain
this surprising behaviour in terms of spurious states of the learned features: we argue that, increasing
the number of stored examples beyond the learning transition, the model also learns to mix the
features to represent both stored and previously unseen examples. We support this claim with the
computation of the phase diagram of the model.

_̸_

_̸_


The Hopfield Model [ 1 ] is a paradigmatic model of
associative memory with relevance in physics, biology,
and computer science. Starting from corrupted signals,
stored binary memories are retrieved as fixed points of a
dynamical system which is also an energy minimization
process. Recently, generalizations of the Hopfield Model _̸_
have gained attention thanks to the addition of several
desirable properties that nonetheless preserve the energy
minimization and the associative mapping paradigms. In
particular, Modern Hopfield Networks overcome the linear
(in the system size) capacity limit of the Hopfield Model
and are able to store a polynomial [ 2 – 4 ] or even expo- _̸_
nential [ 5 – 7 ] number of memories. Continuous variables
and differentiable update rules allow to plug in trainable
components in machine learning applications [ 6, 8 ]. A
Lagrangian formalism can be used to describe a large family of such models [ 9 ], also accommodating popular deep
learning components such as the attention mechanism

[10] and layer normalization [11].
In connection with recent advances in theoretical machine learning [ 12 – 14 ], it is shown in Ref. [ 15 ] that even
the standard Hopfield Model, which has no trainable parameters, when given memories generated by a latent
manifold, _**ξ**_ _ν_ = _σ_ ( _F_ _**c**_ _ν_ ), is able to "learn" essential features of the data generating process: when provided with
enough samples, the (unobserved) columns of _F_ become
attractors of the dynamics. In this work, we further characterize the Random Features Hopfield Model of Ref. [ 15 ].
In particular, we show that the model is also able to
"generalize", that is to store unobserved samples of the
data manifold. Spurious minima (mixtures in particular),
normally detrimental for the Hopfield Model, become
beneficial in this context.
_Random Features Hopfield Model._ Given _N_ binary
neurons _s_ _i_ = _±_ 1, and _P_ binary memories _{_ _**ξ**_ _ν_ _}_ _ν_ _[P]_ =1 [that]

_∗_ [Corresponding author; matteo.negri@uniroma1.it](mailto:matteo.negri@uniroma1.it)


_̸_

It can be shown [ 1 ] that as long as the memories are few
(at most _O_ ( _N_ )) and far apart enough, they approximately
correspond to fixed points of the dynamics and can be
retrieved from a perturbed configuration. The model
admits an energy function _H_ ( _**s**_ ) = _−_ 2 [1] � _i_ = _̸_ _j_ _[J]_ _[ij]_ _[s]_ _[i]_ _[s]_ _[j]_ [.]

The standard Hopfield Model setting considers random
uncorrelated memories with uniform components. Using
statistical physics techniques [ 16, 17 ], it has been shown
that the model is able to store up to _P ≈_ 0 _._ 138 _N_ memories
for large _N_ .
The Random Features Hopfield Model [ 15 ] keeps the
same update rule and couplings of eq. 1, but considers
a data structure given by a random projection of a _D_ dimensional latent space [13, 14, 18, 19]:

_**ξ**_ _ν_ = sign ( _F_ _**c**_ _ν_ ) _._ (2)

The matrix _F ∈{−_ 1 _,_ +1 _}_ _[N]_ _[×][D]_ has i.i.d. uniform components. We call _features_ its columns _**f**_ _k_ . The latents
vectors _**c**_ _ν_ _∈_ R _[D]_ are called _coefficients_ instead. We take
each _**c**_ _ν_ to have exactly _L_ non-zero entries, in random
locations and uniformly sampled in _±_ 1. We will discuss
two cases: the sparse case, _L_ = _O_ (1) as _D →∞_, and the
fully dense case, _L_ = _D_ .
In the dense case Ref. [ 15 ] shows that when a large
number of examples is given to the Random Features
Hopfield Model, beyond its storage capacity, it enters
a phase where features instead become attractors. The
model enters the learning phase if _α_ = _P/N_ is larger than
a critical value that depends on _α_ _D_ = _D/N_ . We refer to
this as the _learning transition_ .
Given that the examples are correlated, it is useful
to call _training example_, _**ξ**_ [train], any of the _P_ examples


we want to store, the Hopfield Model defines the sequential
update rule

_̸_

_̸_


� _J_ _ij_ _s_ [(] _j_ _[t]_ [)]

_j_ = _̸_ _i_

_̸_


_s_ [(] _i_ _[t]_ [+1)] = sign

_̸_

_̸_




_̸_

_̸_


_N_



�
 _j_ = _̸_ _i_

_̸_


_N_

_̸_

_̸_




_̸_

_̸_


_,_ _J_ _ij_ = [1]

_N_

_̸_ 

_̸_


_P_
� _ξ_ _νi_ _ξ_ _νj_ _._ (1)

_̸_ _ν_ =1

_̸_

FIG. 1. **Training and test examples become fixed points**
**after the features have been learned.** Magnetization as a
function of _α_, for fixed _α_ _D_ . The **blue** line is the magnetization
_µ_ of hidden features, which grows to 1 if _α_ is high enough
( _learning phase_ ). The **orange** line is the magnetization _m_ [train]

of the training examples, which is _m_ [train] _≃_ 1 for low _α_ and
drops when _α_ increases, as expected from an associative memory ( _storage phase_ ). Surprisingly, _m_ [train] grows to 1 again for
high values of _α_ . Near this transition, also test examples have
_m_ [test] = 1, as shown by the **red** line ( _generalization phase_ ).
_N_ = 32000; averages of 40 samples. The dashed line shows
the analytical prediction for the magnetization of mixtures of
3 features.


2

a learning transition even when it is trained with a sparse
combination of features (see Fig. 1, blue line), extending
the result of [ 15 ]. The position of the learning transition
depends weakly (if at all) on the number _L_ of features
per example (see dotted lines in Fig. 2a).

Surprisingly, after the features have been learned, the
model enters a phase in which the training examples are
stable again (see Fig. 1, orange line). We note that it is
physically implausible that a phase disappears for low _α_
and reappears when _α_ is large. To understand what is
happening, we check the magnetization of test examples:
we find that they become fixed points together with the
training examples. We call this the _generalization phase_,
as it resembles the behavior of inference models that
perform well on previously unseen examples. The fact
that both train and test examples are fixed points for
high _α_, while only training examples are fixed points for
low _α_, indicates that the model must be using different
mechanisms to achieve these results.


that are used in the Hebb rule defining the couplings
matrix _J_ in (1) . We also call _test example_, _**ξ**_ [test], any
linear combination (followed by the sign activation) of
the features that is not used in _J_ . Note that in principle
we could have different values of _L_ in training and test
examples. To keep the numerical analysis simple, we
only consider _L_ train = _L_ test = _L_ . We can now say that
an Hopfield Model _generalizes_ if test examples are fixed
points of the update rule (1) . In the same spirit of the
learning phase in [ 15 ], we study whether the network
enters a _generalization phase_ in some region of _α_, _α_ _D_ and
_L_ space.

_Generalization transition._ The numerical results of
this work are all measures of _magnetizations_, obtained
in the following way. First, we initialize the model to
the configuration _**s**_ [(0)] whose stability we want to check
(we consider _**s**_ [(0)] = _**ξ**_ [train], _**s**_ [(0)] = _**ξ**_ [test], and _**s**_ [(0)] = _**f**_,
with _**f**_ any of the columns of _F_ ). Then, we run the
update rule (1) until we reach a fixed point ˜ _**s**_ . Finally,
we compute the magnetization as the normalized scalar
product between the fixed point and the initial condition:
_m_ [train] = _N_ 1 � _Ni_ =1 _[s]_ [˜] _[i]_ _[ξ]_ _i_ [train], _m_ [test] = _N_ 1 � _Ni_ =1 _[s]_ [˜] _[i]_ _[ξ]_ _i_ [test], and

_N_

_µ_ = _N_ [1] � _i_ =1 _[s]_ [˜] _[i]_ _[f]_ _[i]_ [ respectively. If, for given] _[ α]_ [ and] _[ α]_ _[D]_ [, we]

find that the magnetization is close to 1, we say that there
is a fixed point of eq. (1) that corresponds to _**ξ**_ [train], _**ξ**_ [test]

or **f** respectively.

The first result that we present is that the model shows


In Fig. 2a we show generalization transitions (solid lines)
for different values of _L_ and _α_ _D_ . We see that increasing
either _L_ or _α_ _D_ has the effect of moving the generalization
transition to higher values of _α_ . Additionally, in Fig. 2b we
study the maximum number of features _D_ gen compatible
with a generalization transition for a given value of _α_ . We
see that is _D_ gen = _O_ ( _N_ ) in the sparse case _L_ = _O_ (1) and
_D_ gen = _O_ ( _√N_ ) in the dense case _L_ = _D_ . This scaling

is compatible with the signal-to-noise analysis discussed
below.

_Theoretical results._ We now provide a theoretical
framework to explain how the Random Features Hopfield
Model stores examples that it never saw before. First,
we provide a simple signal-to-noise argument to get an
intuition on why denser combination require more training examples, as shown in Fig. 2. This argument shows
that we can interpret the generalization transition for a
given _L_ as the mixtures of _L_ features becoming a fixed
point of 1. Then, we test this interpretation by resorting to the statistical physics formulation of the Random
Features Hopfield Model [ 15 ] and computing the spinodal
lines of mixtures of features. Note that, to do this, we
consider dense train examples _L_ train = _D_, since treating
sparse training examples would be much harder and is
outside of the scope of this work. This simplification
is justified by the surprising numerical result that the
learning transition depends weakly on _L_ (see Fig. 2a).
These assumpions prove to be consistent, as we see that
the spinodal lines predict accurately the generalization
transitions (see Fig. 1 and Fig. 2a).

_Signal-to-noise analysis._ Let us define a mixture of
features as _χ_ _νi_ = [�] _k_ _[′′]_ _[ c]_ _[νk]_ _[′′]_ _[f]_ _[k]_ _[′′]_ _[i]_ _[,]_ [ where at this level the]

coefficients can be either dense or sparse (for simplicity
we set _σ_ ( _x_ ) = _x_ just for this analysis). The local field on a
mixture can be written (see appendix A in supplemental
material (SM)) as a signal term, proportional to the
mixture itself, plus "crosstalk" terms that include the

3







|Col1|L = 3 = 5<br>L = 3 = 1<br>L = D = 5<br>L = D = 1<br>N<br>N1/2|Col3|
|---|---|---|
||L = 3 = 5<br>L = 3 = 1<br>L = D = 5<br>L = D = 1<br>N<br>N1/2||


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||= 0.003<br>D<br>L = 3<br>L = 5<br>L = 7|= 0.003<br>D<br>L = 3<br>L = 5<br>L = 7|
|= 0.005<br>D<br>L = 3<br>L = 5<br>L = 7<br>10 2 10 1<br>model load =|= 0.005<br>D<br>L = 3<br>L = 5<br>L = 7|= 0.005<br>D<br>L = 3<br>L = 5<br>L = 7|= 0.005<br>D<br>L = 3<br>L = 5<br>L = 7|||||
|= 0.005<br>D<br>L = 3<br>L = 5<br>L = 7<br>10 2 10 1<br>model load =|= 0.005<br>D<br>L = 3<br>L = 5<br>L = 7|= 0.005<br>D<br>L = 3<br>L = 5<br>L = 7|= 0.005<br>D<br>L = 3<br>L = 5<br>L = 7|P/N|P/N||100|


FIG. 2. **Combinations of more features require more training examples.** a) Feature magnetization _µ_ ( _dotted_ ) and
test examples magnetization _m_ [test] ( _solid_ ) as a function of _α_ for different _α_ _D_ (subplots). Different colors represent increasing
features per example _L_ . Dashed vertical lines are the analytical predictions: in black we show the learning transition; colors
correspond to the transitions for mixtures of _L_ = 3 _,_ 5 _,_ 7 features. ( _N_ = 32000; averages of 40 samples.) b) Scaling with _N_ of the
maximum number of features _D_ gen for which we observe a generalization transition. Specifically, we plot the maximum _D_ ( _N_ ) at
which 10 samples have _m_ [test] _>_ 0 _._ 9, and we average over 4 to 10 groups of 10 samples, depending on _N_ .


noise coming from the other features:


To identify phase transitions we write the average free

energy


��


_N_
�


_D_


� _J_ _ij_ _χ_ _νj_ = _D_ _[P]_

_j_ =1


_LD_


�


_ϕ_ = lim (4)
_N_ _→∞_ _[−]_ _βN_ [1] _[⟨]_ [ln] _[ Z][⟩]_ _[c,f]_ _[,]_


_χ_ _νi_ + _O_

�


_N_


�


��


where _Z_ = [�]


where _Z_ = [�] _**s**_ [exp] [(] _[−][β][H]_ [(] _**[s]**_ [))][ is the partition function]

and _⟨...⟩_ _c,f_ is the average over the coefficients _c_ and the
features _f_ . To compute _⟨_ ln _Z⟩_ _c,f_ we use the replica
method [ 20 ], namely we represent the logarithm as
_⟨_ ln _Z⟩_ = lim _n→_ 0 ( _⟨Z_ _[n]_ _⟩−_ 1) _/n_ . After inserting delta functions for the definition of the example magnetizations
_m_ _[a]_ _ν_ [=] [1] � _i_ _[ξ]_ _[νi]_ _[s]_ _i_ _[a]_ [, the replicated partition function reads]


+ _O_


� ~~�~~


_L_ [3]

_DP_


+ _O_


��


_L_ [3]

_NP_


_._ (3)


Now we can see that, if _L_ = _O_ (1), the second and third
noise terms go to zero when _N, D →∞_ ; then, to keep
the first noise term finite, we must set _D_ = _O_ ( _N_ ). This
means that the retrieval of (sparse) mixtures happens
at _α >_ 0. On the contrary, it is impossible to retrieve
(dense) mixtures at finite _α_ when _L_ = _D_ = _O_ ( _N_ ), since
the signal would be overwhelmed by noise. It becomes
possible instead when _L_ = _D_ = _O_ ( _N_ [1] _[/]_ [2] ), meaning _α_ = 0:
in this regime, the first noise term is finite again (as well
as the second and third ones). This scaling is confirmed
by numerical results (see Fig. 2b).
_Replica analysis._ We extend the calculation reported
in [ 15 ] for the single-feature retrieval to the case of retrieval of mixtures of features. Here we sketch the main
steps, describing the full derivation in appendix C (SM).
We are interested in predicting the location of the phase
transition characterized by a discontinuity of the feature
magnetizations _**µ**_ = _{µ_ [1] _, ..., µ_ _[D]_ _}_ . Given that we have one
order parameter for each feature, we need to make an
ansatz on the structure of the magnetizations to make the
problem treatable. The single-feature retrieval consists in
looking in a solution of the form _**µ**_ = _{µ,_ 0 _, ...,_ 0 _}_, where
one magnetization remains finite and the others vanish
in the thermodynamic limit. Here, instead we consider
_L_ finite magnetizations, namely _**µ**_ = _{µ_ 1 _, ..., µ_ _L_ _,_ 0 _, ...,_ 0 _}_ .


_N_ � _i_ _[ξ]_ _[νi]_ _[s]_ _i_ _[a]_ [, the replicated partition function reads]


_dm_ _[a]_ _ν_ _β_ 2 � _Pν_ =1 � _na_ =1 [(] _[m]_ _ν_ _[a]_ [)] [2]
_√_ 2 _π_ _[e]_


_⟨Z_ _[n]_ _⟩_ = _e_ _[−]_ _[β]_ 2 _[P n]_ [ �]

_{_ _[s]_ _i_ _[a]_ _[}]_


�� _νa_


�


1

_√D_
�


_D_
� _c_ _νk_ _f_ _ki_

_k_ =1


�


_._

�� _c,f_


_×_


_δ_

�� _νa_


_m_ _[a]_ _ν_ _[−]_ [1]


_N_


_N_

_σ_

�

_i_ =1


_s_ _[a]_ _i_


(5)

Following [ 15 ], to proceed we need to isolate the finite
terms inside the function _σ_ . Without loss of generality we
can assume that the non-vanishing magnetizations correspond to the first _L_ values of the index _k_, so that we integrate away the coefficients _c_ _L_ +1 _, ..., c_ _D_ . Compared with
the single-retrieval calculation, the resulting distribution
of _m_ _[a]_ _ν_ [is also a Gaussian] _[ N]_ [(] _[m]_ _[ν]_ [;] [ ¯] _[m, Q]_ [)][ with the same co-]
variance _Q_, but a different mean ¯ _m_ = ~~_√_~~ _κα_ 1 _D_ � _Lk_ =1 _[c]_ _[νk]_ _[µ]_ _k_ _[a]_ _[,]_

where _κ_ 1 = � ~~_√_~~ _dz_ 2 _π_ _[e]_ _[−][z]_ [2] _[/]_ [2] _[ z σ]_ [(] _[z]_ [)][. See appendix][ B][ (SM)]

for the explicit derivation, which can also be seen as
a specialization of the Gaussian Equivalence Theorem

[12–14, 19, 21, 22].

4

this transition happens are plotted in Fig. 3 for _L_ = 3 _,_ 5 _,_ 7
(for binary neurons, mixtures are known to be unstable for
even _L_ since [ 16, 17 ]). For reference, in the vertical panel
we also show the phase diagram of the standard Hopfield
Model, showing that the lines of the Random Features
Hopfield Model correctly converge to the corresponding
critical points at zero temperature. The spinonal lines
predict with good agreement the generalization transitions
i ~~d~~ entified with the numerical experiments in Fig. 1 and
in Fig. 2a.
_Discussion._ The ability of a Hopfield Network to store
combinations of examples has been known since [16, 17],
but it gains a new framework in the light of the learning
transition: if the features hidden in the examples become
attractors, the network can combine them to produce new
attractors that will recognize all the possible examples
generated with the same features (regardless of whether
they were used to train the model or not). Note that one
shortcoming of the learning phase is that, while the model
is able to retrieve features if initialized close enough to
one of them, there is no known way to find features if no
information is known, since the model converges to a spinglass state with high probability if initialized at random.
The generalization phase circumvents this problem: the
model builds local minima for all the possible mixtures
features (up to a certain _L_ ), so that a test example is
recognized as a stable configuration without any explicit
information about the underlying features.
The scheme for generalization presented in this work
could be explored in more advanced architectures. Of
particular importance is the exponential version of the
Hopfield Model [ 5 – 7 ]: given its connection with the crossattention mechanism, the learning and generalization
transitions might play a role in the forward pass of transformers architectures [ 10 ]. Furthermore, the exponential
Hopfield Model has been connected with diffusion models

[ 23 ]: since the temperature can be mapped to the diffusion time, studying the stability of mixtures of features at
finite temperatures might connect to the results described
in [24].
Note that the critical lines of mixtures for the standard
Hopfield Model that we reported in the upper panel of
Fig. 3 were previously unknown in the literature except
for their intercepts with the axes. This is because the
standard calculation for symmetric mixtures leads to nonphysical results at low load and high temperature, as it
was already noted in [ 16 ]. Since it is outside of the scope
of this work, we report the calculation in [25].
On the technical side, the fact that the theory for the
dense case _L_ train = _D_ matches the numerical results in
the sparse case _L_ train = _O_ (1) is unexpected and further
inquiry on this property is desirable. In particular, it
might mean that an extension of the Gaussian Equivalence
Theorem might hold in the case of sparse coefficients.
_Acknowledgements._ MN acknowledges the support of
PNRR MUR project PE0000013-FAIR. E.M.M. acknowledges the MUR-Prin 2022 funding Prot. 20229T9EAT,
financed by the European Union (Next Generation EU).












FIG. 3. **Comparison between the phase diagrams** of a
standard Hopfield Model from [ 16 ] ( _top_, temperature _T_ vs _α_ )
and the phase diagram of the dense Random Features Hopfield
Model ( _bottom_, _α_ vs _α_ _D_ ). In both panels, the **blue** line is
the retrieval line, below which the features can be stored and
retrieved. The **red**, **green** and **yellow** lines are the retrieval
lines of mixtures, respectively of 3, 5 and 7 examples. When
_α →∞_, the curves of the Random Features Hopfield Model
connect to the critical points at _T_ = 0 in the Hopfield Model:
high order mixtures are stable at lower _α_ _D_ . We also show how
these critical points evolve at finite temperature.

We end up with the following expression for the replicated partition function:


_dm_ _[a]_ _ν_
_√_ 2 _π_


_⟨Z_ _[n]_ _⟩_ = _e_ _[−]_ _[β]_ 2 _[P n]_ [ �]

_{_ _[s]_ _i_ _[a]_ _[}]_


�� _νa_


_β_

� exp

_νa_ �


_×_
�


_β_

2 [(] _[m]_ _ν_ _[a]_ [)] [2] � _⟨N_ ( _m_ _[a]_ _ν_ [; ¯] _[m, Q]_ [)] _[⟩]_ _c_ 1 _,...,c_ _L_ _,f_ (6)


where _⟨...⟩_ represents the average over _f_ and the remaining
coefficients _c_ _k_ with _k_ = 1 _, ..., L_ . We solve this model in
the limit _β →∞_ in the replica-symmetric ansatz and for
symmetric mixtures, namely


_D−L_

~~��~~ � ~~�~~
0 _, ...,_ 0 _}._ (7)


_**µ**_ = _{_


_L_

~~��~~ � ~~�~~
_µ, ..., µ,_


From _⟨Z_ _[n]_ _⟩_ we obtain the free energy _ϕ_ as a function
of _µ_ and other order parameters, for any value of _L_, _α_
and _α_ _D_ . Then, we extremize it with respect to all the
order parameters and we find the transition by looking
for the point at which _µ_ jumps from zero to a finite value,
meaning that the symmetric mixtures become a local
minimum of the free energy (see appendix D in SM for the
explicit derivation). The lines in the _α_ _D_ _, α_ space where

[1] John J Hopfield. Neural networks and physical systems
with emergent collective computational abilities. _Proceed-_
_ings of the national academy of sciences_, 79(8):2554–2558,
1982.

[2] E Gardner. Multiconnected neural network models. _Jour-_
_nal of Physics A: Mathematical and General_, 20(11):3453,
1987.

[3] Dmitry Krotov and John J Hopfield. Dense associative
memory for pattern recognition. _Advances in neural in-_
_formation processing systems_, 29, 2016.

[4] Elena Agliari, Linda Albanese, Francesco Alemanno,
Andrea Alessandrelli, Adriano Barra, Fosca Giannotti,
Daniele Lotito, and Dino Pedreschi. Dense hebbian neural networks: a replica symmetric picture of supervised
learning. _Physica A: Statistical Mechanics and its Appli-_
_cations_, 626:129076, 2023.

[5] Mete Demircigil, Judith Heusel, Matthias Löwe, Sven
Upgang, and Franck Vermet. On a model of associative
memory with huge storage capacity. _Journal of Statistical_
_Physics_, 168:288–299, 2017.

[6] Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner,
Philipp Seidl, Michael Widrich, Thomas Adler, Lukas
Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil
Sandve, et al. Hopfield networks is all you need. _arXiv_
_preprint arXiv:2008.02217_, 2020.

[7] Carlo Lucibello and Marc Mézard. The exponential capacity of dense associative memories. _arXiv preprint_
_arXiv:2304.14964_, 2023.

[8] Dmitry Krotov. A new frontier for hopfield networks.
_Nature Reviews Physics_, pages 1–2, 2023.

[9] Dmitry Krotov and John J. Hopfield. Large associative
memory problem in neurobiology and machine learning.
In _International Conference on Learning Representations_,
2021.

[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. _Advances_
_in neural information processing systems_, 30, 2017.

[11] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.
Layer normalization. _arXiv preprint arXiv:1607.06450_,
2016.

[12] Song Mei and Andrea Montanari. The generalization
error of random features regression: Precise asymptotics
and the double descent curve. _Communications on Pure_
_and Applied Mathematics_, 75(4):667–766, 2022.

[13] Sebastian Goldt, Marc Mézard, Florent Krzakala, and
Lenka Zdeborová. Modeling the influence of data structure
on learning in neural networks: The hidden manifold
model. _Physical Review X_, 10(4):041044, 2020.

[14] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc
Mézard, and Lenka Zdeborová. Generalisation error in
learning with random features and the hidden manifold
model. In _International Conference on Machine Learning_,
pages 3452–3462. PMLR, 2020.

[15] Matteo Negri, Clarissa Lauditi, Gabriele Perugini, Carlo
Lucibello, and Enrico Malatesta. Storage and learning
phase transitions in the random-features hopfield model.
_arXiv preprint arXiv:2303.16880_, 2023.

[16] Daniel J Amit, Hanoch Gutfreund, and Haim Sompolinsky. Statistical mechanics of neural networks near saturation. _Annals of physics_, 173(1):30–67, 1987.


5

[17] Daniel J Amit, Hanoch Gutfreund, and Haim Sompolinsky. Information storage in neural networks with low
levels of activity. _Physical Review A_, 35(5):2293, 1987.

[18] Ali Rahimi and Benjamin Recht. Random features for
large-scale kernel machines. In _Proceedings of the 20th In-_
_ternational Conference on Neural Information Processing_
_Systems_, pages 1177–1184, 2007.

[19] Carlo Baldassi, Clarissa Lauditi, Enrico M. Malatesta,
Rosalba Pacelli, Gabriele Perugini, and Riccardo Zecchina.
Learning through atypical phase transitions in overparameterized neural networks. _Phys. Rev. E_, 106:014116, Jul
2022.

[20] Marc Mézard, Giorgio Parisi, and Miguel Angel Virasoro.
_Spin glass theory and beyond: An Introduction to the_
_Replica Method and Its Applications_, volume 9. World
Scientific Publishing Company, 1987.

[21] Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent
Krzakala, Marc Mézard, and Lenka Zdeborová. The gaussian equivalence of generative models for learning with
shallow neural networks. In _Mathematical and Scientific_
_Machine Learning_, pages 426–471. PMLR, 2022.

[22] Hong Hu and Yue M Lu. Universality laws for highdimensional learning with random features. _IEEE Trans-_
_actions on Information Theory_, 2022.

[23] Luca Ambrogioni. In search of dispersed memories: Generative diffusion models are associative memory networks.
_arXiv preprint arXiv:2309.17290_, 2023.

[24] Giulio Biroli, Tony Bonnaire, Valentin De Bortoli, and
Marc Mézard. Dynamical regimes of diffusion models.
_arXiv preprint arXiv:2402.18491_, 2024.

[25] Silvio Kalaj and Matteo Negri. Manuscript in preparation.

6

**CONTENTS**

References 5

A. Signal-to-noise analysis 6
1. Retrieval of a feature with _L_ = _O_ ( _D_ ) coefficients 6
2. Retrieval of a feature with _L_ = _O_ (1) coefficients 7
3. Retrieval of a mixture of features with _L_ = _O_ (1) coefficients 7

B. Extension of the Gaussian Equivalence Theorem for mixtures of features 7

C. Replica-symmetric calculation for mixtures of features 9
1. Retrieval of a mixture of _L_ features 9
2. Average over coefficients matrix c 9
3. Integrating pattern magnetizations 10
4. Integrating the feature magnetizations 10
5. Summary expression of the partition function 12
6. RS ansatz 12
7. Saddle-point equations 16

D. Specialization for symmetric mixtures 16
1. Zero-temperature limit 17
2. Numerical analysis of saddle-point equations 18
a. Iterative method 18
b. Phase diagram 18

**Appendix A: Signal-to-noise analysis**

In this section we derive eq. 3. To do this, we first do the signal-to-noise analysis for the retrieval of a feature in the
dense case, so that we understand the meaning of the various noise terms in a case for which we already know that the
retrieval of the features happens at _α_ _D_ _>_ 0 [15].
Then, we move to the sparse case and we argue that the retrieval of features is still possible and depends weakly on
_L_ . This exercise also allows us to set up the final case of the retrieval of a mixture of _L_ features, since the steps are
the same except for the very last one.

**1.** **Retrieval of a feature with** _L_ = _O_ ( _D_ ) **coefficients**

In the fully dense case we study the signal-to-noise ratio of a single feature retrieval, being the local field

_h_ _ki_ = � _J_ _ij_ _f_ _kj_

_j_


= [1]

_N_


�

_j_


� _ξ_ _µi_ _ξ_ _µj_ _f_ _kj_

_µ_


_f_ _k_ _′′_ _j_ _f_ _kj_

_j_


(A1)


� [] 


 [�] _j_


1

=
_ND_


�

_k_ _[′′]_


� _f_ _k_ _′_ _i_

_k_ _[′]_


_c_ _µk_ _′_ _c_ _µk_ _′′_

�� _µ_







= [1]

_D_


�

_k_ _[′′]_


� _f_ _k_ _′_ _i_ � _Pδ_ _k_ _′_ _k_ _′′_ + (1 _−_ _δ_ _k_ _′_ _k_ _′′_ ) _O_ ( _P_ [1] _[/]_ [2] )�� _δ_ _k_ _′′_ _k_ + (1 _−_ _δ_ _k_ _′′_ _k_ ) _O_ ( _N_ _[−]_ [1] _[/]_ [2] )�

_k_ _[′]_


which is separated into the true signal (contributing to the retrieval) for the diagonal term, while the rest is noise. By
making the orders of different contributions explicit, we have


� ~~�~~


_h_ _ki_ = _[P]_

_D_


_f_ _ki_ + _O_

�


_D_

_N_


�


�


~~�~~


_D_

_P_


��


+ _O_


��


_D_

_P_


+ _O_


��


_D_

_N_


(A2)

7

where we collected the order of the signal. This is compatible with the learning phase described in [ 15 ] for finite values
of _α_ = _[P]_ _[D]_

_N_ [and] _[ α]_ _[D]_ [ =] _N_ [: in fact, taking] _[ D]_ [ =] _[ O]_ [(] _[N]_ [)][ and] _[ P]_ [ =] _[ O]_ [(] _[N]_ [)][ makes all the noise terms finite in the limit] _[ N][ →∞]_ [.]

Fixing the scaling of the lower order noises also guarantees that the higher orders one are finite in the thermodynamic
limit.

**2.** **Retrieval of a feature with** _L_ = _O_ (1) **coefficients**

For the sparse case, instead,


_h_ _k_ _′′_ _i_ =


_N_
� _J_ _ij_ _f_ _k_ _′′_ _j_

_j_ =1


= [1]

_N_

= [1]

_L_


�

_kk_ _[′]_


_δ_ _kk_ _′_ _P_ _[L]_ _√_
� _D_ [+ (1] _[ −]_ _[δ]_ _[kk]_ _[′]_ [)] _[O]_ �


_δ_ _kk_ _′_ _P_ _[L]_
� _D_


�

_j_


�

_µ_


1

_√L_
�


� _c_ _µk_ _f_ _ki_

_k_


1

_√L_
��


� _c_ _µk_ _′_ _f_ _k_ _′_ _i_

_k_ _[′]_


�


�


_f_ _k_ _′′_ _j_


(A3)


_L_ 2
_P_
� _D_ [2]


_L_ 2
_P_
� _D_


1
_δ_ _k_ _′_ _k_ _′′_ + (1 _−_ _δ_ _k_ _′_ _k_ _′′_ ) _O_
�� � _√N_


1
_δ_ _k_ _′_ _k_ _′′_ + (1 _−_ _δ_ _k_ _′_ _k_ _′′_ ) _O_
�� � _√_


_f_ _ki_ _._
��


where now the patterns in _N_ -dimension are properly scaled with _√_


_L_ instead of _√_


where now the patterns in _N_ -dimension are properly scaled with _√L_ instead of _√D_ . Solving for the four cases and

rescaling with the signal (feature) order we have


_D_ [2]

_PN_

��


��


�


+ _[L]_

_D_ _[O]_


�


+ _[L]_

_D_ _[O]_


_h_ _k_ _′′_ _i_ = _[P]_

_D_


_f_ _k_ _′′_ _i_ + _O_

�


_D_

_N_


+ _[L]_


��


_D_

_P_


+ _[L]_


��


(A4)


which provides additional insight that the learning transition also exists in the sparse case at finite _α_ and that, as long
as _L_ = _O_ (1) and _N →∞_, _D_ = _α_ _D_ _N_, _P_ = _αN_ the transition seems to weakly depend on _L_ (see also Fig. 2a), since
the noise terms depending on _L_ vanish in this limit.

**3.** **Retrieval of a mixture of features with** _L_ = _O_ (1) **coefficients**

We report the steps for eq. (3) in the main text. Being the mixture of features defined as _χ_ _νi_ = [�] _k_ _[′′]_ _[ c]_ _[νk]_ _[′′]_ _[f]_ _[k]_ _[′′]_ _[i]_ [, we]

have


_h_ _νi_ =


_N_
� _J_ _ij_ _χ_ _µj_

_j_ =1


= � _c_ _µk_ _′′_

_k_ _[′′]_


_N_
� _J_ _ij_ _f_ _k_ _′′_ _i_

_j_ =1







= � _c_ _µk_ _′′_

_k_ _[′′]_





 _N_ [1]


�

_µ_


1

_√L_
�


� _c_ _µk_ _f_ _ki_

_k_


�


1

_√L_
��


1

_√_
��


� _c_ _µk_ _′_ _f_ _k_ _′_ _i_

_k_ _[′]_


�


(A5)


_N_


�

_j_


_f_ _k_ _′′_ _j_


_D_

_P_

�


��


� _c_ _µk_ _′′_ _D_ _[P]_

_k_ _[′′]_


�


+ _[L]_

_D_ _[O]_


��


+ _[L]_

_D_ _[O]_


_D_ [2]

� ~~�~~ _PN_


=
�


_D_


��

��


_D_

_N_


�


_f_ _k_ _′′_ _i_ + _O_


��


� ~~�~~


+ _O_

�


_L_ [3]

_NP_


_._

��


= _[P]_

_D_


_c_ _µk_ _′′_ _f_ _k_ _′′_ _i_ + _O_

�� _k_ _[′′]_


_LD_

_N_


+ _O_

�


_L_ [3]

_DP_


**Appendix B: Extension of the Gaussian Equivalence Theorem for mixtures of features**

We are interested in the retrieval of a mixtures of features, hence, we consider the case of _L_ patterns condensed.
The average of the delta function inside equation (5) yields the probability distribution of the pattern magnetizations
_m_ _[a]_ _ν_ [given the feature matrix.]

Assuming that


1

_D_


8

_D_
� _f_ _ki_ [2] [= 1] _[,]_ _∀i,_ (B1a)

_k_ =1


1

_√D_


_D_
� _f_ _ki_ _f_ _kj_ = _O_ (1) _,_ _∀i ̸_ = _j,_ (B1b)

_k_ =1


the **Gaussian Equivalence Theorem** [ 12 – 14, 19, 21, 22 ], in the case of mixtures of features, states that the probability
distribution of the patterns magnetizations is a multivariate Gaussian


1
_P_ ( _{m_ _[a]_ _ν_ _[}|][c]_ _[ν]_ [1] _[, ..., c]_ _[νL]_ _[, f]_ _[ki]_ _[, s]_ _[a]_ _i_ [) =] _√_ 2 _π_ det **Q** exp � _−_ [1] 2


�( _m_ _[a]_ _ν_ _[−]_ _[m]_ [¯] _[a]_ [)(] **[Q]** _[−]_ [1] [)] _[ab]_ [(] _[m]_ _[b]_ _ν_ _[−]_ _[m]_ [¯] _[b]_ [)] _,_ (B2)

�
_a,b_


where the difference with the single feature retrieval case is the mean magnetization ¯ _m_ _[a]_ .
We can compute it using the Fourier representation of the delta function


(B3)


1
_i_ _[v]_ _i_ _[ν]_ _[v]_ [ˆ] _i_ _[ν]_
� _√N_


1
_i_ _[v]_ _i_ _[ν]_ _[v]_ [ˆ] _i_ _[ν]_
� _√_


1
_i_ _s_ _[a]_ _i_ _[σ]_ � _v_ _i_ _[ν]_ [+] _√_


_D_


_m_ ¯ _[a]_ _ν_ _[≡]_ � _m_ _[a]_ _ν_ �


_N_
_c_ [=] �
�

_i_ =1


_N_
_c_ [=] �
�


_dv_ _i_ _[ν]_ _[d][v]_ [ˆ] _i_ _[ν]_
_e_ _[i]_ [ �]
2 _π_


�


_L_
� _c_ _νk_ _f_ _ki_

_k_ =1 ��


_v_ ˆ _i_ _[ν]_ _[f]_ _[ki]_

�� � [�]

_i_ _c_


_v_ ˆ _i_ _[ν]_ _[f]_ _[ki]_
� [�]
_i_ _c_


_,_

_c_ _νk_


_×_
�

_k>L_


exp _−i_ _[c]_ _[νk]_
� _√D_


the coefficients matrix elements are distributed as i.i.d. Gaussian variable, therefore the average over the _c_ _νk_
corresponding to uncondensed features is


�

_ij_


ˆ
_v_ _i_ _[ν]_ _[f]_ _[ki]_ = exp _−_ [1]

�� _i_ � [�] _c_ _νk_ � 2


� _f_ _ki_ _f_ _kj_ � _v_ ˆ _i_ _[ν]_ _[v]_ [ˆ] _j_ _[ν]_ � (B4)

_k>L_


�

_k>L_


exp _−i_ _[c]_ _[νk]_
� _√D_


� _D_ 1


so that


� _c_ _νk_ _f_ _ki_

_k_ =1


_L_
�


_m_ ¯ _[a]_ _ν_ [=] 1
_√N_


� _s_ _[a]_ _i_

_i_


1
_Dv σ_ _v_ _i_ _[ν]_ [+]
� � _√D_


_≃_
�


(B5)

_f_ _ki_ _s_ _[a]_ _i_
�
_i_


_≃_ _[k]_ [0]
_√N_


_k_ 1
_i_ _s_ _[a]_ _i_ [+] _√_


�


_ND_


_L_
�


� _c_ _νk_ ��

_k_ =1 _i_


where we have used the definitions


_k_ 0 _≡_ _Dz σ_ ( _z_ ) _,_ (B6a)
�

_k_ 1 _≡_ _Dz z σ_ ( _z_ ) = _Dz σ_ _[′]_ ( _z_ ) (B6b)
� �


_−_ [1]
2 _π_ [exp] � 2


_dv_
_Dv ≡_
_√_ 2



[1] _•_

(B6c)
2 _[v]_ [2] �


Introducing the magnetization of the condensed features defined by


_µ_ _[a]_ _k_ [=] _N_ [1]


� _f_ _ki_ _s_ _[a]_ _i_ _[,]_ _k_ = 1 _, . . ., L_ (B7)

_i_


and considering the case of odd activation function _σ_ ( _z_ ) so that _k_ 0 = 0, we get


¯ _k_ 1
_m_ _[a]_ _ν_ [=]
_√α_ _D_


_L_
� _c_ _νk_ _µ_ _[a]_ _k_ _[.]_ (B8)

_k_ =1


The covariance matrix can be shown to be the same as in the single feature retrieval case and therefore is given by

_Q_ _ab_ = � _m_ _[a]_ _ν_ _[m]_ _[b]_ _ν_ �


_c_ � _m_ _[b]_ _ν_ �


9

_c_ [=] _[ k]_ _∗_ [2] _[q]_ _[ab]_ [ +] _[ k]_ 1 [2] _[p]_ _[ab]_ (B9)


_c_ _[−]_ � _m_ _[a]_ _ν_ �


where


_q_ _[ab]_ = [1]

_N_


_p_ _[ab]_ = [1]

_D_


_N_
� _s_ _[a]_ _i_ _[s]_ _[b]_ _i_ (B10)

_i_ =1

_D_
� _µ_ _[a]_ _k_ _[µ]_ _[b]_ _k_ (B11)

_k>L_


1
_µ_ _[a]_ _k_ [=]
_√N_


_N_
� _f_ _ki_ _s_ _[a]_ _i_ _[,]_ _k > L_ (B12)

_i_ =1


**Appendix C: Replica-symmetric calculation for mixtures of features**

**1.** **Retrieval of a mixture of** _L_ **features**

We substitute the multivariate Gaussian distribution of the pattern magnetizations _m_ _[a]_ _ν_ [and insert them in the]
integral using delta functions, we get


�

_ab_


�

_k,a_

�


�

_ab_


d _m_ _[a]_ _ν_
_√_ 2 _π_


� _Z_ _[n]_ [�] = _e_ _[−]_ _[β]_ 2 _[nP]_ [ �]

_{s_ _[a]_ _i_ _[}]_


�� _νa_


d _q_ _[ab]_ _dq_ ˆ _[ab]_

2 _π_


d _p_ _[ab]_ _dp_ ˆ _[ab]_

2 _π_


d _µ_ _[a]_ _k_ _[d][µ]_ [ˆ] _[a]_ _k_

2 _π_


_c_ _ν_ 1 _,...,c_ _νL_


�

_a_


_×_
�

_ν_


_β_
exp
� � 2


2 [��]
_m_ _[a]_ _ν_ [+ ¯] _[m]_ _ν_ _[a]_
� �


_×_
�� _ν_


1
_−_ [1]
_√_ 2 _π_ det **Q** exp � 2


� _m_ _[a]_ _ν_ [(] **[Q]** _[−]_ [1] [)] _[ab]_ _[m]_ _ν_ _[b]_

_a,b_


(C1)


_×_ exp _−_ _αN_ �
�


ˆ
_−_ [1]

� _q_ _[ab]_ ( _q_ _[ab]_ _N_

_a<b_


ˆ
_−_ [1]

� _p_ _[ab]_ ( _p_ _[ab]_ _D_

_a≤b_


_D_


_N_


�


_s_ _[a]_ _i_ _[s]_ _[b]_ _i_ [)] _[ −]_ _[αN]_ �

_i_ _a_ _b_


� _µ_ _[a]_ _k_ _[µ]_ _k_ _[b]_ [)]

�
_k>L_


� exp � _iµ_ ˆ _[a]_ _k_ � _µ_ _[a]_ _k_ _[−]_ _√_ 1

_k>L,a_


_×_
�


_N_


1 _≤_ � _k≤L,a_ exp � _iµ_ ˆ _[a]_ _k_ � _µ_ _[a]_ _k_ _[−]_ _N_ [1]


_N_


� _f_ _ki_ _s_ _[a]_ _i_ � [��]

_i_


_,_
_f_ _ki_


�


_f_ _ki_ _s_ _[a]_ _i_ � [�] �
_i_ 1 _k_


we can now perform the average over the coefficients matrix.

**2.** **Average over coefficients matrix c**

The average over _c_ _ν_ 1 _, ..., c_ _νL_ yields a different result with respect to the single feature retrieval; the term


exp _[β]_
� 2


_k_ 1

� _a_ � _m_ _[a]_ _ν_ [+] _√α_ _D_


(C2)

_c_ _ν_ 1 _,...,c_ _νL_


_L_

2 [�]

� _c_ _νk_ _µ_ _[a]_ _k_ �

_k_ =1


can be evaluated noting that if the _c_ _νk_ are Gaussian distributed with zero mean and unitary standard deviation, the
sum is still a random Gaussian variable with a variance that is the sum of the variances of the variables entering in the
sum, so we can write


_L_
� _c_ _νk_ _µ_ _[a]_ _k_ [= ˜] _[c]_ _[ν]_

_k_ =1


_L_
�


~~�~~
�
�
�


_L_
�

_k_ =1


� _µ_ _[a]_ _k_ � 2 = ˜ _c_ _ν_ _∥_ _**µ**_ _a_ _∥_ with ˜ _c_ _ν_ _∼N_ �0 _,_ 1� (C3)

10


Using the random variable ˜ _c_ _ν_ _∼N_ (0 _,_ 1) we can rewrite the average as


_∥_ _**µ**_ _[a]_ _∥_ [2] [���]

_a_


_c_ ˜ _ν_


_m_ _[a]_ _ν_ _[∥]_ _**[µ]**_ _[a]_ _[∥]_ + [1]
� 2
_a_


exp _[β]_
� 2

where


_a_ [(] _[m]_ _ν_ _[a]_ [)] [2] [�] exp _c_ ˜ _ν_ _β k_ 1
� � _√α_ _D_


�

_a_


2 [�]
_m_ _[a]_ _ν_ [+] _[ k]_ [1] [ ˜] _[c]_ _[ν]_ _∥_ _**µ**_ _[a]_ _∥_
� _√α_ _D_ �


= _e_ _β_ 2 �

_c_ ˜ _ν_


�


_β k_ 12

[1] _ν_

2 _[c]_ [˜] [2] � _α_ _D_


�


2 [�] (C4)
_m_ _[a]_ _ν_ _[∥]_ _**[µ]**_ _[a]_ _[∥]_
�

_a_


1

=
�1 _−_ _β_ _α_ _[k]_ _D_ 1 [2]


_β_
exp
� 2
_a_ _[∥]_ _**[µ]**_ _[a]_ _[∥]_ [2]


1
( _m_ _[a]_ _ν_ [)] [2] [ +] _[β]_ [2] _[k]_ [2]
2 _C α_ _D_
_a_


2 [�] (C4)
_m_ _[a]_ _ν_ _[∥]_ _**[µ]**_ _[a]_ _[∥]_

�� �

_a_


1 _−_ _β_ _α_ _[k]_ _D_ 1 [2] �


2


1

�( _m_ _[a]_ _ν_ [)] [2] [ +] 2 _[β]_ _C α_ [2] _[k]_ [2] _D_

_a_


_C_ = 1 _−_ _β_ _[k]_ 1 [2]
_α_ _D_


� _∥_ _**µ**_ _[a]_ _∥_ [2] _._ (C5)

_a_


**3.** **Integrating pattern magnetizations**

We can perform the integral over the pattern magnetizations, for a given pattern _ν_ we have


2
_m_ _[a]_ _ν_ _[∥]_ _**[µ]**_ _[a]_ _[∥]_ _−_ [1]
� 2
_a_


d _m_ _[a]_ _ν_
_√_ 2 _π_



_[a]_ _ν_ _β_

2 _π_ [exp] � 2


1

_√_ det _**Q**_


�
� _a_


2


�


2


1

�( _m_ _[a]_ _ν_ [)] [2] [ +] 2 _[β]_ _C α_ [2] _[k]_ [2] _D_

_a_


��


� _m_ _[a]_ _ν_ [(] **[Q]** _[−]_ [1] [)] _[ab]_ _[m]_ _ν_ _[b]_

_a,b_


d _m_ _[a]_ _ν_
_√_ 2 _π_



_[a]_ _ν_ _−_ [1]

2 _π_ [exp] � 2


1

=
_√_ det _**Q**_


�
� _a_


2


� _m_ _[a]_ _ν_ � _−_ _βδ_ _ab_ _−_ _C α_ _[β]_ [2] _[k]_ _D_ 1 [2] _∥_ _**µ**_ _[a]_ _∥∥_ _**µ**_ _[b]_ _∥|_ + ( **Q** _[−]_ [1] ) _ab_ � _m_ _[b]_ _ν_

_a,b_


�


(C6)


1

=
~~�~~ det �I _−_ _β_ **Q** _−_ _C α_ _[β]_ [2] _[k]_ _D_ 1 [2] **[p]** _[L]_ **[Q]** �


where we have defined


_p_ _[ab]_ _L_ [=] _[ ∥]_ _**[µ]**_ _[a]_ _[∥∥]_ _**[µ]**_ _[b]_ _[∥]_ _[.]_ (C7)

The contribution to the partition function can be written as _Z_ 0 = _e_ _[−]_ _[αN]_ 2 _[ϕ]_ [0], where

_ϕ_ 0 = ln det I _−_ _β_ **Q** _−_ _[β]_ [2] _[k]_ 1 [2] **p** _L_ **Q** _._ (C8)
� _C α_ _D_ �

**4.** **Integrating the feature magnetizations**

We consider now the contribution to the partition function given by the integral over uncondensed feature magnetisation


� _p_ ˆ _[ab]_ [ �]

_a,b_ _k>L_


_Z_ 1 = �
�

_k>L,a_


_dµ_ _[a]_ _k_ _[d][µ]_ [ˆ] _[a]_ _k_ _α_ 1
exp
2 _π_ � _α_ _D_ 2


�



[ �]

_µ_ _[a]_ _k_ _[µ]_ _[b]_ _k_ [+] _[ i]_ �
_k>L_


� _µ_ ˆ _[a]_ _k_ _[µ]_ _[a]_ _k_

_k>L,a_


(C9)

_,_
_f_ _ki_ _, k>L_


�


_i_
_×_ exp _−_
� � _√N_


� _µ_ ˆ _[a]_ _k_ �

_k>L,a_ _i_


�


_f_ _ki_ _s_ _[a]_ _i_

_i_


��


for the last term we have

_i_
exp _−_
� � _√N_


� _µ_ ˆ _[a]_ _k_ �

_k>L,a_ _i_


_i,k>L_


_µ_ ˆ _[a]_ _k_ _[s]_ _[a]_ _i_

_a_


�


_f_ _ki_ _s_ _[a]_ _i_

_i_


��


�


=
�
_f_ _ki_ _, k>L_


_i_
_Df_ _ki_ exp _−_
� � _√_


_f_ _ki_ �
_N_


_−_ [1]
= exp
� 2


��


_N_


�

_a,b_


_µ_ ˆ _[a]_ _k_ _[µ]_ [ˆ] _[b]_ _k_ 1
�� _N_
_k>L_


�


(C10)
_s_ _[a]_ _i_ _[s]_ _[b]_ _i_ _._
� [�]
_i_

11

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


We can write the total contribution of the integration over feature magnetizations as _Z_ 1 = _e_ [(] _[D][−][L]_ [)] _[ϕ]_ [1] noting that

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


ˆ
_−_ [1]
_µ_ _[a]_ _µ_ _[a]_

2

_a_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


_ϕ_ 1 = ln
�� _a_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


_dµ_ ˆ _[a]_ _dµ_ _[a]_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_



_[a]_ _dµ_ _[a]_

2 _π_ exp � _i_ �

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


ˆ

� _p_ _[ab]_ _µ_ _[a]_ _µ_ _[b]_ =

�
_a,b_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


2

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


2

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


ˆ ˆ

� _µ_ _[a]_ _µ_ _[b]_ _q_ _[ab]_ + _[α]_ 2 _[T]_

_a,b_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


�

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


(C11)

_̸_

(C12)

_̸_

_̸_

_̸_

_̸_

_̸_


= ln �
� _a_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


_dµ_ ˆ _[a]_
_√_ 2 _π_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


_µ_ ˆ _[a]_ _−_ [1]

2 _π_ [exp] � 2

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


� _µ_ ˆ _[a]_ ( **ˆp** _[−]_ [1] ) _[ab]_ _µ_ ˆ _[b]_ + [1]

_a,b_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_



[1] =

2 [ln det(] _[α]_ _[T]_ **[ ˆp]** [)] �

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


2

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


� _q_ _[ab]_ _µ_ ˆ _[a]_ _µ_ ˆ _[b]_ + 2 _α_ 1 _T_

_a,b_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


�

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


= _−_ [1]

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_



[1]

2 [ln det(] _[−][α]_ _[T]_ **[ ˆp]** [)] _[ −]_ [1]

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_



[1] **pˆ** _[−]_ [1]

2 [ln det(] **[q]** _[ −]_ _α_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


) =
_α_ _T_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


= _−_ [1] I _−_ _α_ _T_ **qˆp** _,_

2 [ln det] � �

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


the partition function now reads

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


_L_
� _dµ_ 2 _[a]_ _k_ _π_ _[d][µ]_ [ˆ] _[a]_ _k_

_k_ =1

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


_dp_ _[ab]_ _dp_ ˆ _[ab]_

2 _π_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


_n_
�

_a_ =1

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_



_[β]_

2 _[nP]_
��

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


_dq_ _[ab]_ _dq_ ˆ _[ab]_

2 _π_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


�

_ab_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


� _Z_ _[n]_ [�] = _e_ _[−]_ _[β]_ 2

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


_ab_

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


_L_
� _iµ_ ˆ _[a]_ _k_ _[µ]_ _[a]_ _k_

_̸_ _k_ =1 �

_̸_

_̸_

_̸_

_̸_

_̸_


_×_ exp _−_ _[αN]_
� 2

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


�

_̸_

_̸_

_̸_

_̸_

_̸_

_̸_


ˆ

� _q_ _[ab]_ _q_ _[ab]_ _−_ _αN_ �

_a_ = _̸_ _b_ _a≤b_

_̸_

_̸_

_̸_

_̸_

_̸_


� _p_ ˆ _[ab]_ _p_ _[ab]_ +
_̸_ _a≤b_

_̸_

_̸_

_̸_

_̸_

_̸_


_n_
�

_̸_ _a_ =1

_̸_

_̸_

_̸_

_̸_

_̸_


_̸_

_×_ exp _−_ _[αN]_ ln �1 _−_ _β_ _[k]_ 1 [2]
� 2 _α_ _D_

_̸_

_̸_

_̸_

_̸_

_̸_


_̸_

_−_ _[αN]_
_∥_ _**µ**_ _[a]_ _∥_ [2] [�]

2

_a_

_̸_

_̸_

_̸_

_̸_

_̸_


_̸_

�

_̸_

_̸_

_̸_

_̸_

_̸_


_̸_

ln det I _−_ _β_ **Q** _−_ _[β]_ [2] _[k]_ 1 [2] **p** _L_ **Q**
2 � _C α_ _D_ � [�]

_̸_

_̸_

_̸_

_̸_

_̸_


_̸_

_×_ exp _−_ _[α]_ _[D]_ _[N]_ ln det I _−_ _α_ _T_ **qˆp**
� 2 � � [�]

_̸_

_̸_

_̸_

_̸_

_̸_


_̸_

� _µ_ ˆ _[a]_ _k_

_̸_ 1 _≤k≤L,a_

_̸_

_̸_

_̸_

_̸_


_̸_

_._

_̸_ ��� _f_ _ki_ _,_ 1 _≤k≤L_

_̸_

_̸_

_̸_

_̸_


_̸_

_×_
��

_̸_

_̸_

_̸_

_̸_

_̸_


_̸_

_α_
exp
_{s_ _[a]_ _i_ _[}]_ � 2 _̸_

_̸_

_̸_

_̸_

_̸_


_̸_

_−_ _[i]_
� _N_

_̸_

_̸_

_̸_

_̸_

_̸_


_̸_

_−_ _[i]_
� _N_

_̸_

_̸_

_̸_

_̸_

_̸_


_̸_

_s_ _[a]_ _i_ _[s]_ _[b]_ _i_

_̸_ _i_

_̸_

_̸_

_̸_

_̸_


_̸_

_f_ _ki_ _s_ _[a]_ _i_

_̸_ �� _i_

_̸_

_̸_

_̸_

_̸_


_̸_

��

_̸_

_̸_

_̸_

_̸_

_̸_


_̸_

�

_̸_

_̸_

_̸_

_̸_

_̸_


_̸_

2

_̸_

_̸_

_̸_

_̸_

_̸_


_̸_

� _q_ ˆ _[ab]_

_a_ = _̸_ _b_ �� _i_

_̸_

_̸_

_̸_

_̸_


_̸_

�

_̸_

_̸_

_̸_

_̸_

_̸_


_̸_

_̸_

We do the substitution ˆ _µ_ _[a]_ _k_ _[→]_ _[iN]_ [ ˆ] _[µ]_ _[a]_ _k_ [, so that the terms have the right scaling with] _[ N]_ [. The partition function becomes]

_̸_

_̸_

_̸_

_̸_


_̸_

_̸_

�

_ab_

_̸_

_̸_

_̸_

_̸_


_̸_

_̸_

_dp_ _[ab]_ _dp_ ˆ _[ab]_

2 _π_

_̸_

_̸_

_̸_

_̸_


_̸_

_̸_

_L_
�

_k_ =1

_̸_

_̸_

_̸_

_̸_


_̸_

_̸_

_dµ_ _[a]_ _k_ _[d][µ]_ [ˆ] _[a]_ _k_

2 _π_

_̸_

_̸_

_̸_

_̸_


_̸_

_̸_

_[β]_

2 _[nP]_
��

_̸_

_̸_

_̸_

_̸_


_̸_

_̸_

_dq_ _[ab]_ _dq_ ˆ _[ab]_

2 _π_

_̸_

_̸_

_̸_

_̸_


_̸_

_̸_

� _Z_ _[n]_ [�] = _e_ _[−]_ _[β]_ 2

_̸_

_̸_

_̸_

_̸_


_̸_

_̸_

_n_
�

_a_ =1

_̸_

_̸_

_̸_

_̸_


_̸_

_̸_

_ab_

_̸_

_̸_

_̸_

_̸_


_̸_

_̸_

_L_
� _µ_ ˆ _[a]_ _k_ _[µ]_ _k_ _[a]_

_̸_ _k_ =1 �

_̸_

_̸_

_̸_


_̸_

_̸_

_×_ exp _−_ _[αN]_
� 2

_̸_

_̸_

_̸_

_̸_


_̸_

_̸_

ˆ

� _q_ _[ab]_ _q_ _[ab]_ _−_ _αN_ �

_a_ = _̸_ _b_ _a≤b_

_̸_

_̸_

_̸_


_̸_

_̸_

�

_̸_

_̸_

_̸_

_̸_


_̸_

_̸_

ˆ

� _p_ _[ab]_ _p_ _[ab]_ _−_ _N_
_̸_ _a≤b_

_̸_

_̸_

_̸_


_̸_

_̸_

_n_
�

_̸_ _a_ =1

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

� _∥_ _**µ**_ _[a]_ _∥_ [2]

_a_ �

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

_×_ exp _−_ _[αN]_ ln
� 2

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

1 _−_ _β_ _[k]_ 1 [2]
_α_ _D_

�

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

_−_ _[αN]_ ln det I _−_ _β_ **Q** _−_ _[β]_ [2] _[k]_ 1 [2] **p** _L_ **Q**

2 � _C α_ _D_ � [�]

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

(C13)

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

_×_ exp _−_ _[α]_ _[D]_ _[N]_ ln det I _−_ _α_ _T_ **qˆp**
� 2 � � [�]

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

_f_ _ki_ _s_ _[a]_ _i_

_̸_ �� _i_

_̸_

_̸_


_̸_

_̸_

_̸_

��

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

_L_
� _µ_ ˆ _[a]_ _k_

_̸_ _k_ =1

_̸_

_̸_


_̸_

_̸_

_̸_

_L_
�

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

_n_
�

_̸_ _a_ =1

_̸_

_̸_


_̸_

_̸_

_̸_

_._

_̸_ ��� _f_ _ki_ _,_ 1 _≤k≤L_

_̸_

_̸_


_̸_

_̸_

_̸_

���

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

_×_
��

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

_α_
exp
_{s_ _[a]_ _i_ _[}]_ � 2 _̸_

_̸_

_̸_


_̸_

_̸_

_̸_

+
�

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

_s_ _[a]_ _i_ _[s]_ _[b]_ _i_

_̸_ _i_

_̸_

_̸_


_̸_

_̸_

_̸_

2

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

� _q_ ˆ _[ab]_

_a_ = _̸_ _b_ �� _i_

_̸_

_̸_


_̸_

_̸_

_̸_

�

_̸_

_̸_

_̸_


_̸_

_̸_

_̸_

_̸_

We can now rewrite the last term contribution as

_̸_

_̸_


_̸_

_̸_

_̸_

_̸_

_α_
exp
_{s_ _[a]_ _i_ _[}]_ � 2 _̸_

_̸_


_̸_

_̸_

_̸_

_̸_

_,_ (C14)

_̸_ �� _f_ _ki_ _,_ 1 _≤k≤L_

_̸_


_̸_

_̸_

_̸_

_̸_

��

_̸_

_̸_


_̸_

_̸_

_̸_

_̸_

_L_
� _µ_ ˆ _[a]_ _k_ _[f]_ _[ki]_

_̸_ _k_ =1

_̸_


_̸_

_̸_

_̸_

_̸_

_L_
�

_̸_

_̸_


_̸_

_̸_

_̸_

_̸_

_n_
� _s_ _[a]_ _i_

_̸_ _a_ =1

_̸_


_̸_

_̸_

_̸_

_̸_

_n_
�

_̸_

_̸_


_̸_

_̸_

_̸_

_̸_

_Z_ 2 =

_̸_

hence, we have _Z_ 2 = _e_ _[Nϕ]_ [2] with

_̸_


_̸_

_̸_

_̸_

_̸_

_N_
�

_i_ =1 _̸_

_̸_


_̸_

_̸_

_̸_

_̸_

��

_̸_

_̸_


_̸_

_̸_

_̸_

_̸_

2

_̸_

_̸_


_̸_

_̸_

_̸_

_̸_

� _s_ _[a]_ _i_ _[q]_ [ˆ] _[ab]_ _[s]_ _[b]_ _i_ [+]

_a_ = _̸_ _b_

_̸_


_̸_

_̸_

_̸_

_̸_

_̸_

� _µ_ ˆ _[a]_ _k_ _[f]_ _[k]_

_̸_ _k_ =1


_̸_

_̸_

_̸_

_̸_

_̸_

_._ (C15)

_̸_ �� _f_ _k_ _,_ 1 _≤k≤L_


_̸_

_̸_

_̸_

_̸_

_̸_

��

_̸_


_̸_

_̸_

_̸_

_̸_

_̸_

_ϕ_ 2 = ln
��

_̸_


_̸_

_̸_

_̸_

_̸_

_̸_

_α_
exp
_{s_ _[a]_ _}_ � 2 _̸_


_̸_

_̸_

_̸_

_̸_

_̸_

_n_ _L_
� _s_ _[a]_ �

_̸_ _a_ =1 _k_ =1


_̸_

_̸_

_̸_

_̸_

_̸_

_n_
�

_̸_


_̸_

_̸_

_̸_

_̸_

_̸_

2

_̸_


_̸_

_̸_

_̸_

_̸_

_̸_

� _s_ _[a]_ _q_ ˆ _[ab]_ _s_ _[b]_ +

_a_ = _̸_ _b_

**5.** **Summary expression of the partition function**

Summarizing, we defined the following quantities

_̸_

_̸_


_ϕ_ � _{µ_ _[a]_ _k_ _[}]_ � = _−_ ln �1 _−_ _β_ _[k]_ 1 [2]
_α_ _D_

_̸_

_̸_


_n_
� _∥_ _**µ**_ _[a]_ _∥_ [2] [�] ;

_a_ =1

_̸_

_̸_


_ϕ_ 0 � _{µ_ _[a]_ _k_ _[}][,]_ **[ q]** _[,]_ **[ p]** � = ln det �I _−_ _β_ **Q** _−_ _C α_ _[β]_ [2] _[k]_ _D_ 1 [2] **p** _L_ **Q** �;

_ϕ_ 1 � **q** _,_ **ˆp** � = ln det �I _−_ _α_ _T_ **qˆp** �;

_̸_

_̸_


� _µ_ ˆ _[a]_ _k_ _[f]_ _[k]_

_̸_ _k_ =1

_̸_


_ϕ_ 2 � _{µ_ ˆ _[a]_ _k_ _[}][,]_ **[ ˆq]** � = ln
��

_̸_

_̸_


_α_
exp
_{s_ _[a]_ _}_ � 2 _̸_

_̸_


_n_ _L_
� _s_ _[a]_ �

_̸_ _a_ =1 _k_ =1

_̸_


_n_
�

_̸_

_̸_


;

_̸_ �� _f_ _k_ _,_ 1 _≤k≤L_

_̸_


2

_̸_

_̸_


� _s_ _[a]_ _q_ ˆ _[ab]_ _s_ _[b]_ +

_a_ = _̸_ _b_

_̸_


�

_̸_

_̸_


_̸_

where

_̸_


_̸_

_∥_ _**µ**_ _[a]_ _∥_ =

_̸_


_̸_

~~�~~
�
�
�

_̸_


_̸_

_L_
�

_k_ =1

_̸_


_̸_

2
� _µ_ _[a]_ _k_ � ;

_̸_


_̸_

_C_ = 1 _−_ _β_ _[k]_ 1 [2]
_α_ _D_

_̸_


_̸_

_n_
� _∥_ _**µ**_ _[a]_ _∥_ [2] ;

_a_ =1

_̸_


_̸_

_p_ _[ab]_ _L_ [=] _[ ∥]_ _**[µ]**_ _[a]_ _[∥∥]_ _**[µ]**_ _[b]_ _[∥|][.]_

Using these, we can write the complete partition function as

_̸_


_̸_

�

_ab_

_̸_


_̸_

_n_ _L_
� �

_a_ =1 _k_ =1

_̸_


_̸_

_dµ_ _[a]_ _k_ _[d][µ]_ [ˆ] _[a]_ _k_

2 _π_

_̸_


_̸_

� _Z_ _[n]_ [�] =
��

_ab_

_̸_


_̸_

_dq_ _[ab]_ _dq_ ˆ _[ab]_

2 _π_

_̸_


_̸_

_dp_ _[ab]_ _dp_ ˆ _[ab]_

2 _π_

_̸_


_̸_

_L_
� _µ_ ˆ _[a]_ _k_ _[µ]_ _k_ _[a]_

_̸_ _k_ =1


_̸_

_×_ exp _N_ _−_ _n_ _[α][β]_
� 2

_̸_


_̸_

2 _[−]_ _[α]_ 2

_̸_


_̸_

ˆ

� _p_ _[ab]_ _p_ _[ab]_ _−_
_̸_ _a≤b_


_̸_

_n_
�

_̸_ _a_ =1


_̸_

�

_̸_


_̸_

2

_̸_


_̸_

�

_̸_


_̸_

ˆ

� _q_ _[ab]_ _q_ _[ab]_ _−_ _α_ �

_a_ = _̸_ _b_ _a≤b_


_̸_

_̸_

_α_
_×_ exp _N_
� 2


12

(C16)

_̸_

(C17)

(C18)

_̸_

(C19)

(C20)


_̸_

_̸_

_α_

_−_ _[α]_
� _{µ_ _[a]_ _k_ _[}]_ �
2 _[ϕ]_ 2


_̸_

_̸_

_[α]_ _−_ _[α]_ _[D]_

� _{µ_ _[a]_ _k_ _[}][,]_ **[ q]** _[,]_ **[ p]** �
2 _[ϕ]_ [0] 2


_̸_

_̸_

_[D]_ ˆ

� **q** _,_ **ˆp** � + _ϕ_ 2 � _{µ_ _[a]_ _k_ _[}][,]_ **[ ˆq]** � [�] _._
2 _[ϕ]_ [1]


_̸_

_̸_

**6.** **RS ansatz**

We now use the replica symmetric (RS) ansatz

_µ_ _[a]_ _k_ [=] _[ µ]_ _[k]_ [;]
_µ_ ˆ _[a]_ _k_ [= ˆ] _[µ]_ _[k]_ [;]

_q_ _[ab]_ = _δ_ _[ab]_ + _q_ (1 _−_ _δ_ _[ab]_ );

ˆ
_q_ _[ab]_ = _δ_ _[ab]_ + ˆ _q_ (1 _−_ _δ_ _[ab]_ );

_p_ _[ab]_ = _p_ _d_ _δ_ _[ab]_ + _p_ (1 _−_ _δ_ _[ab]_ );

_p_ ˆ _[ab]_ = ˆ _p_ _d_ _δ_ _[ab]_ + ˆ _p_ (1 _−_ _δ_ _[ab]_ );

the quantities defined in equation (C17) become


_̸_

_̸_

_∥_ _**µ**_ _[a]_ _∥_ =


_̸_

_̸_

~~�~~
�
�
�


_̸_

_̸_

_L_
�


_̸_

_̸_

� _µ_ [2] _k_ [;]

_k_ =1


_̸_

_̸_

1
_C_ = 1 _−_ _β_ _[k]_ [2] _n∥_ _**µ**_ _∥_ [2] _≈_ 1;
_α_ _D_


_̸_

_̸_

_p_ _[ab]_ _L_ [=] _[ ∥]_ _**[µ]**_ _[∥]_ [2] [ =]


_̸_

_̸_

_L_
� _µ_ [2] _k_ _[.]_

_k_ =1

We define for convenience the RS matrix

with the definitions


13

_Q_ _[ab]_ = _Q_ _d_ _δ_ _[ab]_ + _Q_ (1 _−_ _δ_ _[ab]_ ) _,_ (C21)

_Q_ _d_ = _k_ _∗_ [2] [+] _[ k]_ 1 [2] _[p]_ _[d]_ _[,]_ (C22a)

_Q_ = _k_ _∗_ [2] _[q]_ [ +] _[ k]_ 1 [2] _[p.]_ (C22b)


We can evaluate the terms inside the partition function using the RS ansatz. The term _ϕ_ 2 becomes


ˆ

� _µ_ _k_ _f_ _k_

_k_ =1


_n_
�
�


=

_f_ _k_ _,_ 1 _≤k≤L_


_ϕ_ 2 = ln _e_ _[−]_ _[α]_ 2
�



[ �] _α_

exp
_{s_ _[a]_ _}_ �


_n_ 2 _L_

_s_ _[a]_ +

� �

_a_ =1 � � _k_ =1



_[α]_

2 _[n][q]_ [ˆ] [ �]


_n_

_α_ 2 _[q]_ [ˆ] � �


� _s_ _[a]_

_a_ =1 ��


=

_f_ _k_ _,_ 1 _≤k≤L_


2 _[q]_ [ˆ][ + ln] ��


= _−_ _[αn]_


_αqz_ ˆ +


_L_

ˆ

� _µ_ _k_ _f_ _k_

_k_ =1


_n_
�
�


� _s_ _[a]_

_a_ =1 ��


_{s_ _[a]_ _}_


D _z_ exp
� ���


��


D _z_ ln 2 cosh ~~�~~
2 _[q]_ [ˆ][ +] _[ n]_ �� �


_._

_f_ _k_ _,_ 1 _≤k≤L_


= _−_ _[αn]_


_αqz_ ˆ +


_L_

ˆ

� _µ_ _k_ _f_ _k_

_k_ =1


_L_
�


We can rewrite the quantity (C8) as _ϕ_ 0 = ln det **D** 0 with


_D_ 0 _[ab]_ [=] _[δ]_ _[ab]_ _[ −]_ _[βQ]_ _[ab]_ _[ −]_ _Cα_ _[β]_ [2] _[k]_ _D_ 1 [2]


_ab_
� **p** _L_ **Q** �


= _δ_ _[ab]_ _−_ _β_ _Q_ _d_ _δ_ _[ab]_ + _Q_ �1 _−_ _δ_ _[ab]_ [�][�] _−_ _[β]_ [2] _[k]_ 1 [2]
� _Cα_ _D_


_L_
� _µ_ [2] _k_
� _k_ =1


_n_
�
� _c_ =1


_Q_ _d_ _δ_ _[cb]_ + _Q_ �1 _−_ _δ_ _[cb]_ [��] =
�


(C23)

(C24)


_L_
_Q_ _d_ + � _n −_ 1� _Q_ � _µ_ [2] _k_
� �� _k_ =1


_L_
_Q_ _d_ + � _n −_ 1� _Q_ �
� ��


_,_
�


= _δ_ _[ab]_ 1 _−_ _β_ � _Q_ _d_ _−_ _Q_ � [�] _−_ _βQ −_ _[β]_ [2] _[k]_ 1 [2]
� _Cα_ _D_


where in the last line we use the RS ansatz for the features magnetizations.

For a generic _n × n_ replica symmetric matrix of the form

_X_ _[ab]_ = _x_ _d_ _δ_ _[ab]_ + _x_ �1 _−_ _δ_ _[ab]_ [�] _,_ (C25)

holds the formula

_x_
ln det **X** = _n_ ln � _x_ _d_ _−_ _x_ � + _n_ _x_ _d_ _−_ _x_ [+] _[ O]_ � _n_ [2] [�] _,_ (C26)

using it in equation (C24) and considering that in the limit _n →_ 0 we have _C ≈_ 1 we get


1 _L_
_βQ_ + _[β]_ _α_ [2] _D_ _[k]_ [2] � _Q_ _d_ _−_ _Q_ � [�] � _k_ =1 _[µ]_ _k_ [2]
_ϕ_ 0 = _n_ ln �1 _−_ _β_ � _Q_ _d_ _−_ _Q_ � [�] _−_ _n_ 1


�


_._ (C27)
1 _−_ _β_ � _Q_ _d_ _−_ _Q_ �


We can write in a similar way _ϕ_ 1 = _−_ 2 [1] [ln det] **[ D]** [1] [, with]

**D** 1 = I _−_ _α_ _T_ **qˆp** _,_ (C28)

using the RS ansatz we get

_D_ 1 _[ab]_ [=] _[δ]_ _[ab]_ _[ −]_ _[α]_ _[T]_

= _δ_ _[ab]_ _−_ _α_ _T_

= _δ_ _[ab]_ _−_ _α_ _T_

= _δ_ _[ab]_ _−_ _α_ _T_

= _δ_ _[ab]_ _−_ _α_ _T_

_̸_


_n_

ˆ

� � _δ_ _[ac]_ + _q_ �1 _−_ _δ_ _[ac]_ [���] _p_ _d_ _δ_ _[cb]_ + ˆ _p_ �1 _−_ _δ_ _[cb]_ [��] =

_c_ =1

_n_

ˆ ˆ

� ��1 _−_ _q_ � _δ_ _[ac]_ + _q_ ��� _p_ _d_ _−_ _p_ � _δ_ _[cb]_ + ˆ _p_ � =

_c_ =1

ˆ ˆ ˆ ˆ ˆ ˆ
1 _−_ _q_ �� _p_ _d_ _−_ _p_ � _δ_ _[ab]_ + �1 _−_ _q_ � _p_ + _q_ � _p_ _d_ _−_ _p_ � + _nqp_ =
�� �

ˆ ˆ ˆ ˆ ˆ ˆ
_δ_ _[ab]_ [�] _p_ _d_ _−_ _p −_ _qp_ _d_ + _qp_ + ˆ _p_ + _qp_ _d_ + _n −_ 2 _qp_ =
� � � � �

_̸_


_n_

ˆ

� _q_ _[ac]_ _p_ _[cb]_ =

_c_ =1

_̸_


14

(C29)

_̸_


ˆ ˆ ˆ ˆ ˆ
= _δ_ _[ab]_ �1 _−_ _α_ _T_ � _p_ _d_ + � _n −_ 1� _qp_ � [�] _−_ _α_ _T_ �1 _−_ _δ_ _[ab]_ [��] _p_ + _qp_ _d_ + � _n −_ 2� _qp_ � _,_

using formula (C26) we get

_ϕ_ 1 = _n_ ln 1 _−_ _α_ _T_ � _p_ ˆ _d_ _−_ _p_ ˆ��1 _−_ _q_ � [�] _−_ _nα_ _T_ _p_ ˆ + ˆ _qp_ ˆ _d_ _−_ ˆ2 _qp_ ˆ _._ (C30)
� 1 _−_ _α_ _T_ � _p_ _d_ _−_ _p_ ��1 _−_ _q_ �

The term _ϕ_ in the RS ansatz becomes

_̸_


1
_ϕ_ = _−_ ln �1 _−_ _β_ _[k]_ [2]
_α_ _D_

_̸_


_n_
� _∥_ _**µ**_ _[a]_ _∥_ [2] [�] =

_a_ =1

_̸_


= _−_ ln �1 _−_ _β_ _[k]_ 1 [2] _n_
_α_ _D_

_̸_


_L_
� _µ_ [2] _k_ � =

_k_ =1

_̸_


(C31)

_̸_


= _nβ_ _[k]_ 1 [2]
_α_ _D_

Finally, the remaining terms in eq. C18 become

_̸_


_L_
� _µ_ [2] _k_ [+] _[ O]_ � _n_ [2] [�] _._

_k_ =1

_̸_


ˆ ˆ ˆ

� _q_ _[ab]_ _q_ _[ab]_ = _n_ � _n −_ 1� _qq_ = _−nqq_ + _O_ � _n_ [2] [�] ; (C32)

_a_ = _̸_ _b_

ˆ ˆ ˆ

� _p_ _[ab]_ _p_ _[ab]_ = _n_ � _p_ _d_ _p_ _d_ _−_ _pp_ �; (C33)

_a,b_


_̸_

_L_

ˆ

� _µ_ _k_ _µ_ _k_ _._ (C34)

_k_ =1


_̸_

_n_
�

_a_ =1

The partition function (C18) now reads


_̸_

_L_
� _µ_ ˆ _[a]_ _k_ _[µ]_ _[a]_ _k_ [=] _[n]_

_k_ =1


_̸_

_dqdq_ ˆ
� _Z_ _[n]_ [�] =
� 2 _π_


_̸_

_qdq_ ˆ _dpdp_ ˆ

2 _π_ 2 _π_


_̸_

2 _π_ _[dp]_ _[d]_ _[d][p]_ [ˆ] _[d]_


_̸_

_L_
�

_k_ =1


_̸_

_dµ_ 2 _k_ _πdµ_ ˆ _k_ _exp_ � _−_ _βNf_ � _q,_ ˆ _q, p,_ ˆ _p, p_ _d_ _,_ ˆ _p_ _d_ _, {µ_ _k_ _}, {µ_ ˆ _k_ _}_ � [�] _,_ (C35)


_̸_

where the function at the exponent is defined as

ˆ
_−βf_ � _q,_ ˆ _q, p,_ ˆ _p, p_ _d_ _,_ ˆ _p_ _d_ _, {µ_ _k_ _}, {µ_ _k_ _}_ � = _n_ lim _→_ 0
_N_ _→_ + _∞_


_̸_

1

_N_


_̸_

� _Z_ _[n]_ [�] _−_ 1
; (C36)
_n_

we have


ˆ
_−_ _βf_ � _q,_ ˆ _q, p,_ ˆ _p, p_ _d_ _,_ ˆ _p_ _d_ _, {µ_ _k_ _}, {µ_ _k_ _}_ � =


_L_
� _µ_ [2] _k_ [+]

_k_ =1


15

(C37)


= _−_ _β_ _[α]_



_[α]_

2 _[−]_ _[α]_



_[α]_ �1 _−_ _q_ � _−_ _[α]_

2 _[q]_ [ˆ]



_[α]_

2 _[p]_ [ˆ] _[d]_ _[p]_ _[d]_ [ +] _[ α]_ 2


2 _[p][p]_ [ˆ] _[ −]_


_L_

ˆ 1

� _µ_ _k_ _µ_ _k_ + _αβ_ 2 _[k]_ _α_ [2] _D_

_k_ =1


1 _L_
� ln �1 _−_ _β_ � _Q_ _d_ _−_ _Q_ � [�] _−_ _βQ_ + _[β]_ _α_ [2] _D_ 1 _[k]_ [2] _−_ � _Qβ_ _d_ � _−Q_ _d_ _Q −_ ��� _Q_ � _k_ =1 _[µ]_ _k_ [2] � �+


1 _L_
� ln �1 _−_ _β_ � _Q_ _d_ _−_ _Q_ � [�] _−_ _βQ_ + _[β]_ _α_ [2] _D_ 1 _[k]_ [2] _−_ � _Qβ_ _d_ � _−Q_ _d_ _Q −_ ��� _Q_ � _k_ =1 _[µ]_ _k_ [2] �


_−_ _[α]_

2


_−_ _[α]_ _[D]_ ln 1 _−_ _α_ _T_ � _p_ ˆ _d_ _−_ _p_ ˆ��1 _−_ _q_ � [�] _−_ _α_ _T_ _p_ ˆ + ˆ _qp_ ˆ _d_ _−_ ˆ2 _qp_ ˆ +

2 � � 1 _−_ _α_ _T_ � _p_ _d_ _−_ _p_ ��1 _−_ _q_ � �


+ D _z_ ln 2 cosh ~~�~~ _αqz_ ˆ +
�� �


_L_

ˆ

� _µ_ _k_ _f_ _k_

_k_ =1


�� _f_ _k_ _,_ 1 _≤k≤L_


We impose the following scaling with the inverse temperature _β_ :

_q_ ˆ _→_ _β_ [2] _q_ ˆ; (C38a)
_µ_ ˆ _k_ _→_ _βµ_ ˆ _k_ ; (C38b)
_p_ ˆ _→_ _βp_ ˆ; (C38c)
_p_ ˆ _d_ _→_ _βp_ ˆ _d_ ; (C38d)

the resulting RS free energy is


_L_
� _µ_ [2] _k_

_k_ =1



_[α]_

2 [+] _[ α]_ 2



_[α]_

2 _[p]_ [ˆ] _[d]_ _[p]_ _[d]_ _[ −]_ _[α]_ 2


2 _[p][p]_ [ˆ][ +]


_f_ _[RS]_ = + _[α]_


�1 _−_ _q_ � + _[α]_
2 _[qβ]_ [ˆ]


_L_

ˆ 1 _α_

� _µ_ _k_ _µ_ _k_ _−_ _[k]_ 2 [2] _α_ _D_

_k_ =1


+ _[α]_

2 _β_


+ _[α]_


1 _L_
� ln �1 _−_ _β_ � _Q_ _d_ _−_ _Q_ � [�] _−_ _βQ_ + _[β]_ _α_ [2] _D_ 1 _[k]_ [2] _−_ �� _β_ � _k_ _Q_ =1 _d_ _−_ _[µ]_ _k_ [2] _Q_ ��� _Q_ _d_ _−_ _Q_ � �


1 _L_
� ln �1 _−_ _β_ � _Q_ _d_ _−_ _Q_ � [�] _−_ _βQ_ + _[β]_ _α_ [2] _D_ 1 _[k]_ [2] _−_ �� _β_ � _k_ _Q_ =1 _d_ _−_ _[µ]_ _k_ [2] _Q_ ��� _Q_ _d_ _−_ _Q_ �


ln 1 _−_ _α_ _T_ � _p_ ˆ _d_ _−_ _p_ ˆ� _β_ �1 _−_ _q_ � [�] _−_ _α_ _T_ _β_ _p_ ˆ +ˆ _qp_ ˆ _d_ _−_ ˆ 2 _qp_ ˆ
� � 1 _−_ _α_ _T_ � _p_ _d_ _−_ _p_ � _β_ �1 _−_ _q_ � �


(C39)


+ _[α]_ _[D]_

2 _β_


+ _[α]_ _[D]_


_,_
_f_ _k_ _,_ 1 _≤k≤L_


_−_ [1]

_β_


_Dz_ ln 2 cosh _β_
�� ��


_αqz_ ˆ +


_L_

ˆ

� _µ_ _k_ _f_ _k_

_k_ =1


��


where the order parameters are the solutions of the saddle-point equations.

16

**7.** **Saddle-point equations**

We can now search for the saddle-point equations deriving the free energy with respect to the order parameters, the
final result is:


; (C40a)

� [�]

_f,z_


� [�]


_L_

ˆ

� _µ_ _k_ _f_ _k_

_k_ =1


_L_
�


_µ_ _k_ =

_q_ =


�


tanh [2] _β_

� ��


�


_f_ _k_ tanh _β_ ~~�~~
�


_αqz_ ˆ +


; (C40b)

� [�]

_f,z_


_αqz_ ˆ +


_L_

ˆ

� _µ_ _k_ _f_ _k_

_k_ =1


_α_ _D_ [;] (C40c)

�1 _−_ _αα_ _D_ _[β]_ �1 _−_ _q_ �� _p_ ˆ _d_ _−_ _p_ ˆ� [�] [2]


1
_p_ =
_α_ _D_

1
_p_ _d_ =
_α_ _D_


_L_ _q_ + _αα_ _D_ _[pβ]_ [ˆ] �1 _−_ _q_ � 2

_k_ � =1 _µ_ [2] _k_ [+] 1 _−_ _α_ 1 _−_ ˆ


_L_
�


_L_ 1 + _αα_ _D_ _[β]_ �2ˆ _p −_ _p_ ˆ _d_ ��1 _−_ _q_ � 2

_k_ � =1 _µ_ [2] _k_ [+] 1 _−_ _α_ 1 _−_ ˆ ˆ


_L_
�


_α_ _D_ [;] (C40d)

�1 _−_ _αα_ _D_ _[β]_ �1 _−_ _q_ �� _p_ ˆ _d_ _−_ _p_ ˆ� [�] [2]


_k_ _∗_ [2] � _k_ 1 [2] _[p]_ [ +] _[ k]_ _∗_ [2] _[q]_ � _p_ ˆ + _αα_ _D_ _[βq]_ � _p_ ˆ _d_ _−_ _p_ ˆ� 2

[+]
�1 _−_ _βk_ 1 [2] � _p_ _d_ _−_ _p_ � _−_ _βk_ _∗_ [2] �1 _−_ _q_ � [�] [2] _β_ � _αα_ _D_ _[β]_ �1 _−_ _q_ �� _p_ ˆ _d_ _−_ _p_ ˆ


ˆ _k_ _∗_ [2] � _k_ 1 [2] _[p]_ [ +] _[ k]_ _∗_ [2] _[q]_ �
_q_ =


_α_ _D_ [;] (C40e)

_β_ � _αα_ _D_ _[β]_ �1 _−_ _q_ �� _p_ ˆ _d_ _−_ _p_ ˆ� [�] [2]


ˆ _βk_ 1 [2] � _k_ 1 [2] _[p]_ [ +] _[ k]_ _∗_ [2] _[q]_ �
_p_ = [;] (C40f)

�1 _−_ _βk_ 1 [2] � _p_ _d_ _−_ _p_ � _−_ _βk_ _∗_ [2] �1 _−_ _q_ � [�] [2]


_p_ ˆ _d_ = _[k]_ 1 [2] �1 + _βk_ 1 [2] �2 _p −_ _p_ _d_ � + _βk_ _∗_ [2] �2 _q −_ 1�� ; (C40g)

�1 _−_ _βk_ 1 [2] � _p_ _d_ _−_ _p_ � _−_ _βk_ _∗_ [2] �1 _−_ _q_ � [�] [2]


ˆ
_µ_ _k_ = _[α]_

_α_ _D_


ˆ ˆ
_p_ _d_ _−_ _p_ _µ_ _k_ _._ (C40h)
� �


**Appendix D: Specialization for symmetric mixtures**

We focus on symmetric mixtures of features defined by

_L_

~~��~~ � ~~�~~
_**µ**_ = _µ_ _L_ � 1 _, . . .,_ 1 _,_ 0 _, . . .,_ 0 � _._ (D1)

Note that, in the main text, for clarity we used _µ_ instead of _µ_ _L_ in the definition of symmetric mixtures. Here we keep
the subscript _L_ to stress that the solution depends on the number of features in the mixture.
Substituting this ansatz into the saddle point equations (C40h) we get


_µ_ _L_ = [1]

_L_


� _ζ_ _L_ tanh _β_ �� _αqz_ ˆ + ˆ _µ_ _L_ _ζ_ _L_ ��


; (D2a)
_z,ζ_ _L_


_q_ = tanh [2] _β_
� ��


_αqz_ ˆ + ˆ _µ_ _L_ _ζ_ _L_ ; (D2b)
�� _z,ζ_ _L_


ˆ
_µ_ _L_ = _[α]_

_α_ _D_


ˆ ˆ
_p_ _d_ _−_ _p_ _µ_ _L_ ; (D2c)
� �



_[L]_ _µ_ [2] _L_ [+] _q_ + _αα_ _D_ _[β][p]_ [ˆ] �1 _−_ _q_ � 2

_α_ _D_ _α_ ˆ


_p_ = _[L]_


_α_ _D_ [;] (D2d)

�1 _−_ _αα_ _D_ _[β]_ �1 _−_ _q_ �� _p_ ˆ _d_ _−_ _p_ ˆ� [�] [2]



_[L]_ _µ_ [2] _L_ [+] 1 + _αα_ _D_ _[β]_ �2ˆ _p −_ _p_ ˆ _d_ ��1 _−_ _q_ � 2

_α_ _D_ _α_ ˆ ˆ


_p_ _d_ = _[L]_


_α_ _D_ [;] (D2e)

�1 _−_ _αα_ _D_ _[β]_ �1 _−_ _q_ �� _p_ ˆ _d_ _−_ _p_ ˆ� [�] [2]

17

where _ζ_ _L_ = [�] _[L]_ _k_ =1 _[f]_ _[k]_ [ is the sum of feature components, which follows a binomial distribution defined as]

_L_
_P_ ( _ζ_ _L_ ) = 2 _[−][L]_ _,_ with _k_ = ( _ζ_ _L_ + _L_ ) _/_ 2 _._ (D3)
� _k_ �

The other saddle point equations do not depend on the particular ansatz chosen for the feature magnetizations.
Substituting the ansatz inside the RS free energy expression (C39) we get for a _L_ -mixture of features


_f_ _n_ _[RS]_ = + _[α]_



_[α]_

2 [+] _[ α]_


�1 _−_ _q_ � + _[α]_
2 _[qβ]_ [ˆ]



_[α]_

2 _[p]_ [ˆ] _[d]_ _[p]_ _[d]_ _[ −]_ _[α]_ 2



_[α]_ 1 _α_ _µ_ [2] _L_ [+]

2 _[p][p]_ [ˆ][ +] _[ L][µ]_ [ˆ] _[L]_ _[µ]_ _[L]_ _[ −]_ _[Lk]_ 2 [2] _α_ _D_


+ _[α]_

2 _β_


+ _[α]_


� ln �1 _−_ _β_ � _Q_ _d_ _−_ _Q_ � [�] _−_ _βQ_ +1 _L −_ _[β]_ _α_ _β_ [2] _D_ _[k]_ � 1 [2] _Q_ _[µ]_ _dL_ [2] _−_ � _QQ_ _d_ _−_ � _Q_ � �+


� ln �1 _−_ _β_ � _Q_ _d_ _−_ _Q_ � [�] _−_ _βQ_ +1 _L −_ _[β]_ _α_ _β_ [2] _D_ _[k]_ � 1 [2] _Q_ _[µ]_ _dL_ [2] _−_ � _QQ_ _d_ _−_ � _Q_ �


ln 1 _−_ _α_ _T_ � _p_ ˆ _d_ _−_ _p_ ˆ� _β_ �1 _−_ _q_ � [�] _−_ _α_ _T_ _β_ _p_ ˆ +ˆ _qp_ ˆ _d_ _−_ ˆ 2 _qp_ ˆ +
� � 1 _−_ _α_ _T_ � _p_ _d_ _−_ _p_ � _β_ �1 _−_ _q_ � �


+ _[α]_ _[D]_

2 _β_


+ _[α]_ _[D]_


��


_._

_ζ_ _L_


_−_ [1]

_β_


_Dz_ ln 2 cosh _β_ ~~�~~ _αqz_ ˆ + ˆ _µ_ _L_ _ζ_ _L_
�� �


**1.** **Zero-temperature limit**

We will focus on the _T →_ 0 limit that can be performed defining the quantities:

_δq_ = _β_ (1 _−_ _q_ ); (D4a)
_δp_ = _β_ ( _p_ _d_ _−_ _p_ ); (D4b)
_δp_ ˆ = ˆ _p_ _d_ _−_ _p_ ˆ; (D4c)

ˆ
_δp_ _d_ = _[p]_ [ˆ] _[d]_ [ + ˆ] _[p]_ _._ (D4d)

2 _β_

The resulting equations are


_µ_ _L_ = [1]

_L_


ˆ
_µ_ _L_ _ζ_ _L_
_ζ_ _L_ erf ˆ
� � _√_ 2 _αq_


��


; (D5a)
_ζ_ _L_


2
_δq_ = ˆ
_√αq_


ˆ
_µ_ _L_ _ζ_ _L_
_G_ ˆ
� � _√αq_ ��


; (D5b)
_ζ_ _L_


_δq_
_δp_ = 1 _−_ _α_ (D5c)
_α_ _D_ _[δqδ][p]_ [ˆ][;]

1 + _αα_ _D_ _[δ][p]_ [ˆ] _[d]_ _[δq]_ [2]

_p_ _d_ = _α_ _[L]_ _D_ _µ_ [2] _L_ [+] 1 _−_ _α_ 2 [;] (D5d)

� _α_ _D_ _[δqδ][p]_ [ˆ] �

ˆ ˆ
_µ_ _L_ = _[α]_ _δp µ_ _L_ ; (D5e)

_α_ _D_

ˆ _k_ _∗_ [2] � _k_ 1 [2] _[p]_ _[d]_ [+] _[ k]_ _∗_ [2] � _δp_ ˆ _d_ + _αα_ _D_ _[δ][p]_ [ˆ] [2]
_q_ = �1 _−_ _k_ 1 [2] _[δp][ −]_ _[k]_ _∗_ [2] _[δq]_ � 2 [+] �1 _−_ _αα_ _D_ _[δqδ][p]_ [ˆ] � 2 [;] (D5f)

ˆ _k_ 1 [2]
_δp_ = (D5g)
1 _−_ _k_ 1 [2] _[δp][ −]_ _[k]_ _∗_ [2] _[δq]_ [ ;]

ˆ _k_ 1 [2] � _k_ 1 [2] _[p]_ _[d]_ [+] _[ k]_ _∗_ [2] �
_δp_ _d_ = 2 [;] (D5h)

�1 _−_ _k_ 1 [2] _[δp][ −]_ _[k]_ _∗_ [2] _[δq]_ �

where we have used the functions defined by


2
erf _x_ =
� �
_√π_


+ _∞_

d _t e_ _[−][t]_ [2] ; (D6a)

� 0


1
_G_ � _x_ � =
_√_ 2


2

2 _π_ _[e]_ _[−]_ _[x]_ [2]


2 _._ (D6b)

18

**2.** **Numerical analysis of saddle-point equations**

The saddle-point equations (D5h) are solved numerically using the following iterative method.

_a._ _Iterative method_

Suppose we have a system of _M_ non-linear coupled equations for the order parameters _x_ _i_ with external parameters
_**ϑ**_, given by:

_x_ _i_ = _g_ _i_ � **x** ; _**ϑ**_ � _,_ (D7)

with _i_ = 1 _, . . ., M_ . The iterative process for fixed external parameters _ϑ_ unfolds as follows:

1. Begin with an initial guess **x** [(0)] .

2. Compute order parameters at time _t_ using the rule

_x_ [(] _i_ _[t]_ [)] = _γx_ [(] _i_ _[t][−]_ [1)] + (1 _−_ _γ_ ) _g_ _i_ � **x** [(] _[t][−]_ [1)] ; _ϑ_ � _,_ (D8)

where _γ ∈_ [0 _,_ 1) serves as an hyperparameter for damping, enhancing convergence.

3. Evaluate _δ_ = _|_ **x** [(] _[t]_ [)] _−_ **x** [(] _[t][−]_ [1)] _|_ .

4. If _δ < ϵ_ stop the procedure; otherwise, restart from step 2.

A fixed point **x** _[∗]_ of this method is also a solution of the set of equations (D7). The convergence of this procedure
depends significantly on the initial point. Therefore, it proves helpful to know the theoretical solution for some specific
values of the external parameters. Thereafter we can systematically explore the parameter space with small steps,
employing the previously identified fixed point as our initial guess.

_b._ _Phase diagram_

With this method, we build the spinodal lines as the locus of points where the solution with _µ >_ 0 becomes unstable.
In this case we span the phase diagram with horizontal cuts corresponding to a fixed value of _α_ and varying the
value of _α_ _D_, starting from _α_ _D_ = 0 we find the ferromagnetic solution with the iterative method using an initial guess
with _µ >_ 0.
Then, we increase _α_ _D_ by a small quantity _δα_ _D_ and repeat the procedure using the previously found fixed point as
initial guess for the new iteration. For a certain value _α_ _D_ ( _α_ ) the ferromagnetic state is no more a fixed point of the
iterative method signaling the developing of instability.

