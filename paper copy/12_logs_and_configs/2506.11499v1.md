## **On the Effectiveness of Integration Methods for Multimodal Dialogue** **Response Retrieval**

**Seongbo Jang** **[1]** **Seonghyeon Lee** **[2]** **Dongha Lee** **[3]** **Hwanjo Yu** **[1,]** _[∗]_

1 Pohang University of Science and Technology
2 Kyungpook National University 3 Yonsei University
{jang.sb,hwanjoyu}@postech.ac.kr
sh0416@knu.ac.kr donalee@yonsei.ac.kr


**Abstract**

Multimodal chatbots have become one of the

major topics for dialogue systems in both research community and industry. Recently, researchers have shed light on the multimodality
of responses as well as dialogue contexts. This
work explores how a dialogue system can output responses in various modalities such as text
and image. To this end, we first formulate a
multimodal dialogue response retrieval task for
retrieval-based systems as the combination of
three subtasks. We then propose three integration methods based on a two-step approach
and an end-to-end approach, and compare the
merits and demerits of each method. Experimental results on two datasets demonstrate that

the end-to-end approach achieves comparable
performance without an intermediate step in
the two-step approach. In addition, a parameter
sharing strategy not only reduces the number
of parameters but also boosts performance by
transferring knowledge across the subtasks and
the modalities.

**1** **Introduction**

As the demand for open-domain chatbots and
technical development of dialogue systems rise
steeply, researchers have brought attention to multimodal dialogue systems. Among various modalities, image-grounded conversations have been actively researched along with the advent of several benchmarks (Lin et al., 2014; Plummer et al.,
2015; Antol et al., 2015; Das et al., 2017; Zellers
et al., 2019). Most of the benchmarks focus on
the factual contents of images, usually given in
the form of question-answer pairs. In addition
to factual information, recent studies started to
consider emotional exchange and engagingness,
which are humane aspects of open-domain dialogues (Hu et al., 2014), by collecting more chitchat-like datasets such as image-grounded conver
_∗_ Corresponding author


sation (IGC) (Mostafazadeh et al., 2017) and ImageChat (Shuster et al., 2020a).

While most work focused on understanding dialogue contexts of multiple modalities, little work
tried to build an integrated system that outputs
multimodal responses for retrieval-based chatbots.
Zang et al. (2021) proposed photo-sharing intent
prediction and image retrieval as individual tasks
for multimodal response retrieval, yet the combination of an image retriever and a text retriever
remains ambiguous.

To overcome this limitation, we first formulate
multimodal response retrieval task which aims to
choose the most appropriate text or image response
for the next utterance, given a dialogue context
composed of text utterances. Then, we explore
three unified methods that integrate subcomponents
for the end task in different ways. To be specific,
dual retriever (DR) and shared dual retriever (SDR)
are based on a two-step approach: 1) intent prediction determines the modality of the next utterance and 2) response retrieval finds out the most
likely utterance from the predicted modality; on
the contrary, multimodal dual retriever (MDR) is
an end-to-end approach that selects responses from
a heterogeneous candidate pool of the both modalities without the explicit intent prediction step.

We evaluate the effectiveness of each method for

multimodal response retrieval and investigate the
effect of model size on two benchmark datasets.

The two-step approach performs better than the
end-to-end approach in unimodal retrieval tasks,
whereas the end-to-end approach achieves comparable performance compared to the two-step approach in multimodal retrieval, posing a question
on the necessity of intent prediction. In terms of
model size, SDR and MDR reduce the number of
parameters by sharing context encoders, which is
impossible for DR that trains separate context encoders for each modality and intent prediction.

**2** **Related Work**

**2.1** **Dialogue Response Retrieval**

There are two representative approaches for dialogue response retrieval task: dual encoder and
cross-encoder. Dual encoder approaches (Huang
et al., 2013; Henderson et al., 2017; Mazaré et al.,
2018; Dinan et al., 2019) encode dialogue contexts
and responses separately with distinct encoders
and compute matching scores to select the most
appropriate response. This nature enables us to precompute representations to reduce computational
burden at inference time, thus widely adopted in
many studies (Humeau et al., 2020; Lan et al.,
2021; Chen et al., 2021). In particular, researchers
have focused on the selection of encoding functions such as CNNs (Yan et al., 2018; Wu et al.,
2018), RNNs (Lowe et al., 2015), combined architectures of CNNs and RNNs (Yan et al., 2016; Zhou
et al., 2016), memory networks (Zhang et al., 2018),
transformers (Dinan et al., 2019; Xu et al., 2021b),
and BERT (Henderson et al., 2019). Cross-encoder
approaches (Gu et al., 2020; Whang et al., 2020;
Wu et al., 2020; Xu et al., 2021a) take the concatenated inputs of contexts and responses to enrich interactions between the two. They are also actively
studied owing to the development of pre-trained
language models (PLMs) such as BERT (Devlin
et al., 2019). In this work, we focus on dual encoder
architecture since cross-encoders are less preferred
to dual encoders in practical situations.

**2.2** **Multimodal Open-domain Dialogue**

There have been several studies to build multimodal

open-domain dialogue systems which output text
responses related to a given image followed by
a few turns of dialogue contexts. Shuster et al.
(2020a) constructed ImageChat dataset which involves images and dialogue contexts along with personality traits allocated to speakers, and proposed
a unified architecture using Transformer (Vaswani
et al., 2017) and ResNet (He et al., 2016). Shuster et al. (2020b) built a multi-task dialogue
agent using 12 open-domain dialogue datasets including ImageChat and IGC. On the success of
Blender (Roller et al., 2021), Shuster et al. (2021)
incorporated an image encoder (Xie et al., 2017)
to enable image-grounded conversation. Sun et al.
(2021) focused on generation models which generate either text or image responses conditioned on
the preceding textual dialogue contexts. To the best
of our knowledge, we propose a single integrated


system for multimodal dialogue response retrieval
for the first time.

**3** **Method**

**3.1** **Task: Multimodal Response Retrieval**

Multimodal response retrieval aims to select the
most appropriate response among the text candidates and image candidates for a given dialogue
context. The text is represented as a sequence of tokens _r_ **[t]** = ( _t_ 1 _, ..., t_ _l_ ) where each token is included
in a pre-defined vocabulary. The image is represented by 3-dimensional tensor _r_ **[i]** _∈_ R _[H][×][W]_ _[×][C]_ . To
solve this task, the model predicts the score _s_ ( _c, r_ )
that indicates how much each response _r_ in the
candidate pool is appropriate for a dialogue context
_c_ = ( _c_ 1 _, ..., c_ _l_ ) where _c_ _i_ is previous text utterance.
Using this score, the model selects the response
with the highest score for the given context. Therefore, the goal of the model is to accurately select
the ground truth response while harmonizing the
multimodal response candidates,

_r_ _[∗]_ = argmax _s_ ( _c, r_ ) _._
_r∈{r_ 1 **[t]** _[,...,r]_ _i_ **[t]** _[}∪{][r]_ 1 **[i]** _[,...,r]_ _j_ **[i]** _[}]_

**Subtask: Intent Prediction** (Zang et al., 2021)
aims to determine the modality of the next utterance given a context. In the case of two modalities (text and image), an intent prediction model
_f_ _i_ takes a context _c_ as input and produces a binary
logit. Given the pair of context and either image
response or text response, the binary cross entropy
loss is used for training as follows:

_L_ intent = [�] ( _c,r_ _[i]_ ) _[L]_ [BCE] [(] _[f]_ _[i]_ [(] _[c]_ [)] _[,]_ [ 1) +][ �] ( _c,r_ _[t]_ ) _[L]_ [BCE] [(] _[f]_ _[i]_ [(] _[c]_ [)] _[,]_ [ 0)] _[.]_

**Subtask: Text Response Retrieval** is to select
the most appropriate text response given the current
context. We adopt dual encoder architecture widely
used in response selection (Yang et al., 2018). To
be specific, a context encoder _f_ _c_ **[t]** [and a response en-]
coder _f_ _r_ **[t]** [compute the representations of a context]
_c_ and a response _r_ respectively. The cosine similarity between two representations is regarded as the
score, and the encoders are optimized to accurately
predict the score based on the cross entropy loss.
The loss computed from _i_ -th pair of context _c_ **[t]** _i_ [and]
text response _r_ _i_ **[t]** [is as follows:]

exp( _s_ **t** _,_ **t** ( _c_ **[t]** _i_ _[,][ r]_ _i_ **[t]** [))]
_L_ text = _−_ log
� ( _·,r_ _j_ **[t]** [)] _[∈][B]_ [ exp(] _[s]_ **[t]** _[,]_ **[t]** [(] _[c]_ _i_ **[t]** _[, r]_ _j_ **[t]** [))]

_−_ exp( _s_ **t** _,_ **t** ( _c_ **[t]** _i_ _[,][ r]_ _i_ **[t]** [))]
log
� ( _c_ **[t]** _j_ _[,][·]_ [)] _[∈][B]_ [ exp(] _[s]_ **[t]** _[,]_ **[t]** [(] _[c]_ _j_ **[t]** _[, r]_ _i_ **[t]** [))] _[,]_

(a) Dual Retriever (b) Shared Dual Retriever (c) Multimodal Dual Retriever

Figure 1: Three different architectures for the multimodal response retrieval task. _E_ _C_ : context encoder, _E_ _T R_ : text
response encoder, _E_ _IR_ : image response encoder.


where _s_ _·,·_ ( _c, r_ ) = cos( _f_ _c_ _[·]_ [(] _[c]_ [)] _[, f]_ _r_ _[·]_ [(] _[r]_ [))] [ for further no-]
tational simplicity.

**Subtask: Image Response Retrieval** handles
image responses during conversation. This task is
the same with text response retrieval except that
the modality of response is image. Therefore, the
response encoder is built on the pretrained image
encoder (He et al., 2016). The loss for the _i_ -th pair
of context _c_ **[t]** _i_ [and image response] _[ r]_ _i_ **[i]** [is described as]
follows:

exp( _s_ **t** _,_ **i** ( _c_ **[t]** _i_ _[, r]_ _i_ **[i]** [))]
_L_ image = _−_ log
� ( _·,r_ _j_ **[i]** [)] _[∈][B]_ [ exp(] _[s]_ **[t]** _[,]_ **[i]** [(] _[c]_ _i_ **[t]** _[, r]_ _j_ **[i]** [))]

_−_ exp( _s_ **t** _,_ **i** ( _c_ **[t]** _i_ _[, r]_ _i_ **[i]** [))]
log
� ( _c_ **[t]** _j_ _[,][·]_ [)] _[∈][B]_ [ exp(] _[s]_ **[t]** _[,]_ **[i]** [(] _[c]_ _j_ **[t]** _[, r]_ _i_ **[i]** [))] _[.]_

While the models for each subtasks are trained

separately, these models do not capture useful supervision across the subtasks and cannot effectively
solve the multimodal response retrieval task. Therefore, careful integration of these models become
important for the ultimate task. Considering the
knowledge derived across the subtasks, we introduce three approaches (Fig. 1) to integrate these
modules in the following subsections.

**3.2** **Dual Retriever (DR)**

One simple integration is to weave the separately
trained models. Each model is trained by three
different optimization problems:

minimize _L_ intent, minimize _L_ text, minimize _L_ image,
_θ_ _i_ _θ_ _c_ **[t]** _,θ_ _r_ **[t]** _θ_ _c_ **[t]** _,θ_ _r_ **[i]**


where _θ_ represents the parameters of each model.
Using these trained models, DR combines the produced outputs to obtain final outputs during inference. To be specific, the intent predictor predicts
the modality of the response for an input context.
Depending on the prediction, we select the corresponding retrieval model and then find out the most
appropriate response.


_f_ ( _c_ **[t]** ) =


argmax _r_ _i_ **i** _[s]_ **[t]** _[,]_ **[i]** [(] _[c]_ **[t]** _[, r]_ _i_ **[i]** [)] if _f_ _i_ ( _c_ ) _>_ 0 _._ 5
�argmax _r_ _i_ **t** _[s]_ **[t]** _[,]_ **[t]** [(] _[c]_ **[t]** _[, r]_ _i_ **[t]** [)] otherwise.


However, since the supervision from each modality
is not transferred across different retrievers, the
derived knowledge is not fully reflected to both
retrieval models.

**3.3** **Shared Dual Retriever (SDR)**

We propose a simple but effective scheme that
shares the retriever to encourage the models to
communicate with each other across the subtasks.

Our key idea comes from the observation that the
two retrieval tasks follow the same dual encoder

architecture. Also, although the architecture of
response encoder is different due to the different
modality, the architecture of context encoder is the
same. Thus, we can share the parameters of the
context encoder between the two subtasks without any modification of the architecture ( _θ_ _c_ **[t]** [=] _[ θ]_ _c_ **[t]** [).]
With the help of parameter sharing, we integrate the
optimization problems for two response retrieval
tasks:

minimize _L_ intent, minimize _L_ text + _L_ image _._
_θ_ _i_ _θ_ _c_ **[t]** _,θ_ _r_ **[t]** _,θ_ _r_ **[i]**

Furthermore, we make the context encoder inside the intent predictor ( _θ_ _i_ = _θ_ _c_ ) share its parameters with those of the response retrieval models.
By doing so, the separate optimization problems
are merged into a unified optimization problem for
the multimodal dialogue response retrieval task as
follows:

minimize _L_ intent + _L_ image + _L_ text _._
_θ_ _c_ **[t]** _,θ_ _r_ **[t]** _,θ_ _r_ **[i]**

We further hypothesize the ineffectiveness inside the inference process due to the intent predictor. One reason is the cascaded error coming
from the intent predictor. The intent predictor acts
as a branch for choosing the modality of the most
appropriate response. However, it means that the
final prediction result is wrong if the intent predictor predicts wrong modality. In addition, from the
recent success in modeling cross-modal representation space (Radford et al., 2021; Akbari et al.,
2021), we hypothesize that the response representation space from different modalities can become
naturally aligned.

**3.4** **Multimodal Dual Retriever (MDR)**

From the above hypothesis, we propose our final
integration approach by removing the intent predictor and modeling multimodal response representation space. We remove the intent predictor
and directly compare the cosine similarities across
different modality. Then an integrated response en
= =
coder is defined as _f_ _r_ **[m]** _f_ _r_ **[i]** [if] _[ r]_ [ =] _[ r]_ **[i]** [, or] _[ f]_ _r_ **[m]** _f_ _r_ **[t]**
if _r_ = _r_ **[t]** . The loss for the _i_ -th pair of the two
different modalities is defined as follows:

exp( _s_ **t** _,_ **m** ( _c_ **[t]** _i_ _[,][ r]_ _[i]_ [))]
_L_ joint = _−_ log
� ( _·,r_ _j_ ) _∈B_ [exp(] _[s]_ **[t]** _[,]_ **[m]** [(] _[c]_ **[t]** _i_ _[, r]_ _[j]_ [))]

_−_ exp( _s_ **t** _,_ **m** ( _c_ **[t]** _i_ _[,][ r]_ _[i]_ [))]
log
� ( _c_ _j_ _,·_ ) _∈B_ [exp(] _[s]_ **[t]** _[,]_ **[m]** [(] _[c]_ **[t]** _j_ _[, r]_ _[i]_ [))] _[,]_

where a batch _B_ consists of context-response pairs
whose response is either image or text. All the parameters can be effectively optimized to minimize
the joint loss in an end-to-end manner,

minimize _L_ _._
joint
_θ_ _c_ _,θ_ _r_ **t** _,θ_ _r_ **i**

Note that the absence of the intent predictor further
simplifies the inference process by taking the response that has the most highest score among all
the multimodal candidates.


**4** **Experimental Setup**

**4.1** **Datasets**

**PhotoChat** is a multimodal dialogue dataset
which includes dyadic dialogues covering various topics in daily lives. It consists of
10,286/1,000/1,000 dialogue contexts with a single
image attached to each context in the train/dev/test
set. There exist 8,889/1,000/1,000 unique images
along with object labels in the train/dev/test set.

**Multi-modal Dialogue (MMDial)** (Lee et al.,
2021) was constructed by substituting text utterances in existing text-only dialogue datasets
with relevant images from large-scale image
datasets using a state-of-the-art image-text matching model (Li et al., 2019). It consists of
39,956/2,401/2,673 dialogue contexts with a single
image attached to each context in the train/dev/test
set. There exist 12,272/334/682 unique images in
the train/dev/test set.

**Preprocessing.** For each dialogue context, we
extract the first _n ∈{_ 1 _,_ 2 _, . . ., l}_ utterances as an
example to augment contexts, where _l_ is the length
of a context before sharing an image. Since object
labels are not explicitly attached in MMDial, we
match the corresponding image captions to images
from MS-COCO and Flickr30k.

**4.2** **Metric**

We measure recall at _k_ (R@ _k_ ) to evaluate multimodal response retrieval. It calculates the occurrence when the gold response is retrieved in top- _k_
( _k_ = 1 _,_ 5 _,_ 10 ) candidates. For dev and test, we use
the fixed 50 candidates of each modality randomly
chosen from the whole images and text responses
in the dev and the test set, respectively. We use
the checkpoint that achieves the best dev R@ _k_ and
report the test R@ _k_ evaluated by the checkpoint.

**5** **Results**

**5.1** **Effects of Integration Methods**

In contrast to relatively small gaps of R@ _k_ among
the three methods in unimodal retrieval, the end
task performance is largely affected depending on
how we integrate the subcomponents. On PhotoChat (Table 1), SDR achieves the highest R@ _k_
for multimodal retrieval, except that MDR reaches
slightly higher R@10 when the model size is large.
Meanwhile on MMDial (Table 2), MDR achieves
the highest R@ _k_ for multimodal retrieval, except

Text Retrieval Image Retrieval Multimodal Retrieval

Model Size Method R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10

DR **0.4325** 0.7371 0.8701 **0.4519** **0.7994** **0.9007** 0.2475 0.4234 0.4949

Small SDR 0.4274 **0.7553** **0.8721** 0.4396 0.7787 0.8873 **0.4000** **0.7046** **0.8086**

MDR 0.3990 0.7076 0.8518 0.4122 0.7269 0.8599 0.3766 0.6594 0.7721

DR 0.4305 0.7482 0.8751 **0.4620** **0.8247** **0.9058** 0.2079 0.3560 0.4143
Large SDR **0.4650** **0.7990** **0.9066** 0.4315 0.8061 0.9046 **0.4152** **0.7239** 0.8157
MDR 0.4315 0.7299 0.8812 0.4406 0.7797 0.8964 0.4020 0.6949 **0.8193**

Table 1: Results on the test set of PhotoChat. Text (image) retrieval: retrieval among 50 text (image) response
candidates for the examples whose ground truths are text (image) responses, assuming the intent predictor is an
oracle for DR and SDR . MDR does not get advantage in this setting since there is no explicit intent prediction step.
Small: BERT M INI and ResNet 50, Large: BERT B ASE and ResNet 152 (Turc et al., 2019).

Text Retrieval Image Retrieval Multimodal Retrieval

Model Size Method R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10

DR 0.6118 0.8039 **0.8946** 0.0430 0.2148 0.3687 **0.2874** 0.3749 0.4161

Small SDR **0.6138** **0.8115** 0.8870 **0.1356** **0.3958** **0.5430** 0.2619 0.3934 0.4527

MDR 0.5386 0.7558 0.8512 0.1281 0.3846 0.5334 0.2731 **0.4517** **0.5378**

DR **0.6333** **0.8453** **0.9141** 0.0605 0.2204 0.3544 0.2900 0.3835 0.4125
Large SDR 0.5986 0.8313 0.8986 **0.1484** **0.4300** **0.6002** 0.2516 0.4113 0.4735
MDR 0.5967 0.8202 0.8938 0.1281 0.4097 0.5728 **0.3079** **0.4883** **0.5895**

Table 2: Results on the test set of MMDial. Defined terms are the same as in Table 1.


that DR scores the highest R@1 for the small
model. We note that although DR outperforms
MDR in text retrieval and image retrieval, MDR
outperforms DR on the contrary due to the cascaded error from the intent prediction step. These
results highlight the significance of choosing an
appropriate integration method of submodules.

**5.2** **Effects of Model Size**

Overall, all the methods in large models tend to
achieve higher R@ _k_ than their counterparts in
small models on the both datasets. On PhotoChat,
MDR shows comparable performance for multimodal retrieval to SDR when the model size grows
large. Similarly on MMDial, MDR effectively increases R@ _k_ for multimodal retrieval compared to
the two-step approach when the large model is used.
From these results, we can conclude that large models are more capable of aligning the multimodal
representation space than small models.

**5.3** **Effects of Parameter Sharing**

On the both datasets, the performance of DR for
multimodal retrieval lags behind those of SDR and
MDR which share the parameters of context encoders. DR trains each submodule separately on
the three individual subtasks so none of the sub
components can get the knowledge from the other


subtasks and modalities positively transferred, resulting in disharmony for accomplishing the ultimate goal (Wu et al., 2021).
In addition, weight sharing decreases the number
of total parameters from 72M to 49M in small models and from 501M to 281M in large models, which
become around 1.5x and 1.8x smaller respectively.

**6** **Conclusion**

We propose an integrated task to build a multimodal
dialogue system that outputs both text and image
responses, and present three architectures for the
task, named DR, SDR, and MDR, respectively. We
then analyze their advantages and disadvantages in
terms of retrieval performance and model size.
Specifically, we empirically analyze the effectiveness of intent prediction which was introduced
in the previous work. The experimental results
on two datasets demonstrate that the end-to-end

approach without an intent predictor shows competitiveness for multimodal retrieval compared to
the two-step approach. In addition, SDR and MDR
successfully reduce the number of model parameters without compromising the end task performance. As the end-to-end approach is the efficient
method to solve the end task directly, elaborating
cross-modal interactions in this architecture will be
come one promising direction for our future work.

**References**

Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong
Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong.
2021. Vatt: [Transformers for multimodal self-](https://arxiv.org/abs/2104.11178)
[supervised learning from raw video, audio and text.](https://arxiv.org/abs/2104.11178)
_arXiv preprint arXiv:2104.11178_ .

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and
[Devi Parikh. 2015. Vqa: Visual question answering.](https://openaccess.thecvf.com/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf)
In _Proceedings of the IEEE international conference_
_on computer vision_, pages 2425–2433.

Wei Chen, Yeyun Gong, Can Xu, Huang Hu, Bolun
Yao, Zhongyu Wei, Zhihao Fan, Xiaowu Hu, Bartuer
[Zhou, Biao Cheng, et al. 2021. Contextual fine-to-](https://arxiv.org/abs/2109.13087)
[coarse distillation for coarse-grained response selec-](https://arxiv.org/abs/2109.13087)
[tion in open-domain conversations.](https://arxiv.org/abs/2109.13087) _arXiv preprint_
_arXiv:2109.13087_ .

Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,
Deshraj Yadav, José MF Moura, Devi Parikh, and
[Dhruv Batra. 2017. Visual dialog. In](https://openaccess.thecvf.com/content_cvpr_2017/papers/Das_Visual_Dialog_CVPR_2017_paper.pdf) _Proceedings_
_of the IEEE Conference on Computer Vision and_
_Pattern Recognition_, pages 326–335.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
[Kristina Toutanova. 2019. BERT: Pre-training of](https://doi.org/10.18653/v1/N19-1423)
[deep bidirectional transformers for language under-](https://doi.org/10.18653/v1/N19-1423)
[standing. In](https://doi.org/10.18653/v1/N19-1423) _Proceedings of the 2019 Conference of_
_the North American Chapter of the Association for_
_Computational Linguistics: Human Language Tech-_
_nologies, Volume 1 (Long and Short Papers)_, pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.

Emily Dinan, Stephen Roller, Kurt Shuster, Angela
[Fan, Michael Auli, and Jason Weston. 2019. Wizard](https://openreview.net/forum?id=r1l73iRqKm)
[of wikipedia: Knowledge-powered conversational](https://openreview.net/forum?id=r1l73iRqKm)
[agents. In](https://openreview.net/forum?id=r1l73iRqKm) _International Conference on Learning_
_Representations_ .

Jia-Chen Gu, Tianda Li, Quan Liu, Zhen-Hua Ling,
Zhiming Su, Si Wei, and Xiaodan Zhu. 2020.
[Speaker-aware bert for multi-turn response selection](https://dl.acm.org/doi/10.1145/3340531.3412330)
[in retrieval-based chatbots. In](https://dl.acm.org/doi/10.1145/3340531.3412330) _Proceedings of the_
_29th ACM International Conference on Information_
_& Knowledge Management_, pages 2041–2044.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
[Sun. 2016. Deep residual learning for image recogni-](https://doi.org/10.1109/CVPR.2016.90)
[tion. In](https://doi.org/10.1109/CVPR.2016.90) _2016 IEEE Conference on Computer Vision_
_and Pattern Recognition (CVPR)_, pages 770–778.

Matthew Henderson, Rami Al-Rfou, Brian Strope, YunHsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Ku[mar, Balint Miklos, and Ray Kurzweil. 2017. Effi-](https://arxiv.org/abs/1705.00652)
[cient natural language response suggestion for smart](https://arxiv.org/abs/1705.00652)
[reply.](https://arxiv.org/abs/1705.00652) _arXiv preprint arXiv:1705.00652_ .

Matthew Henderson, Ivan Vuli´c, Daniela Gerz, Iñigo
Casanueva, Paweł Budzianowski, Sam Coope, Georgios Spithourakis, Tsung-Hsien Wen, Nikola Mrkši´c,
and Pei-Hao Su. 2019. [Training neural response](https://doi.org/10.18653/v1/P19-1536)


[selection for task-oriented dialogue systems. In](https://doi.org/10.18653/v1/P19-1536) _Pro-_
_ceedings of the 57th Annual Meeting of the Asso-_
_ciation for Computational Linguistics_, pages 5392–
5404, Florence, Italy. Association for Computational
Linguistics.

Yuheng Hu, Lydia Manikonda, and Subbarao Kamb[hampati. 2014. What we instagram: A first analysis](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8118)
[of instagram photo content and user types. In](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8118) _Eighth_
_International AAAI conference on weblogs and social_
_media_ .

Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
[Alex Acero, and Larry Heck. 2013. Learning deep](https://dl.acm.org/doi/10.1145/2505515.2505665)
[structured semantic models for web search using](https://dl.acm.org/doi/10.1145/2505515.2505665)
[clickthrough data. In](https://dl.acm.org/doi/10.1145/2505515.2505665) _Proceedings of the 22nd ACM_
_international conference on Information & Knowl-_
_edge Management_, pages 2333–2338.

Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,
[and Jason Weston. 2020. Poly-encoders: Architec-](https://openreview.net/forum?id=SkxgnnNFvH)
[tures and pre-training strategies for fast and accurate](https://openreview.net/forum?id=SkxgnnNFvH)
[multi-sentence scoring. In](https://openreview.net/forum?id=SkxgnnNFvH) _International Conference_
_on Learning Representations_ .

[Diederik P Kingma and Jimmy Ba. 2015. Adam: A](https://arxiv.org/abs/1412.6980)
[method for stochastic optimization. In](https://arxiv.org/abs/1412.6980) _ICLR (Poster)_ .

Tian Lan, Deng Cai, Yan Wang, Yixuan Su, Xian-Ling
[Mao, and Heyan Huang. 2021. Exploring dense re-](https://arxiv.org/abs/2110.06612)
[trieval for dialogue response selection.](https://arxiv.org/abs/2110.06612) _arXiv preprint_
_arXiv:2110.06612_ .

Nyoungwoo Lee, Suwon Shin, Jaegul Choo, Ho-Jin
[Choi, and Sung-Hyon Myaeng. 2021. Construct-](https://doi.org/10.18653/v1/2021.acl-short.113)
[ing multi-modal dialogue dataset by replacing text](https://doi.org/10.18653/v1/2021.acl-short.113)
[with semantically relevant images. In](https://doi.org/10.18653/v1/2021.acl-short.113) _Proceedings_
_of the 59th Annual Meeting of the Association for_
_Computational Linguistics and the 11th International_
_Joint Conference on Natural Language Processing_
_(Volume 2: Short Papers)_, pages 897–906, Online.
Association for Computational Linguistics.

Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and
[Yun Fu. 2019. Visual semantic reasoning for image-](https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Visual_Semantic_Reasoning_for_Image-Text_Matching_ICCV_2019_paper.pdf)
[text matching. In](https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Visual_Semantic_Reasoning_for_Image-Text_Matching_ICCV_2019_paper.pdf) _Proceedings of the IEEE/CVF In-_
_ternational Conference on Computer Vision (ICCV)_ .

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. [Microsoft coco:](https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48)
[Common objects in context. In](https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48) _European confer-_
_ence on computer vision_, pages 740–755. Springer.

Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle
[Pineau. 2015. The Ubuntu dialogue corpus: A large](https://doi.org/10.18653/v1/W15-4640)
[dataset for research in unstructured multi-turn dia-](https://doi.org/10.18653/v1/W15-4640)
[logue systems. In](https://doi.org/10.18653/v1/W15-4640) _Proceedings of the 16th Annual_
_Meeting of the Special Interest Group on Discourse_
_and Dialogue_, pages 285–294, Prague, Czech Republic. Association for Computational Linguistics.

Pierre-Emmanuel Mazaré, Samuel Humeau, Martin Rai[son, and Antoine Bordes. 2018. Training millions of](https://doi.org/10.18653/v1/D18-1298)
[personalized dialogue agents. In](https://doi.org/10.18653/v1/D18-1298) _Proceedings of the_
_2018 Conference on Empirical Methods in Natural_

_Language Processing_, pages 2775–2779, Brussels,
Belgium. Association for Computational Linguistics.

Nasrin Mostafazadeh, Chris Brockett, Bill Dolan,
Michel Galley, Jianfeng Gao, Georgios Spithourakis,
[and Lucy Vanderwende. 2017. Image-grounded con-](https://aclanthology.org/I17-1047)
[versations: Multimodal context for natural ques-](https://aclanthology.org/I17-1047)
[tion and response generation.](https://aclanthology.org/I17-1047) In _Proceedings of_
_the Eighth International Joint Conference on Nat-_
_ural Language Processing (Volume 1: Long Papers)_,
pages 462–472, Taipei, Taiwan. Asian Federation of
Natural Language Processing.

Bryan A. Plummer, Liwei Wang, Chris M. Cervantes,
Juan C. Caicedo, Julia Hockenmaier, and Svetlana
Lazebnik. 2015. [Flickr30k entities:](https://openaccess.thecvf.com/content_iccv_2015/papers/Plummer_Flickr30k_Entities_Collecting_ICCV_2015_paper.pdf) Collecting
[region-to-phrase correspondences for richer image-](https://openaccess.thecvf.com/content_iccv_2015/papers/Plummer_Flickr30k_Entities_Collecting_ICCV_2015_paper.pdf)
[to-sentence models. In](https://openaccess.thecvf.com/content_iccv_2015/papers/Plummer_Flickr30k_Entities_Collecting_ICCV_2015_paper.pdf) _Proceedings of the IEEE In-_
_ternational Conference on Computer Vision (ICCV)_ .

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. [Learning transferable visual models](https://arxiv.org/abs/2103.00020)
[from natural language supervision.](https://arxiv.org/abs/2103.00020) _arXiv preprint_
_arXiv:2103.00020_ .

Stephen Roller, Emily Dinan, Naman Goyal, Da Ju,
Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott,
Eric Michael Smith, Y-Lan Boureau, and Jason We[ston. 2021. Recipes for building an open-domain](https://aclanthology.org/2021.eacl-main.24)
[chatbot. In](https://aclanthology.org/2021.eacl-main.24) _Proceedings of the 16th Conference of_
_the European Chapter of the Association for Compu-_
_tational Linguistics: Main Volume_, pages 300–325,
Online. Association for Computational Linguistics.

Kurt Shuster, Samuel Humeau, Antoine Bordes, and Ja[son Weston. 2020a. Image-chat: Engaging grounded](https://doi.org/10.18653/v1/2020.acl-main.219)
[conversations. In](https://doi.org/10.18653/v1/2020.acl-main.219) _Proceedings of the 58th Annual_
_Meeting of the Association for Computational Lin-_
_guistics_, pages 2414–2429, Online. Association for
Computational Linguistics.

Kurt Shuster, Da Ju, Stephen Roller, Emily Dinan, Y[Lan Boureau, and Jason Weston. 2020b. The dia-](https://doi.org/10.18653/v1/2020.acl-main.222)
[logue dodecathlon: Open-domain knowledge and im-](https://doi.org/10.18653/v1/2020.acl-main.222)
[age grounded conversational agents. In](https://doi.org/10.18653/v1/2020.acl-main.222) _Proceedings_
_of the 58th Annual Meeting of the Association for_
_Computational Linguistics_, pages 2453–2470, Online. Association for Computational Linguistics.

Kurt Shuster, Eric Michael Smith, Da Ju, and Jason
[Weston. 2021. Multi-modal open-domain dialogue.](https://aclanthology.org/2021.emnlp-main.398)
In _Proceedings of the 2021 Conference on Empiri-_
_cal Methods in Natural Language Processing_, pages
4863–4883, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Qingfeng Sun, Yujing Wang, Can Xu, Kai Zheng,
Yaming Yang, Huang Hu, Fei Xu, Jessica Zhang,
Xiubo Geng, and Daxin Jiang. 2021. [Multi-](https://arxiv.org/abs/2110.08515)
[modal dialogue response generation.](https://arxiv.org/abs/2110.08515) _arXiv preprint_
_arXiv:2110.08515_ .


Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina
[Toutanova. 2019. Well-read students learn better:](https://arxiv.org/abs/1908.08962)
[On the importance of pre-training compact models.](https://arxiv.org/abs/1908.08962)
_arXiv preprint arXiv:1908.08962_ .

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
[Kaiser, and Illia Polosukhin. 2017. Attention is all](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
[you need. In](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) _Advances in neural information pro-_
_cessing systems_, pages 5998–6008.

Taesun Whang, Dongyub Lee, Chanhee Lee, Kisu Yang,
[Dongsuk Oh, and Heuiseok Lim. 2020. An effec-](https://arxiv.org/abs/1908.04812)
[tive domain adaptive post-training method for bert in](https://arxiv.org/abs/1908.04812)
[response selection. In](https://arxiv.org/abs/1908.04812) _INTERSPEECH_, pages 1585–
1589.

Chien-Sheng Wu, Steven C.H. Hoi, Richard Socher,
[and Caiming Xiong. 2020. TOD-BERT: Pre-trained](https://doi.org/10.18653/v1/2020.emnlp-main.66)
[natural language understanding for task-oriented di-](https://doi.org/10.18653/v1/2020.emnlp-main.66)
[alogue. In](https://doi.org/10.18653/v1/2020.emnlp-main.66) _Proceedings of the 2020 Conference on_
_Empirical Methods in Natural Language Processing_
_(EMNLP)_, pages 917–929, Online. Association for
Computational Linguistics.

Ruihan Wu, Chuan Guo, Awni Hannun, and Lau[rens van der Maaten. 2021. Fixes that fail: Self-](https://openreview.net/forum?id=xZvuqfT6Otj)
[defeating improvements in machine-learning systems.](https://openreview.net/forum?id=xZvuqfT6Otj)
In _Thirty-Fifth Conference on Neural Information_
_Processing Systems_ .

[Yu Wu, Zhoujun Li, Wei Wu, and Ming Zhou. 2018. Re-](https://www.sciencedirect.com/science/article/abs/pii/S0925231218309093)
[sponse selection with topic clues for retrieval-based](https://www.sciencedirect.com/science/article/abs/pii/S0925231218309093)
[chatbots.](https://www.sciencedirect.com/science/article/abs/pii/S0925231218309093) _Neurocomputing_, 316:251–261.

Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu,
[and Kaiming He. 2017. Aggregated residual transfor-](https://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf)
[mations for deep neural networks. In](https://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf) _Proceedings of_
_the IEEE conference on computer vision and pattern_
_recognition_, pages 1492–1500.

Ruijian Xu, Chongyang Tao, Daxin Jiang, Xueliang
[Zhao, Dongyan Zhao, and Rui Yan. 2021a. Learning](https://ojs.aaai.org/index.php/AAAI/article/view/17666)
[an effective context-response matching model with](https://ojs.aaai.org/index.php/AAAI/article/view/17666)
[self-supervised tasks for retrieval-based dialogues.](https://ojs.aaai.org/index.php/AAAI/article/view/17666)
_Proceedings of the AAAI Conference on Artificial_
_Intelligence_, 35(16):14158–14166.

[Yi Xu, Hai Zhao, and Zhuosheng Zhang. 2021b. Topic-](https://ojs.aaai.org/index.php/AAAI/article/view/17668)
[aware multi-turn dialogue modeling. In](https://ojs.aaai.org/index.php/AAAI/article/view/17668) _The Thirty-_
_Fifth AAAI Conference on Artificial Intelligence_
_(AAAI-21)_ .

[Rui Yan, Yiping Song, and Hua Wu. 2016. Learning to](https://dl.acm.org/doi/10.1145/2911451.2911542)
[respond with deep neural networks for retrieval-based](https://dl.acm.org/doi/10.1145/2911451.2911542)
[human-computer conversation system. In](https://dl.acm.org/doi/10.1145/2911451.2911542) _Proceed-_
_ings of the 39th International ACM SIGIR conference_
_on Research and Development in Information Re-_
_trieval_, pages 55–64.

Zhao Yan, Nan Duan, Junwei Bao, Peng Chen, Ming
[Zhou, and Zhoujun Li. 2018. Response selection](https://www.sciencedirect.com/science/article/abs/pii/S0950705117305646)
[from unstructured documents for human-computer](https://www.sciencedirect.com/science/article/abs/pii/S0950705117305646)
[conversation systems.](https://www.sciencedirect.com/science/article/abs/pii/S0950705117305646) _Knowledge-Based Systems_,
142:149–159.

Yinfei Yang, Steve Yuan, Daniel Cer, Sheng-yi Kong,
Noah Constant, Petr Pilar, Heming Ge, Yun-Hsuan
[Sung, Brian Strope, and Ray Kurzweil. 2018. Learn-](https://doi.org/10.18653/v1/W18-3022)
[ing semantic textual similarity from conversations.](https://doi.org/10.18653/v1/W18-3022)
In _Proceedings of The Third Workshop on Represen-_
_tation Learning for NLP_, pages 164–174, Melbourne,
Australia. Association for Computational Linguistics.

Xiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song,
[Hao Zhang, and Jindong Chen. 2021. PhotoChat: A](https://doi.org/10.18653/v1/2021.acl-long.479)
[human-human dialogue dataset with photo sharing](https://doi.org/10.18653/v1/2021.acl-long.479)
[behavior for joint image-text modeling. In](https://doi.org/10.18653/v1/2021.acl-long.479) _Proceed-_
_ings of the 59th Annual Meeting of the Association for_
_Computational Linguistics and the 11th International_
_Joint Conference on Natural Language Processing_
_(Volume 1: Long Papers)_, pages 6142–6152, Online.
Association for Computational Linguistics.

Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin
Choi. 2019. [From recognition to cognition: Vi-](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zellers_From_Recognition_to_Cognition_Visual_Commonsense_Reasoning_CVPR_2019_paper.pdf)
[sual commonsense reasoning. In](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zellers_From_Recognition_to_Cognition_Visual_Commonsense_Reasoning_CVPR_2019_paper.pdf) _Proceedings of the_
_IEEE/CVF Conference on Computer Vision and Pat-_
_tern Recognition_, pages 6720–6731.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
[Szlam, Douwe Kiela, and Jason Weston. 2018. Per-](https://doi.org/10.18653/v1/P18-1205)
[sonalizing dialogue agents: I have a dog, do you](https://doi.org/10.18653/v1/P18-1205)
[have pets too? In](https://doi.org/10.18653/v1/P18-1205) _Proceedings of the 56th Annual_
_Meeting of the Association for Computational Lin-_
_guistics (Volume 1: Long Papers)_, pages 2204–2213,
Melbourne, Australia. Association for Computational
Linguistics.

Xiangyang Zhou, Daxiang Dong, Hua Wu, Shiqi Zhao,
Dianhai Yu, Hao Tian, Xuan Liu, and Rui Yan. 2016.
[Multi-view response selection for human-computer](https://doi.org/10.18653/v1/D16-1036)
[conversation. In](https://doi.org/10.18653/v1/D16-1036) _Proceedings of the 2016 Conference_
_on Empirical Methods in Natural Language Process-_
_ing_, pages 372–381, Austin, Texas. Association for
Computational Linguistics.

**A** **Appendix**

**A.1** **Implementation Details**

**Architectural details of encoders** The context

encoder ( _E_ _C_ ) and the text response encoder ( _E_ _TR_ )
consist of a single BERT encoder followed by a projection layer. The parameters of two encoders are
shared across all experiments since this yields better performance consistently than when not shared.
The image response encoder ( _E_ _IR_ ) consists of an
image encoder which extracts visual features with
ResNet and a object label encoder which extracts
object label features with BERT. Note that both
BERT and ResNet are used to encode image features since an object label is attached to every image. The two representations are then concatenated
and projected to the joint embedding space. We
use BERT M INI and ResNet 50 for small models and
BERT B ASE and ResNet 152 for large models as specified by Turc et al. (2019).

**Hyperparameters** For BERT, the dropout rate
is 0.2 and the maximum sequence length is set to
128. We use cosine similarity with the temperature
_τ_ = 0 _._ 01 as the similarity measure. We train the
model for 10 epochs for intent prediction and text
text dialogue retrieval, and 20 epochs for image dialogue retrieval and multimodal dialogue retrieval
with the batch size of 64 in one V100 GPU. We apply Adam optimizer (Kingma and Ba, 2015) with
the learning rate of 5e-5 and linear decay of 0.1%
per 1,000 steps.

