## **FlashRAG: A Modular Toolkit for Efficient** **Retrieval-Augmented Generation Research**

**Jiajie Jin** **Yutao Zhu** _[∗]_ **Guanting Dong** **Yuyao Zhang** **Xinyu Yang**
**Chenghao Zhang** **Tong Zhao** **Zhao Yang** **Zhicheng Dou** _[∗]_ **Ji-Rong Wen**
Gaoling School of Artificial Intelligence, Renmin University of China
```
       {jinjiajie, dou}@ruc.edu.cn, yutaozhu94@gmail.com
```

[Homepage](https://ruc-nlpir.github.io/FlashRAG/) [Datasets](https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets) [Datasets](https://www.modelscope.cn/datasets/hhjinjiajie/FlashRAG_Dataset) [Code](https://github.com/RUC-NLPIR/FlashRAG)


**Abstract**


With the advent of large language models (LLMs) and multimodal large language
models (MLLMs), the potential of retrieval-augmented generation (RAG) has
attracted considerable research attention. Various novel algorithms and models
have been introduced to enhance different aspects of RAG systems. However,
the absence of a standardized framework for implementation, coupled with the
inherently complex RAG process, makes it challenging and time-consuming for
researchers to compare and evaluate these approaches in a consistent environment.
Existing RAG toolkits, such as LangChain and LlamaIndex, while available, are
often heavy and inflexibly, failing to meet the customization needs of researchers.
In response to this challenge, we develop FlashRAG, an efficient and modular
open-source toolkit designed to assist researchers in reproducing and comparing
existing RAG methods and developing their own algorithms within a unified
framework. Our toolkit has implemented 16 advanced RAG methods and gathered
and organized 38 benchmark datasets. It has various features, including a
customizable modular framework, multimodal RAG capabilities, a rich collection
of pre-implemented RAG works, comprehensive datasets, efficient auxiliary preprocessing scripts, and extensive and standard evaluation metrics.


**1** **Introduction**


In the era of large language models (LLMs), retrieval-augmented generation (RAG) [ 1, 2 ] has
emerged as an effective solution to mitigate hallucination issues by leveraging external knowledge
bases [ 3, 4, 5 ]. The substantial applications and potential of RAG have attracted considerable research
attention [ 6, 7, 8 ]. However, with the introduction of a large number of new algorithms and models in
recent years, comparing and evaluating these methods in a consistent setting has become increasingly
challenging.


Many existing methods are not open-source or require specific configurations for implementation,
making adaptation to custom data and innovative components challenging. The datasets and retrieval
corpora often vary, with resources being scattered and requiring considerable pre-processing efforts.
Besides, the inherent complexity of RAG systems, which involve indexing, retrieval, and generation,
often demands extensive technical implementation. While there are some existing RAG toolkits such
as LangChain [ 9 ] and LlamaIndex [ 10 ], they are typically complex and cumbersome, restricting
researchers from tailoring processes to their specific needs. Therefore, there is a clear demand for a


_∗_ Corresponding authors.


Preprint. Under review.


Table 1: The overall comparison between FlashRAG and other toolkits.


**Automatic** **Corpus** **# Provided** **# Support**
**Toolkit** **Multimodal**
**Evaluation** **Helper** **Dataset** **Methods**


Langchain [9] ✕ ✓ ✓  - 2
LlamaIndex [10] ✓ ✓ ✓  - 2
Haystack [18] ✓ ✕ ✕  -  FastRAG [19] ✕ ✕ ✕ 2 1
LocalRQA [20] ✓ ✕ ✕ 3  AutoRAG [21] ✓ ✕ ✕ 4 2
RAGLab [22] ✓ ✕ ✓ 10 6
**FlashRAG** (ours) ✓ ✓ ✓ **38** **16**


unified, research-focused RAG toolkit to simplify method development and facilitate comparative
studies.


To address the aforementioned issues, we introduce FlashRAG, an open-source library that empowers
researchers to reproduce, benchmark, and innovate within the RAG domain efficiently. This library
offers built-in pipelines for replicating existing work, customizable components for crafting tailored
RAG workflows, and streamlined access to organized datasets and corpora to accelerate research
processes. FlashRAG provides a more researcher-friendly solution compared to existing toolkits. To
summarize, the key features of our FlashRAG library include:


**A comprehensive, customizable, and efficient modular RAG framework.** FlashRAG offers
a highly modular setup at both the component and pipeline levels, featuring 5 core modules and
16 diverse RAG subcomponents that can be independently integrated or combined into pipelines.
Additionally, we provide 9 standardized RAG processes and auxiliary scripts for tasks such as
downloading and chunking Wikipedia for corpus construction, building retrieval indexes, and
preparing retrieval results, resulting in an efficient and user-friendly end-to-end RAG framework.


**Pre-implemented advanced RAG algorithms.** To our knowledge, FlashRAG has provided the
most comprehensive implementation of existing work, including 16 advanced RAG algorithms, such
as Self-RAG [ 11 ] and FLARE [ 12 ], covering sequential, conditional, branching, and loop RAG
categories. These methods are evaluated within a unified framework, and benchmark reports are
available, supporting transparent evaluation and comparison. More approaches will continue to be
incorporated into our library.


**Support for multi-modal RAG scenarios.** FlashRAG supports comprehensive multi-modal RAG
deployments by integrating mainstream multi-modal large language models (MLLMs) like Qwen [ 13 ],
InternVL [ 14 ] and LLaVA [ 15, 16 ], along with various CLIP-based retrievers [ 17 ]. The framework
provides researchers with extensive technical support across both text-only and multi-modal scenarios,
equipped with multiple widely-used MRAG benchmark datasets for systematic evaluation.


**Comprehensive benchmark datasets.** To improve the consistency and utility of datasets in RAG
research, we have collected 38 commonly used datasets and standardized their formats. Some of
these datasets, such as WikiAsp [ 23 ] and NarrativeQA [ 24 ], have undergone specific adjustments
for RAG scenarios to ensure consistency. These datasets are readily available on the HuggingFace
platform, facilitating easy access and application.


**Visual web interface for RAG experimentation.** FlashRAG features an intuitive web interface
that visualizes the complete RAG pipeline. Users can inspect intermediate results at each step, from
retrieval to answer generation, while performing one-click parameter tuning and automatic benchmark
evaluation. The interface enables seamless corpus loading, real-time component visualization, and
comprehensive pipeline assessment, making RAG experimentation more transparent and efficient.


**2** **Background and Related work**


**2.1** **Retrieval-Augmented Generation (RAG)**


Hallucinations and factual inaccuracies present significant challenges in existing LLMs [ 3, 25, 26, 27 ].
To address these problems, RAG has been introduced. The typical RAG architecture includes a
retriever and a generator ( _e.g._, LLMs). The retriever retrieves relevant passages from an external
knowledge base in response to a user’s query [ 28, 29, 30 ], and then the generator uses these passages


2


as part of the input for generation. By leveraging external knowledge, the quality of the generation
can be significantly enhanced [ 31, 32, 33, 34 ]. As research in the RAG field progresses, various
additional components have been introduced to further improve the effectiveness and robustness of
RAG systems [ 6, 35, 36 ]. This motivates us to develop an accessible, user-friendly toolkit designed
specifically to support and facilitate RAG research.


**2.2** **RAG Toolkits**


RAG typically involves complex components and preliminary engineering tasks, such as corpus
construction and generator setup. Due to the absence of a dedicated RAG library for research, existing
open-source codes often use custom implementations with specific environment configurations,
hindering code reusability and adaptation to new settings.


Recent developments have seen the introduction of several open-source RAG toolkits such as
Langchain [ 9 ], LlamaIndex [ 10 ], and Haystack [ 18 ]. These libraries provide advanced APIs facilitate
interactions with LLMs, such as vector databases and embedding models, simplifying the execution of
RAG processes. However, these toolkits generally do not cater to the needs of the research community.
They often lack comprehensive implementations of existing RAG methods, do not provide access to
commonly used retrieval corpora, and are typically heavy and overly encapsulated, which obscures
details and complicates customization.


To address these challenges, lighter and more adaptable RAG toolkits have emerged. For example,
FastRAG [ 19 ] builds upon Haystack’s API to offer optimized support with a select range of methods
and datasets. LocalRQA [ 20 ] focuses on the training stage of the RAG process, providing training
scripts for various components (such as retrievers and generators). AutoRAG [ 21 ] adopts a modular
approach by treating each RAG component as a node that connects to form the overall process,
but it lacks the implementation of existing RAG methods and their evaluations. In contrast, our
FlashRAG toolkit provides a wide array of RAG components and pre-implemented methods, allowing
for straightforward replication of existing RAG studies in various settings with a few lines of code.
Furthermore, we provide a rich resource pool, including a large number of processed datasets and
scripts for obtaining and pre-processing widely used corpora, significantly reducing the preparatory
time required for researchers.


**3** **The Toolkit: FlashRAG**


FlashRAG is designed primarily to support research in RAG, although its capabilities are not
confined to this area alone. As illustrated in Figure 1, FlashRAG comprises three hierarchical
modules: the environment module, the component module, and the pipeline module. The environment
module is fundamental to the toolkit, providing essential resources such as datasets, hyperparameters,
and evaluation metrics, for experiments. Building upon the environment module, the component
module consists of various RAG components, each tailored for specific functions ( _e.g._, retrieval and
generation). The pipeline module integrates these components into a complete RAG process. In this
paper, we will introduce the component and pipeline modules. Additional details are available in the
documentation of our library.


**3.1** **Component Module**


FlashRAG is organized into five main components, each designed to function autonomously or within
a combined application, enhancing both flexibility and specificity in the RAG process.


**Judger** functions as a preliminary component that determines whether a query needs retrieval [ 37,
38, 39 ]. Given the limited studies in this domain, we implement a judger based on SKR [ 37 ], which
utilizes LLM self-knowledge data to determine the necessity of retrieval.


**Retriever** implementations are extensively covered by our toolkit. For sparse retrieval, we integrate
the Pyserini library [ 40 ] to support the use of BM25 [ 41 ]. For dense retrieval, we consider various
BERT-based embedding models such as DPR [ 42 ], E5 [ 43 ], and BGE [ 44 ], along with T5-based
models such as ANCE [ 45 ]. We employ FAISS [ 46, 47 ] for efficient vector database operations and
utilize the HuggingFace datasets library to enhance corpus loading speed. Additionally, FlashRAG


3


Environment Configure File Parameter Dictionary Evaluation Module

















Pipelines













































Figure 1: An overview of FlashRAG.


includes a “retrieval cache” feature, allowing for the reuse of retrieval results and supporting custom
formats from non-open source retrievers.


**Reranker** aims at refining the order of retrieved results to improve retrieval accuracy. FlashRAG
supports many cross-encoder models, such as bge-reranker [ 44 ] and jina-reranker, and facilitates the
use of bi-encoder models such as E5 for scenarios using embedding models for reranking. Rerankers
can be seamlessly integrated with any retriever through a decorator, enabling simple and flexible
combinations.


**Refiner** processes input text to optimize it for generation by reducing token usage and noise. We
have implemented three types of refiners: extractive, abstractive, perplexity-based [ 48, 49, 50 ].
Each type employs different methods to handle retrieved passages, such as semantic extraction or
summarization [ 51 ], utilizing both dedicated models like RECOMP [ 52 ] and general models available
on HuggingFace.


**Generator** is the final component in the RAG process. We integrate two advanced LLM acceleration
libraries, vLLM [ 53 ] and FastChat [ 54 ], supporting a wide range of LLMs. Furthermore, we provide
the native interface of the Transformers library [ 55 ] to enhance robustness. The module also includes
encoder-decoder models like Flan-T5 [ 56 ] and utilizes fusion-in-decoder techniques [ 57 ] to enhance
processing efficiency with retrieved content.


**3.2** **Pipeline Module**


Building on the diverse components, FlashRAG allows the decoupling of the algorithmic flow of the
RAG process from the specific implementations of each component. In constructing the pipeline,
users only need to determine the required components for the RAG process and the logic of data flow
between these components. The pipeline will automatically executes the corresponding RAG process
and provides both the intermediate results and evaluation results.


To systematically execute the operational logic of various RAG tasks, we conduct an in-depth analysis
of RAG-related literature. Following a recent survey on RAG [6], we identify four primary types of
RAG process flows: Sequential, Branching, Conditional, and Loop.


**Sequential Pipeline** implements a linear execution path, typically represented as “query _→_ retriever
_→_ post-retrieval (reranker, refiner) _→_ generator”. Once the user has configured their settings, the
library automatically loads the necessary components along with their corresponding process logic.


**Branching Pipeline** executes multiple paths in parallel for a single query (often one path per retrieved
passage) and merges the results from all paths to form the final output. Currently, FlashRAG supports
two advanced branching methods: REPLUG pipeline [ 97 ] and SuRe pipeline [ 98 ]. The REPLUG
pipeline processes each retrieved passage in parallel and combines the generation probabilities from


4


Table 2: Summary of datasets. The sample size of each dataset and the knowledge source of the
answer are listed as references. “-” indicates that the knowledge source is commonsense. The _∗_
symbol represents that the task of this dataset has been modified to fit RAG settings.


**Task** **Dataset Name** **Knowledge Source** **# Train** **# Dev** **# Test**



QA


Multi-Hop QA



NQ [58] Wiki 79,168 8,757 3,610
TriviaQA [59] Wiki & Web 78,785 8,837 11,313
PopQA [60] Wiki - - 14,267
SQuAD [61] Wiki 87,599 10,570 MSMARCO-QA [62] Web 808,731 101,093 NarrativeQA [24] Books, movie scripts 32,747 3,461 10,557
WikiQA [63] Wiki 20,360 2,733 6,165
WebQuestions [64] Google Freebase 3,778 - 2,032
AmbigQA [65, 58] Wiki 10,036 2,002 SIQA [66] - 33,410 1,954 CommonsenseQA [67] - 9,741 1,221 BoolQ [68] Wiki 9,427 3,270 PIQA [69] - 16,113 1,838 Fermi [70] Wiki 8,000 1,000 1,000


HotpotQA [71] Wiki 90,447 7,405 2WikiMultiHopQA [72] Wiki 15,000 12,576 Musique [73] Wiki 19,938 2,417 Bamboogle [74] Wiki - - 125
StrategyQA [75] Wiki 2,290 - 


ASQA [76] Wiki 4,353 948        Long-Form QA ELI5 [77] Reddit 272,634 1,507 WikiPassageQA [78] Wiki 3,332 417 416



Multiple-Choice



MMLU [79, 80] - 99,842 1,531 14,042
TruthfulQA [81] Wiki - 817 HellaSwag [82] ActivityNet 39,905 10,042 ARC [83] - 3,370 869 3,548
OpenBookQA [84] - 4,957 500 500
QuaRTz [85] - 2,696 384 784



AIDA CoNLL-YAGO [86, 87] Wiki & Freebase 18,395 4,784         Entity linking WNED [88, 87] Wiki  - 8,995  

T-REx [89, 87] DBPedia 2,284,168 5,000          Slot Filling Zero-shot RE [90, 87] Wiki 147,909 3,724   

Fact Verification FEVER [91, 87] Wiki 104,966 10,444 

Dialog Generation WOW [92, 87] Wiki 63,734 3,054 

Open-domain
Summarization _[∗]_ WikiAsp [23] Wiki 300,636 37,046 37,368


In-domain QA DomainRAG [93] Web pages of RUC  -  - 485


Gaokao-MM [94]         - 519         - 127
Vision QA MultimodalQA [95] Wiki 2,099 230  MathVista [96]          -          -          -          

5


Table 3: The benchmarking results. _Optimize component_ represents the primary component optimized
by the method, while _flow_ indicates optimization of the entire RAG process. Methods marked with _∗_
denote the use of a trained generator.


Optimize Pipeline NQ TriviaQA HotpotQA 2Wiki PopQA WebQA
Method
component type (EM) (EM) (F1) (F1) (F1) (EM)


Naive Generation - Sequential 22.6 55.7 28.4 33.9 21.7 18.8
Standard RAG - Sequential 35.1 58.8 35.3 21.0 36.7 15.7
AAR [101] Retriever Sequential 30.1 56.8 33.4 19.8 36.1 16.1
LongLLMLingua [49] Refiner Sequential 32.2 59.2 37.5 25.0 38.7 17.5
RECOMP-abstractive [52] Refiner Sequential 33.1 56.4 37.5 32.4 39.9 20.2
Selective-Context [50] Refiner Sequential 30.5 55.6 34.4 18.5 33.5 17.3
Trace [102] Refiner Sequential 30.7 50.2 34.0 15.5 37.4 19.9
Spring [103] Generator Sequential 37.9 64.6 42.6 37.3 54.8 27.7
Ret-Robust _[∗]_ [104] Generator Sequential 42.9 68.2 35.8 43.4 57.2 33.7
SuRe [98] Flow Branching 37.1 53.2 33.4 20.6 48.1 24.2
REPLUG [97] Generator Branching 28.9 57.7 31.2 21.1 27.8 20.2
SKR [37] Judger Conditional 33.2 56.0 32.4 23.4 31.7 17.0
Adaptive-RAG [38] Judger Conditional 35.1 56.6 39.1 28.4 40.4 16.0
Self-RAG _[∗]_ [11] Flow Loop 36.4 38.2 29.6 25.1 32.7 21.9
FLARE [12] Flow Loop 22.5 55.8 28.0 33.9 20.7 20.2
Iter-RetGen [99], ITRG [100] Flow Loop 36.8 60.1 38.3 21.6 37.9 18.2
IRCoT [105] Flow Loop 33.3 56.9 41.5 32.4 45.6 20.7
RQRAG [106] Flow Loop 32.6 52.5 33.5 35.8 46.4 26.2


all passages to produce the final answer. The SuRe pipeline generates a candidate answer from each
retrieved passage and then ranks all candidate answers.


**Conditional Pipeline** utilizes a judger to direct the query into different execution paths based on
predefined criteria. Queries requiring retrieval follow the standard sequential process, while others
bypass retrieval and proceed directly to generation. FlashRAG provides utility functions to split and
merge the input dataset based on the judger’s decisions, ensuring batch processing and enhanced
pipeline efficiency. Additionally, the conditional pipeline supports integration with various types of
pipelines, enabling dynamic execution based on judger’s results.


**Loop Pipeline** involves complex interactions between retrieval and generation processes, often
containing multiple cycles of retrieval and generation. Compared to the previous ones, this pipeline is
more flexible, and thus can often yield better performance. FlashRAG currently supports four typical
methods, including Iterative [99, 100], Self-Ask [74], Self-RAG [11], and FLARE [12].


**3.3** **Datasets**


**3.3.1** **Datasets for RAG**


We collect and pre-process 32 benchmark datasets, covering a broad spectrum utilized in existing
studies on RAG. The statistics of these datasets are shown in Table 2. Each dataset has been
standardized into a unified JSONL format, comprising four fields per item: ID, question, golden
answer, and metadata. For multiple-choice datasets such as MMLU [ 79, 80 ], an additional “choices”
field is provided as options. These processed datasets are readily accessible on HuggingFace. Further
details on dataset processing are available in Appendix B.


In addition to the datasets, we provide a variety of dataset filtering tools that enable users to tailor
the dataset according to their needs. For instance, user can select a certain number of samples,
either randomly or sequentially, for evaluation, or filter subsets based on the dataset’s metadata.
These methods are unified within a dataset loading function, providing a consistent user interface.
Customized filtering functions can also be implemented by users.


**3.3.2** **Retrieval Corpus**


The corpus used for retrieval, also known as the knowledge base, is also crucial for preparing
experiments. Typically used are the Wikipedia passages and MS MARCO passages.


6


**Wikipedia passages** : The Wikipedia passages comprise a collection of passages from English
Wikipedia pages, widely used as knowledge sources in datasets like KILT [ 87 ]. The process of
acquiring the Wikipedia corpus includes downloading snapshots in XML format, cleaning the text of
HTML tags, extracting textual content, and segmenting it into passages suitable for retrieval.


In research, various versions of Wikipedia are often used, increasing the difficulty of reproduction. We
provide easy-to-use scripts for automatically downloading and pre-processing any required version of
Wikipedia. Additionally, we provide various chunking functions to support customized segmentation,
enabling alignment with standard or previously used corpora. Our toolkit includes the widely used
Wikipedia dump from DPR [42] dated December 20, 2018, as a fundamental resource.


**MS MARCO passages** [ 62 ]: This dataset consists of 8.8 million passages sourced from Bing
search engine. Compared to the Wikipedia dump, it contains fewer passages and has undergone
pre-processing. This dataset has already been released on HuggingFace, so we provide direct links in
our library for easy access.


**3.4** **Evaluation Metrics**


FlashRAG supports several commonly used evaluation metrics to measure the quality of the RAG
process. Depending on the subject of evaluation, these metrics can be categorized into retrieval-aspect
metrics and generation-aspect metrics.


**Retrieval-aspect metrics** : FlashRAG supports four metrics including recall@ _k_, precision@ _k_, F1@ _k_,
and mean average precision (MAP) to evaluate retrieval quality. Different from evaluation in
standalone retrieval systems, the passages retrieved in the RAG process often lack golden labels
( _e.g._, related or unrelated tags). Therefore, we consider the presence of the golden answer within
retrieved passages as an indicator of relevance. Other metrics can be implemented by inheriting
existing metrics and modifying the calculation methods.


**Generation-aspect metrics** : To evaluate the quality of generation, FlashRAG supports five metrics:
token-level F1 score, exact match, accuracy, BLEU [ 107 ], and ROUGE-L [ 108 ]. Moreover, FlashRAG
also evaluates the number of tokens used in generation to analyze cost-effectiveness.


To accommodate custom evaluation metrics, FlashRAG provides a metric template for users. As
our library automatically saves intermediate results, users can conveniently evaluate results from
intermediate components, such as analyzing token usage pre- and post-refinement or comparing
precision across multiple retrieval rounds.


**4** **Textual Experimental Result and Discussion**


To validate the effectiveness of FlashRAG, we conduct a series of experiments for providing
reproducible benchmarking results and facilitating further exploration. In our main experiment, by
default, we employ the latest `LLaMA-3-8B-instruct` [ 109 ] model as the generator and `E5-base-v2`
as the retriever. Some methods may fine-tune their own models in different RAG components, and
we provide the details of them in Appendix A.


**Methods.** We evaluate the performance of all supported RAG methods. These methods are
categorized based on the RAG component they primarily aim to optimize: AAR [ 101 ] aims
at optimizing the retriever; LongLLMLingua [ 49 ], RECOMP [ 52 ], Selective-Context [ 50 ] and
Trace [ 102 ] focus on refining and compressing the input; Ret-Robust [ 104 ], Spring [ 103 ] and
REPLUG [ 97 ] aim to enhance the generator and its decoding approaches; SKR [ 37 ], AdaptiveRAG [ 38 ] introduce the judger to decide the necessity of retrieval for a query; SuRe [ 98 ], SelfRAG [ 11 ], FLARE [ 12 ], Iter-RetGen [ 99 ], RQRAG [ 106 ] and ITRG [ 100 ] optimize the entire RAG
flow, including using multi-round retrieval and generation processes.


**4.1** **Benchmarking Results**


The experimental results are shown in Table 3. Overall, RAG methods significantly outperform
the direct generation baseline, which clearly demonstrates the benefits of incorporating external
knowledge into the generation process. We further have the following observations: (1) Standard
RAG, with advanced retrievers and generators, is a strong baseline, showing robust performance


7


|Sta<br>LongLLML<br>REC|ndard<br>ingua<br>OMP|Col3|Col4|Col5|BM25<br>Bge-base|
|---|---|---|---|---|---|
|REC<br>LongLLML<br>Sta|OMP<br>ingua<br>ndard|||||
|REC<br>LongLLML<br>Sta|OMP<br>ingua<br>ndard|||||
|REC<br>LongLLML<br>Sta|OMP<br>ingua<br>ndard|0<br>20<br>30<br>|0<br>20<br>30<br>|0<br>20<br>30<br>|0<br>5<br>E5~~-~~base|

|Stan<br>LongLLMLi|dard<br>ngua|Col3|Col4|Col5|a3-8B|
|---|---|---|---|---|---|
|LongLLMLi<br>Stan|ngua<br>dard|||||
|LongLLMLi<br>Stan|ngua<br>dard|||Llam|Llam|
|LongLLMLi<br>Stan|ngua<br>dard|0<br>20<br>30<br>40<br><br>Qwen|0<br>20<br>30<br>40<br><br>Qwen|0<br>20<br>30<br>40<br><br>Qwen|1.5~~-~~14B|


Figure 2: The average results on three datasets(NQ, TriviaQA, HotpotQA) of baseline methods under
different settings. **Left** : Results under three retrievers. **Right** : Results under two generator models
with different parameter scale.


across six datasets. (2) AAR improves retrievers by fine-tuning the `contriever` model, achieving
results comparable to the `E5` baseline on various datasets. (3) All three methods employing
refiners exhibit significant improvements, particularly on multi-hop datasets such as HotpotQA
and 2WikiMultihopQA. This is potentially because complex problems result in less accurate passage
retrieval, introducing more noise and highlighting the necessity for refiner optimization. (4) As for
generator optimization method, Ret-Robust fine-tunes the `LLaMA-2-13B` model via LoRA [ 110 ],
significantly enhancing generator’s capability of understanding retrieved passages and outperforming
other training-free approaches. (5) The effectiveness of optimizing the RAG process varies depending
on the dataset complexity. On simpler datasets such as NQ and TriviaQA, FLARE and IterRetGen perform comparably to, or slightly below, Standard RAG. In contrast, for more complex
datasets requiring multi-step reasoning, such as HotpotQA, these methods demonstrate substantial
improvements over the baseline. This indicates that adaptive retrieval methods are particularly
advantageous for tackling complex problems, but they may introduce higher operational costs with
limited benefits for simpler tasks.


**4.2** **Impact of Retrievers and Generators**


In RAG, the choice of retrievers and generators plays a crucial role in determining the final
performance. Therefore, we conduct an experiment to explore their impact. Note that we do
not include methods requiring specific retrievers or generators ( _e.g._, Self-RAG requires trained
models).


As shown in the left part of Figure 2, most methods are sensitive to retrieval quality. The performance
gap between using the BM25 and E5 retriever can approach nearly 10%. This gap is likely due to
the presence of more noise in the retrieved passages of BM25, thereby disturbing the generation
process with irrelevant information. In contrast, compression methods such as RECOMP show
better robustness across various retrievers, suggesting that compression effectively mitigates the
noise. Moreover, this robustness can be further enhanced by fine-tuning the generator. For example,
Ret-Robust introduces a generator-specific training strategy that effectively minimizes the impact of
irrelevant passages.


The influence of generators is also explored by using two models in different sizes (Qwen-1.5-14B
and LLaMA-3-8B). Intriguingly, the larger model cannot consistently outperform the smaller one.
For example, in methods such as FLARE and RECOMP, the smaller model yields better performance.
Given that LLaMA-3-8B outperforms Qwen-1.5-14B in many public benchmarks, it suggests that the
LLMs’ RAG performance may be highly relevant to their general generation capabilities rather than
their size. This observation highlights the complexity of LLMs’ performance evaluation, suggesting
that factors other than size, such as model architecture or training data quality, can also play significant
roles.


8


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|Col22|Col23|Col24|Top 3<br>Top 5<br>Top 10<br>Top 15|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||||||||||||||
||||||||||||||||||||||||||
||||||||||||||||||||||||||
||||||||||||||||||||||||||

|40.0<br>E5-base<br>37.5 BM25<br>Bge-base<br>35.0<br>32.5 Score<br>30.0 Average<br>27.5<br>25.0<br>22.5<br>20.0<br>1 3 5 10 15|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|1<br>3<br>5<br>10<br>15<br>20.0<br>22.5<br>25.0<br>27.5<br>30.0<br>32.5<br>35.0<br>37.5<br>40.0<br>Average Score<br>E5~~-~~base<br>~~BM25~~<br>Bge~~-~~base|||||Bge~~-~~bas|e|
|1<br>3<br>5<br>10<br>15<br>20.0<br>22.5<br>25.0<br>27.5<br>30.0<br>32.5<br>35.0<br>37.5<br>40.0<br>Average Score<br>E5~~-~~base<br>~~BM25~~<br>Bge~~-~~base|||||||
|1<br>3<br>5<br>10<br>15<br>20.0<br>22.5<br>25.0<br>27.5<br>30.0<br>32.5<br>35.0<br>37.5<br>40.0<br>Average Score<br>E5~~-~~base<br>~~BM25~~<br>Bge~~-~~base|||||||
|1<br>3<br>5<br>10<br>15<br>20.0<br>22.5<br>25.0<br>27.5<br>30.0<br>32.5<br>35.0<br>37.5<br>40.0<br>Average Score<br>E5~~-~~base<br>~~BM25~~<br>Bge~~-~~base||3<br>|3<br>||0<br>|5|


Figure 3: The results of standard RAG process under different number of retrieved passages and
retrievers. **Left:** Average results on six datasets using three different retrievers with varying numbers
of retrieved passages. **Right:** Individual results on six datasets using E5 as the retriever.


**4.3** **Impact of Retrieval on RAG**


In RAG, both the quantity and quality of the retrieved passages significantly impact the final answer.
However, existing studies often employ a predetermined retriever and a fixed number of retrieved
passages, limiting further exploration. To comprehensively investigate the influence of the retrieval
process on RAG results, we conduct a series of experiments. Figure 3 presents the results of using
different numbers of retrieved passages.


As shown in the left part, the best performance is achieved with approximately five retrieved passages.
Both an excessive and insufficient number of passages lead to a significant drop in performance. This
trend is consistent across various retrievers, including both dense and sparse methods. Moreover, the
performance differences among various retrievers tend to diminish when a larger number of passages
are used, suggesting an increase in noise within the retrieved content. In contrast, when only using
the top-1 result, there is a substantial gap between dense methods ( _i.e._, E5 and BGE) and BM25,
indicating that with fewer passages, the quality of the retriever becomes a more critical factor in
influencing the final performance.


In the right part, we analyze the impact of the number of retrieved passages on different datasets. For
most datasets, the best performance is achieved with the top-5 retrieved results, suggesting that this
may provide an ideal balance between the richness of information and potential noise. This finding
highlights the importance of adjusting the retrieval count to optimize the effectiveness of the RAG
process across various datasets.


**4.4** **Impact of Corpus Chunking**


One key issue in RAG research is determining the optimal chunking method. Following previous
studies in pre-processing Wikipedia [ 111, 112 ], we employ various chunking methods to segment
the latest version of the Wikipedia dump into different variants and explore their corresponding
performance. [2] We experiment with four different chunk sizes: six sentences, eight sentences, ten
sentences, and 100 words. Each configuration uses a stride set to half of the chunk size. Additionally,
we explore a six-sentence chunk with a full chunk stride (non-overlapping) to evaluate the impact of
stride variations on performance. To control the number of input words, we regulate the number of
retrieved passages. The statistics are shown in the left part of Figure 4.


The right part of Figure 4 illustrates the results. We can observe that optimal performance is
typically achieved when each query corresponds to approximately 350-450 words of retrieved results,
regardless of the chunk size. This suggests that with larger chunk sizes, a reduced number of passages
may be beneficial. However, comparisons within the six-sentence chunks reveal that overlapping
chunks tend to yield inferior performance. This highlights the nuanced effects that both chunk size
and stride have on the efficacy of RAG.


2 Wikipedia Dump: `[https://dumps.wikimedia.org/enwiki/](https://dumps.wikimedia.org/enwiki/)`


9


Chunk Size Stride # Passages Avg. Len


6 sents 3 sents 38.4 M 127.7

6 sents 6 sents 23.4 M 115.6

8 sents 4 sents 28.9 M 163.5

10 sents 5 sents 23.3 M 196.9

100 words - 30.1 M 100.5







|36|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|200<br>400<br>600<br>800<br>Average Document Words per Query<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br><br><br>100w<br>~~cs6-s3~~<br>cs6~~-~~s6<br>cs8~~-~~s4<br>~~cs10-s5~~||||||100w<br>|
|200<br>400<br>600<br>800<br>Average Document Words per Query<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br><br><br>100w<br>~~cs6-s3~~<br>cs6~~-~~s6<br>cs8~~-~~s4<br>~~cs10-s5~~||||||~~cs6-s3~~<br>cs6~~-~~s6<br>|
|200<br>400<br>600<br>800<br>Average Document Words per Query<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br><br><br>100w<br>~~cs6-s3~~<br>cs6~~-~~s6<br>cs8~~-~~s4<br>~~cs10-s5~~||||||cs8~~-~~s4<br>~~cs10-s5~~|
|200<br>400<br>600<br>800<br>Average Document Words per Query<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br><br><br>100w<br>~~cs6-s3~~<br>cs6~~-~~s6<br>cs8~~-~~s4<br>~~cs10-s5~~|||||||
|200<br>400<br>600<br>800<br>Average Document Words per Query<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br><br><br>100w<br>~~cs6-s3~~<br>cs6~~-~~s6<br>cs8~~-~~s4<br>~~cs10-s5~~|||||||
|200<br>400<br>600<br>800<br>Average Document Words per Query<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br><br><br>100w<br>~~cs6-s3~~<br>cs6~~-~~s6<br>cs8~~-~~s4<br>~~cs10-s5~~|||||||
|200<br>400<br>600<br>800<br>Average Document Words per Query<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br><br><br>100w<br>~~cs6-s3~~<br>cs6~~-~~s6<br>cs8~~-~~s4<br>~~cs10-s5~~|||||||
|200<br>400<br>600<br>800<br>Average Document Words per Query<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br><br><br>100w<br>~~cs6-s3~~<br>cs6~~-~~s6<br>cs8~~-~~s4<br>~~cs10-s5~~|||||||


Figure 4: (Left) Statistical overview of segmentation variants, where chunk size and stride are
quantified in terms of _sentences_, and average length denotes the average number of _words_ per passage.
(Right) Experimental results of variants under different number of retrieval passages.


Table 4: The overall MRAG performance of different MLLM backbones.


**Gaokao-MM** **MultimodalQA** **MathVista**
**MLLM** **Retriever**


Acc EM F1 Acc EM F1 Acc


Llava-7B      - 0.142 0.317 0.401 0.739 0.262 0.162 0.389

Llava-ov-7B     - 0.374 0.743 0.752 0.761 0.581 0.457 0.649



Qwen2-vl-7B


Internvl2.5-8B



    - 0.299 0.304 0.319 0.352 0.570 0.449 0.644

openai-clip 0.339 0.348 0.351 0.352 0.444 0.344 0.535
jina-clip-v2 0.343 0.404 0.410 0.426 0.471 0.356 0.552
chinese-clip 0.295 0.404 0.410 0.422 0.427 0.337 0.538


    - 0.220 0.343 0.349 0.357 0.569 0.456 0.692

openai-clip 0.287 0.439 0.447 0.452 0.450 0.347 0.580
jina-clip-v2 0.287 0.426 0.438 0.448 0.485 0.372 0.610
chinese-clip 0.248 0.470 0.481 0.500 0.461 0.354 0.585



**5** **Multimodal Experimental Results and Analysis**


**5.1** **Overall Results**


To further explore the performance of multimodal retrieval augmented technologies in conjunction
with different MLLM backbones, Table 4 illustrartes the overall results of various MRAG
configurations across three widely used VQA datasets. For each question, we retrieve the Top1 relevant image-text pair from the corresponding retrieval corpus. In detail, we have identified the
following key insights:


**MRAG delivers stable performance gains across MLLMs in knowledge-intensive tasks.** In
knowledge-intensive multimodal QA datasets, multimodal retrieval knowledge significantly enhances
the performance of MLLM bases, particularly the combination of Qwen2-VL and OpenAI-Clip,
which provides an improvement of over 10 points in EM scores. Focusing on the Gaokao
MM multidimensional reasoning assessment, multimodal retrieval knowledge also delivers stable
enhancements, regardless of whether it involves Qwen2-VL-7B or InternVL-2.5-8B. This confirms
that multimodal retrieval augmentation effectively supplements MLLMs in the domain of general
knowledge reasoning.


**MRAG still struggles with multimodal mathematical reasoning.** However, when focusing on
complex multimodal mathematical reasoning tasks, multimodal retrieval knowledge does not yield
performance gains and instead results in noticeable negative effects. For instance, the combination
of Clip and Qwen2 vL 7B even leads to a 10-point drop in accuracy. This observation aligns with
findings from AR-MCTS [ 113 ], indicating that in the specialized domain of mathematical reasoning,
the in-domain multimodal retrieval augmentation method still exhibits significant limitations.


10


Table 5: The overall result of different retrieved document quantity in MRAG under various retrieval
types.


**GaoKao-MM** **Multimodal QA**
**MLLM** **Retrieval Type** **Retriever** **Top-K**

Acc EM F1



1 0.295 0.352 0.361

2 0.335 0.335 0.343

3 0.307 0.348 0.362

5 0.358 0.374 0.389


1 0.35 0.335 0.344

2 0.307 0.343 0.36

3 0.307 0.357 0.376

5 0.358 0.365 0.391



Qwen2-vl-7B


Internvl2.5-8B



Multimodal Clip


Text-only BM25



Multimodal Clip


Text-only BM25



1 0.35 0.361 0.371
Hybridmodal BM25 + Clip 2 0.307 0.374 0.391
3 0.307 0.378 0.396



1 0.248 0.404 0.412

2 0.26 0.43 0.462

3 0.291 0.443 0.463

5 0.264 0.443 0.462


1 0.287 0.4 0.408

2 0.272 0.37 0.381

3 0.26 0.383 0.396

5 0.248 0.4 0.417



1 0.287 0.417 0.432
Hybridmodal BM25 + Clip 2 0.272 0.413 0.436
3 0.26 0.452 0.472


**5.2** **Impact of different Multimodal Retrievers**


To investigate the impact of various retrieval types, Table 4 presents a comparison of three commonly
used retrievers in MRAG: OpenAI-Clip [3], Jina-Clip-v2, and Chinese-Clip.


**The language consistency of the retriever is not a decisive factor for MRAG.** In the GaokaoMM evaluation, each retriever consistently offers improvements. Notably, Chinese-Clip does not
achieve the highest gains, highlighting that the selection of multimodal retrievers should not solely
depend on linguistic consistency. Similar findings emerge in the Multimodal QA, where all retrievers
demonstrate stable enhancements, with the linguistically inconsistent Chinese-Clip showing the most
substantial performance boost.


However, in MathVista, the knowledge retrieved from different retrievers notably influences reasoning
performance. We hypothesize that the retrieved knowledge may to some extent disrupt the coherence
of problem-solving reasoning [114, 115].


**5.3** **Influence of Retrieval Types & Top-K Retrieved knowledge**


To comprehensively assess the impact of the number of multimodal retrieval documents on
performance, we conducted evaluations using the top 1, 2, 3, and 5 documents in three different
settings: text-only retrieval (BM25), multimodal retrieval (Clip), and hybridmodal retrieval
(BM25+Clip).


Our findings reveal that, in both the Gaokao-MM and MultimodalQA evaluation, utilizing Clip for
multimodal retrieval generally results in more consistent improvements as the number of documents
increases (from Top-1 to Top-3). Nonetheless, once the document count surpasses a certain threshold
(3), MRAG’s performance may become unstable. In the case of text-only and hybridmodal retrieval,
MRAG continues to exhibit steady improvements with an increasing number of documents in
MultimodalQA. Conversely, for Gaokao-MM, adding more documents does not consistently enhance


3 Model Card: `[https://huggingface.co/openai/clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)`


11


|Col1|2B|
|---|---|
|7B|7B|
|||









|Qwen2-VL|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|2B|2B|2B|2B|2B|2B|2B|2B|2B|2B|
|2B|2B|2B|2B|2B|2B|2B|2B|||
|7B|7B|7B|7B|7B|7B|7B|7B|||
|7B|7B|7B|7B|7B|7B|7B||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||

|InternVL-2.5|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||


Figure 5: Analysis of using MLLM backbone with difference param scales in MRAG.


performance, suggesting that complex reasoning datasets are particularly sensitive to the availability
of knowledge.


**5.4** **Effect of MLLM Parameter Size**


In Figure 5, we delve deeper into the influence of varying parameter sizes of MLLM backbones on
MRAG’s performance. Remarkably, both the Qwen2-VL and InternVL2.5 series exhibit consistent
performance enhancements across all three datasets as the parameter size increases, especially in the
math-specific reasoning benchmark MathVista. This reinforces the notion that MLLMs with larger
parameters generally have superior abilities in harnessing retrieval knowledge.


**6** **Conclusion**


In this work, we introduced FlashRAG, a new modular toolkit designed to address the challenges of
replicability and the high development costs in RAG research. FlashRAG consists of a comprehensive
collection of benchmark datasets, state-of-the-art RAG methods, utilities for corpus pre-processing,
and a set of widely used evaluation metrics. This toolkit not only aids in the replication of existing
RAG techniques but also supports the development of new approaches. Our experiments across
multiple datasets have demonstrated the effectiveness of FlashRAG and have highlighted several
critical factors for the successful development of RAG applications. By lowering the technical
barriers and enhancing reproducibility, FlashRAG aims to accelerate the pace of innovation in the
RAG domain, ultimately contributing to more robust and effective systems.


**References**


[1] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie
Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,
Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron
Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey
Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and
Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika
Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors,
_International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,_
_Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 2206–2240.
PMLR, 2022.


[2] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:
retrieval-augmented language model pre-training. _CoRR_, abs/2002.08909, 2020.


12


[3] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung,
Q. V. Do, Y. Xu, and P. Fung. A multitask, multilingual, multimodal evaluation of ChatGPT
on reasoning, hallucination, and interactivity. In _Proc. of IJCNLP_, 2023.

[4] Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao
Ma, Guanting Dong, Meina Song, Wei Lin, Yifan Zhu, and Anh Tuan Luu. Chatkbqa: A
generate-then-retrieve framework for knowledge base question answering with fine-tuned large
language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, _Findings of the_
_Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting,_
_August 11-16, 2024_, pages 2039–2056. Association for Computational Linguistics, 2024.

[5] Guanting Dong, Rumei Li, Sirui Wang, Yupeng Zhang, Yunsen Xian, and Weiran Xu. Bridging
the kb-text gap: Leveraging structured knowledge-aware pre-training for KBQA. In Ingo
Frommholz, Frank Hopfgartner, Mark Lee, Michael Oakes, Mounia Lalmas, Min Zhang,
and Rodrygo L. T. Santos, editors, _Proceedings of the 32nd ACM International Conference_
_on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom,_
_October 21-25, 2023_, pages 3854–3859. ACM, 2023.

[6] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, M. Wang, and H. Wang.
Retrieval-augmented generation for large language models: A survey. _ArXiv preprint_, 2023.

[7] Yiruo Cheng, Kelong Mao, Ziliang Zhao, Guanting Dong, Hongjin Qian, Yongkang
Wu, Tetsuya Sakai, Ji-Rong Wen, and Zhicheng Dou. Coral: Benchmarking multi-turn
conversational retrieval-augmentation generation, 2024.

[8] Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and Ji-Rong Wen.
Understand what LLM needs: Dual preference alignment for retrieval-augmented generation.
_CoRR_, abs/2406.18676, 2024.

[9] Harrison Chase. LangChain, October 2022.

[10] Jerry Liu. LlamaIndex, November 2022.

[11] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-rag: Learning to retrieve, generate,
and critique through self-reflection. _ArXiv preprint_, 2023.

[12] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming
Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In Houda
Bouamor, Juan Pino, and Kalika Bali, editors, _Proceedings of the 2023 Conference on_
_Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December_
_6-10, 2023_, pages 7969–7992. Association for Computational Linguistics, 2023.

[13] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan,
K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin. Qwen2-vl: Enhancing
vision-language model’s perception of the world at any resolution. _ArXiv preprint_, 2024.

[14] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong,
Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai.
Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.
_CoRR_, abs/2312.14238, 2023.

[15] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.

[16] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.
Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.

[17] A. Radford, J. Wook Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from
natural language supervision. In _Proc. of ICML_, Proceedings of Machine Learning Research,
2021.

[18] Malte Pietsch, Timo Möller, Bogdan Kostic, Julian Risch, Massimiliano Pippi, Mayank
Jobanputra, Sara Zanzottera, Silvano Cerza, Vladimir Blagojevic, Thomas Stadelmann, Tanay
Soni, and Sebastian Lee. Haystack: the end-to-end NLP framework for pragmatic builders,
November 2019.

[19] Peter Izsak, Moshe Berchansky, Daniel Fleischer, and Ronen Laperdon. fastRAG: Efficient
Retrieval Augmentation and Generation Framework, February 2023.

[20] Xiao Yu, Yunan Lu, and Zhou Yu. Localrqa: From generating data to locally training, testing,
and deploying retrieval-augmented QA systems. _CoRR_, abs/2403.00982, 2024.


13


[21] D. Kim, B. Kim, D. Han, and M. Eibich. Autorag: Automated framework for optimization of
retrieval augmented generation pipeline. _ArXiv preprint_, 2024.

[22] X. Zhang, Y.-Z. Song, Y. Wang, Shuyun Tang, Xinfeng Li, Zhengran Zeng, Zhen Wu, Wei
Ye, Wenyuan Xu, Yue Zhang, Xinyu Dai, Shikun Zhang, and Qingsong Wen. RAGLAB:
A modular and research-oriented unified framework for retrieval-augmented generation. In
Delia Irazu Hernandez Farias, Tom Hope, and Manling Li, editors, _Proc. of the EMNLP: Demo_,
November 2024.

[23] Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan, and Graham
Neubig. Wikiasp: A dataset for multi-domain aspect-based summarization. _Trans. Assoc._
_Comput. Linguistics_, 9:211–225, 2021.

[24] Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor
Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. _Trans._
_Assoc. Comput. Linguistics_, 6:317–328, 2018.

[25] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and
Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. _arXiv preprint_
_arXiv:2501.05366_, 2025.

[26] Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. Metacognitive retrievalaugmented large language models. In Tat-Seng Chua, Chong-Wah Ngo, Ravi Kumar, Hady W.
Lauw, and Roy Ka-Wei Lee, editors, _Proceedings of the ACM on Web Conference 2024, WWW_
_2024, Singapore, May 13-17, 2024_, pages 1453–1463. ACM, 2024.

[27] Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren
Zhou. Self-play with execution feedback: Improving instruction-following capabilities of
large language models. _CoRR_, abs/2406.13542, 2024.

[28] Xiaoxi Li, Yujia Zhou, and Zhicheng Dou. Unigen: A unified generative framework for
retrieval and question answering with large language models. In Michael J. Wooldridge,
Jennifer G. Dy, and Sriraam Natarajan, editors, _Thirty-Eighth AAAI Conference on Artificial_
_Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial_
_Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial_
_Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada_, pages 8688–8696. AAAI
Press, 2024.

[29] Guanting Dong, Xiaoshuai Song, Yutao Zhu, Runqi Qiao, Zhicheng Dou, and Ji-Rong Wen.
Toward general instruction-following alignment for retrieval-augmented generation. _arXiv_
_preprint arXiv:2410.09584_, 2024.

[30] Yiruo Cheng, Kelong Mao, and Zhicheng Dou. Interpreting conversational dense retrieval
by rewriting-enhanced inversion of session embedding. In Lun-Wei Ku, Andre Martins, and
Vivek Srikumar, editors, _Proceedings of the 62nd Annual Meeting of the Association for_
_Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August_
_11-16, 2024_, pages 2879–2893. Association for Computational Linguistics, 2024.

[31] Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng
Dou, Tsung-Yi Ho, and Philip S. Yu. Trustworthiness in retrieval-augmented generation
systems: A survey. _CoRR_, abs/2409.10102, 2024.

[32] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Qi Ye, and Zhicheng Dou.
Retrollm: Empowering large language models to retrieve fine-grained evidence within
generation. _CoRR_, abs/2412.11919, 2024.

[33] Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, and Ji-Rong Wen. Htmlrag:
HTML is better than plain text for modeling retrieved knowledge in RAG systems. _CoRR_,
abs/2411.02959, 2024.

[34] Fengran Mo, Kelong Mao, Ziliang Zhao, Hongjin Qian, Haonan Chen, Yiruo Cheng, Xiaoxi
Li, Yutao Zhu, Zhicheng Dou, and Jian-Yun Nie. A survey of conversational search, 2024.

[35] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng
Dou. From matching to generation: A survey on generative information retrieval. _CoRR_,
abs/2404.14851, 2024.

[36] Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, and Ji-Rong Wen. Small
models, big insights: Leveraging slim proxy models to decide when and what to retrieve


14


for llms. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, _Proceedings of the_
_62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long_
_Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024_, pages 4420–4436. Association
for Computational Linguistics, 2024.

[37] Y. Wang, P. Li, M. Sun, and Y. Liu. Self-knowledge guided retrieval augmentation for large
language models. In _Proc. of EMNLP 2023 Findings_, 2023.

[38] S. Jeong, J. Baek, S. Cho, S. Hwang, and J. Park. Adaptive-RAG: Learning to adapt retrievalaugmented large language models through question complexity. In _Proc. of NAACL-HLT_,
2024.

[39] Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and Ji-Rong Wen.
Understand what LLM needs: Dual preference alignment for retrieval-augmented generation.
_CoRR_, abs/2406.18676, 2024.

[40] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and
Rodrigo Frassetto Nogueira. Pyserini: A python toolkit for reproducible information retrieval
research with sparse and dense representations. In Fernando Diaz, Chirag Shah, Torsten Suel,
Pablo Castells, Rosie Jones, and Tetsuya Sakai, editors, _SIGIR ’21: The 44th International_
_ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event,_
_Canada, July 11-15, 2021_, pages 2356–2362. ACM, 2021.

[41] S. E. Robertson and H. Zaragoza. The probabilistic relevance framework: BM25 and beyond.
_Found. Trends Inf. Retr._, (4), 2009.

[42] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov,
Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.
In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, _Proceedings of the 2020_
_Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,_
_November 16-20, 2020_, pages 6769–6781. Association for Computational Linguistics, 2020.

[43] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan
Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training.
_CoRR_, abs/2212.03533, 2022.

[44] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. C-pack: Packaged resources
to advance general chinese embedding. _CoRR_, abs/2309.07597, 2023.

[45] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid
Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for
dense text retrieval. In _9th International Conference on Learning Representations, ICLR 2021,_
_Virtual Event, Austria, May 3-7, 2021_ . OpenReview.net, 2021.

[46] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, PierreEmmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. The faiss library. _CoRR_,
abs/2401.08281, 2024.

[47] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus.
_IEEE Trans. Big Data_, 7(3):535–547, 2021.

[48] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua:
Compressing prompts for accelerated inference of large language models. In Houda Bouamor,
Juan Pino, and Kalika Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods_
_in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023_, pages
13358–13376. Association for Computational Linguistics, 2023.

[49] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and
Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via
prompt compression. _CoRR_, abs/2310.06839, 2023.

[50] Yucheng Li. Unlocking context constraints of llms: Enhancing context efficiency of llms with
self-information-based content filtering. _CoRR_, abs/2304.12102, 2023.

[51] Jiajie Jin, Yutao Zhu, Yujia Zhou, and Zhicheng Dou. BIDER: bridging knowledge
inconsistency for efficient retrieval-augmented llms via key supporting evidence. In Lun-Wei
Ku, Andre Martins, and Vivek Srikumar, editors, _Findings of the Association for Computational_
_Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024_, pages
750–761. Association for Computational Linguistics, 2024.


15


[52] F. Xu, W. Shi, and E. Choi. RECOMP: improving retrieval-augmented lms with compression
and selective augmentation. _ArXiv preprint_, 2023.


[53] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica.
Efficient memory management for large language model serving with pagedattention. In _Proc._
_of SOSP_, 2023.


[54] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and
Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In Alice Oh, Tristan
Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, _Advances_
_in Neural Information Processing Systems 36: Annual Conference on Neural Information_
_Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023_,
2023.


[55] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,
M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao,
S. Gugger, M. Drame, Q. Lhoest, and A. Rush. Transformers: State-of-the-art natural language
processing. In _Proc. of EMNLP_, 2020.


[56] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan
Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In
_The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event,_
_April 25-29, 2022_ . OpenReview.net, 2022.


[57] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models
for open domain question answering. In Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty,
editors, _Proceedings of the 16th Conference of the European Chapter of the Association for_
_Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021_, pages
874–880. Association for Computational Linguistics, 2021.


[58] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh,
Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova,
Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le,
and Slav Petrov. Natural questions: a benchmark for question answering research. _Trans._
_Assoc. Comput. Linguistics_, 7:452–466, 2019.


[59] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale
distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and MinYen Kan, editors, _Proceedings of the 55th Annual Meeting of the Association for Computational_
_Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers_, pages
1601–1611. Association for Computational Linguistics, 2017.


[60] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel
Khashabi. When not to trust language models: Investigating effectiveness and limitations of
parametric and non-parametric memories. _CoRR_, abs/2212.10511, 2022.


[61] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+
questions for machine comprehension of text. In Jian Su, Xavier Carreras, and Kevin Duh,
editors, _Proceedings of the 2016 Conference on Empirical Methods in Natural Language_
_Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016_, pages 2383–2392. The
Association for Computational Linguistics, 2016.


[62] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,
and Li Deng. MS MARCO: A human generated machine reading comprehension dataset.
In Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne,
editors, _Proceedings of the Workshop on Cognitive Computation: Integrating neural and_
_symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information_
_Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016_, volume 1773 of _CEUR_
_Workshop Proceedings_ . CEUR-WS.org, 2016.


[63] Yi Yang, Wen-tau Yih, and Christopher Meek. Wikiqa: A challenge dataset for open-domain
question answering. In Lluís Màrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and
Yuval Marton, editors, _Proceedings of the 2015 Conference on Empirical Methods in Natural_
_Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015_, pages 2013–
2018. The Association for Computational Linguistics, 2015.


16


[64] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase
from question-answer pairs. In _Proceedings of the 2013 Conference on Empirical Methods_
_in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle,_
_Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL_, pages
1533–1544. ACL, 2013.

[65] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. Ambigqa: Answering
ambiguous open-domain questions. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang
Liu, editors, _Proceedings of the 2020 Conference on Empirical Methods in Natural Language_
_Processing, EMNLP 2020, Online, November 16-20, 2020_, pages 5783–5797. Association for
Computational Linguistics, 2020.

[66] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social iqa:
Commonsense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng,
and Xiaojun Wan, editors, _Proceedings of the 2019 Conference on Empirical Methods in_
_Natural Language Processing and the 9th International Joint Conference on Natural Language_
_Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_, pages 4462–
4472. Association for Computational Linguistics, 2019.

[67] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A
question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy
Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American_
_Chapter of the Association for Computational Linguistics: Human Language Technologies,_
_NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_,
pages 4149–4158. Association for Computational Linguistics, 2019.

[68] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and
Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In
Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference_
_of the North American Chapter of the Association for Computational Linguistics: Human_
_Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1_
_(Long and Short Papers)_, pages 2924–2936. Association for Computational Linguistics, 2019.

[69] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning
about physical commonsense in natural language. In _The Thirty-Fourth AAAI Conference on_
_Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial_
_Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in_
_Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 7432–7439.
AAAI Press, 2020.

[70] Ashwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, and Peter Clark.
How much coffee was consumed during EMNLP 2019? fermi problems: A new reasoning
challenge for AI. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau
Yih, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language_
_Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November,_
_2021_, pages 7318–7328. Association for Computational Linguistics, 2021.

[71] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan
Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable
multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi
Tsujii, editors, _Proceedings of the 2018 Conference on Empirical Methods in Natural Language_
_Processing, Brussels, Belgium, October 31 - November 4, 2018_, pages 2369–2380. Association
for Computational Linguistics, 2018.

[72] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing
A multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott,
Núria Bel, and Chengqing Zong, editors, _Proceedings of the 28th International Conference on_
_Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020_,
pages 6609–6625. International Committee on Computational Linguistics, 2020.

[73] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique:
Multihop questions via single-hop question composition. _Trans. Assoc. Comput. Linguistics_,
10:539–554, 2022.

[74] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike
Lewis. Measuring and narrowing the compositionality gap in language models. In Houda


17


Bouamor, Juan Pino, and Kalika Bali, editors, _Findings of the Association for Computational_
_Linguistics: EMNLP 2023, Singapore, December 6-10, 2023_, pages 5687–5711. Association
for Computational Linguistics, 2023.


[75] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did
aristotle use a laptop? A question answering benchmark with implicit reasoning strategies.
_Trans. Assoc. Comput. Linguistics_, 9:346–361, 2021.


[76] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: factoid questions
meet long-form answers. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,
_Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,_
_EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pages 8273–8288.
Association for Computational Linguistics, 2022.


[77] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.
ELI5: long form question answering. In Anna Korhonen, David R. Traum, and Lluís Màrquez,
editors, _Proceedings of the 57th Conference of the Association for Computational Linguistics,_
_ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, pages 3558–3567.
Association for Computational Linguistics, 2019.


[78] Daniel Cohen, Liu Yang, and W. Bruce Croft. Wikipassageqa: A benchmark collection for
research on non-factoid answer passage retrieval. In Kevyn Collins-Thompson, Qiaozhu Mei,
Brian D. Davison, Yiqun Liu, and Emine Yilmaz, editors, _The 41st International ACM SIGIR_
_Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI,_
_USA, July 08-12, 2018_, pages 1165–1168. ACM, 2018.


[79] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. In _9th International_
_Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_ .
OpenReview.net, 2021.


[80] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and
Jacob Steinhardt. Aligning AI with shared human values. In _9th International Conference on_
_Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_ . OpenReview.net,
2021.


[81] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic
human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,
_Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics_
_(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 3214–3252.
Association for Computational Linguistics, 2022.


[82] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a
machine really finish your sentence? In Anna Korhonen, David R. Traum, and Lluís Màrquez,
editors, _Proceedings of the 57th Conference of the Association for Computational Linguistics,_
_ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, pages 4791–4800.
Association for Computational Linguistics, 2019.


[83] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
and Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning
challenge. _CoRR_, abs/1803.05457, 2018.


[84] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor
conduct electricity? A new dataset for open book question answering. In Ellen Riloff, David
Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, _Proceedings of the 2018 Conference_
_on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 -_
_November 4, 2018_, pages 2381–2391. Association for Computational Linguistics, 2018.


[85] Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. Quartz: An open-domain
dataset of qualitative relationship questions. In Kentaro Inui, Jing Jiang, Vincent Ng, and
Xiaojun Wan, editors, _Proceedings of the 2019 Conference on Empirical Methods in Natural_
_Language Processing and the 9th International Joint Conference on Natural Language_
_Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_, pages 5940–
5945. Association for Computational Linguistics, 2019.


[86] Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal,
Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. Robust disambiguation


18


of named entities in text. In _Proceedings of the 2011 Conference on Empirical Methods in_
_Natural Language Processing, EMNLP 2011, 27-31 July 2011, John McIntyre Conference_
_Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest Group of the ACL_, pages
782–792. ACL, 2011.


[87] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick S. H. Lewis, Majid Yazdani, Nicola De
Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras,
Tim Rocktäschel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language
tasks. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür,
Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors,
_Proceedings of the 2021 Conference of the North American Chapter of the Association for_
_Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June_
_6-11, 2021_, pages 2523–2544. Association for Computational Linguistics, 2021.


[88] Simone Tedeschi, Simone Conia, Francesco Cecconi, and Roberto Navigli. Named entity
recognition for entity linking: What works and what’s next. In Marie-Francine Moens,
Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, _Findings of the Association for_
_Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic,_
_16-20 November, 2021_, pages 2584–2596. Association for Computational Linguistics, 2021.


[89] Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare,
Frédérique Laforest, and Elena Simperl. T-rex: A large scale alignment of natural language
with knowledge base triples. In _Proceedings of the Eleventh International Conference on_
_Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018._, 2018.


[90] Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction
via reading comprehension. In Roger Levy and Lucia Specia, editors, _Proceedings of the 21st_
_Conference on Computational Natural Language Learning (CoNLL 2017), Vancouver, Canada,_
_August 3-4, 2017_, pages 333–342. Association for Computational Linguistics, 2017.


[91] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a
large-scale dataset for fact extraction and verification. In Marilyn A. Walker, Heng Ji, and
Amanda Stent, editors, _Proceedings of the 2018 Conference of the North American Chapter of_
_the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT_
_2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)_, pages 809–819.
Association for Computational Linguistics, 2018.


[92] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston.
Wizard of wikipedia: Knowledge-powered conversational agents. In _7th International_
_Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,_
_2019_ . OpenReview.net, 2019.


[93] Shuting Wang, Jiongnan Liu, Shiren Song, Jiehan Cheng, Yuqi Fu, Peidong Guo, Kun Fang,
Yutao Zhu, and Zhicheng Dou. Domainrag: A chinese benchmark for evaluating domainspecific retrieval-augmented generation. _CoRR_, abs/2406.05654, 2024.


[94] Yi Zong and Xipeng Qiu. GAOKAO-MM: A chinese human-level benchmark for multimodal
models evaluation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, _Findings_
_of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual_
_meeting, August 11-16, 2024_, pages 8817–8825. Association for Computational Linguistics,
2024.


[95] Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco,
Hannaneh Hajishirzi, and Jonathan Berant. Multimodalqa: complex question answering over
text, tables and images. In _9th International Conference on Learning Representations, ICLR_
_2021, Virtual Event, Austria, May 3-7, 2021_ . OpenReview.net, 2021.


[96] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao
Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical
reasoning of foundation models in visual contexts. In _The Twelfth International Conference_
_on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024_ . OpenReview.net,
2024.


[97] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke
Zettlemoyer, and Wen-tau Yih. REPLUG: retrieval-augmented black-box language models.
In Kevin Duh, Helena Gómez-Adorno, and Steven Bethard, editors, _Proceedings of the 2024_


19


_Conference of the North American Chapter of the Association for Computational Linguistics:_
_Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico,_
_June 16-21, 2024_, pages 8371–8384. Association for Computational Linguistics, 2024.


[98] J. Kim, J. Nam, S. Mo, J. Park, S.-W. Lee, M. Seo, J.-W. Ha, and J. Shin. Sure: Summarizing
retrievals using answer candidates for open-domain QA of LLMs. In _Proc. of ICLR_, 2024.


[99] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen.
Enhancing retrieval-augmented large language models with iterative retrieval-generation
synergy. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Findings of the Association_
_for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023_, pages 9248–
9274. Association for Computational Linguistics, 2023.


[100] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. Retrievalgeneration synergy augmented large language models. _CoRR_, abs/2310.05149, 2023.


[101] Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. Augmentation-adapted retriever
improves generalization of language models as generic plug-in. In Anna Rogers, Jordan L.
Boyd-Graber, and Naoaki Okazaki, editors, _Proceedings of the 61st Annual Meeting of the_
_Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto,_
_Canada, July 9-14, 2023_, pages 2421–2436. Association for Computational Linguistics, 2023.


[102] J. Fang, Z. Meng, and C. Macdonald. TRACE the evidence: Constructing knowledge-grounded
reasoning chains for retrieval-augmented generation. _ArXiv preprint_, 2024.


[103] Y. Zhu, Z. Huang, Z. Dou, and J.-R. Wen. One token can help! learning scalable and pluggable
virtual token. _ArXiv preprint_, 2024.


[104] O. Yoran, T. Wolfson, O. Ram, and J. Berant. Making retrieval-augmented language models
robust to irrelevant context. _ArXiv preprint_, 2023.


[105] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal. Interleaving retrieval with
chain-of-thought reasoning for knowledge-intensive multi-step questions. In _Proc. of ACL_,
2023.


[106] Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. RQRAG: learning to refine queries for retrieval augmented generation. _CoRR_, abs/2404.00610,
2024.


[107] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In _Proceedings of the 40th Annual Meeting of the Association_
_for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA_, pages 311–318. ACL,
2002.


[108] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In _Text_
_Summarization Branches Out_, pages 74–81, Barcelona, Spain, July 2004. Association for
Computational Linguistics.


[109] AI@Meta. Llama 3 model card. 2024.


[110] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora:
Low-rank adaptation of large language models. In _Proc. of ICLR_, 2022.


[111] Manveer Singh Tamber, Ronak Pradeep, and Jimmy Lin. Pre-processing matters! improved
wikipedia corpora for open-domain question answering. In Jaap Kamps, Lorraine Goeuriot,
Fabio Crestani, Maria Maistro, Hideo Joho, Brian Davis, Cathal Gurrin, Udo Kruschwitz, and
Annalina Caputo, editors, _Advances in Information Retrieval - 45th European Conference on_
_Information Retrieval, ECIR 2023, Dublin, Ireland, April 2-6, 2023, Proceedings, Part III_,
volume 13982 of _Lecture Notes in Computer Science_, pages 163–176. Springer, 2023.


[112] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer
open-domain questions. In Regina Barzilay and Min-Yen Kan, editors, _Proceedings of the_
_55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver,_
_Canada, July 30 - August 4, Volume 1: Long Papers_, pages 1870–1879. Association for
Computational Linguistics, 2017.


[113] Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, and Ji-Rong
Wen. Progressive multimodal reasoning via active retrieval. _CoRR_, abs/2412.14835, 2024.


20


[114] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang,
and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. _CoRR_,
abs/2501.05366, 2025.

[115] Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei
Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models
are affected by supervised fine-tuning data composition. In Lun-Wei Ku, Andre Martins,
and Vivek Srikumar, editors, _Proceedings of the 62nd Annual Meeting of the Association for_
_Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August_
_11-16, 2024_, pages 177–198. Association for Computational Linguistics, 2024.

[116] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. F. Nogueira. Pyserini: A python
toolkit for reproducible information retrieval research with sparse and dense representations.
In _Proc. of SIGIR_, 2021.

[117] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised
open domain question answering. In Anna Korhonen, David R. Traum, and Lluís Màrquez,
editors, _Proceedings of the 57th Conference of the Association for Computational Linguistics,_
_ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, pages 6086–6096.
Association for Computational Linguistics, 2019.


21


### **Supplementary Material**

This appendix is organized as follows.


    - In Section A (referred by Section 4), we introduce implementation details for our
benchmarking experiments.


    - In Section B (referred by Section 3.3), we introduce the details in gathering and preprocessing datasets.


    - In Section C, we discuss the limitations of our toolkit.


**A** **Implementation Details for Benchmarking Experiments**


In our work, we test all implemented methods under a unified setting. Users only need to download
the corresponding model and fill in the configure to get the corresponding results using the script we
provide. [4] This section details the implementation specifics of reproducing various algorithms using
our toolkit, allowing users to effortlessly replicate our experimental results.


**A.1** **Global Setting**


**Retriever Setting** : In our main experiments, we utilize `E5-base-v2` as the retriever, retrieving five
passages per query. [5] We use the DPR version of the Wikipedia December 2018 dataset as the retrieval
corpus, which can be downloaded from our dataset page. In subsequent retrieval experiments, we
employ both BM25 and `BGE-base-en-v1.5` as additional retrievers. [6] The BM25 algorithms is
implemented by Pyserini [ 116 ]. During index construction, the maximum padding length is set to
512. The maximum query padding length is set to 128 during retrieval. The batch size for retrieval is
1,024, with FP16 enabled. We employ the Faiss Flat index [47] for accuracy.


**Generator Setting** : We employ `LLaMA-3-8B-instruct` as the generator in our main experiment,
with a maximum input length of 2048 and a maximum output length of 32. [7] Inference is performed
using the vLLM framework with greedy decoding during generation. In generator-related experiments,
we employ Qwen-1.5-14B as additional generators. The experiments are conducted using four
NVIDIA A100 80G GPUs.


**Prompt Setting** : We use a unified prompt to ensure fairness. Specifically, our system prompt is:

```
    Answer the question based on the given passage. Only give me
    the answer and do not output any other words. The following
    are given passages:{retrieval passages}

```

The retrieval passages are listed as:

```
    Doc 1 (Title: {title}) {content}
    Doc 2 (Title: {title}) {content}

```

Our user prompt is:

```
    Question: {question}

```

These prompts are combined using the `tokenizer.apply_chat_template` function, serving as
the final input to the generator model.


4 Running Scripts: `[https://github.com/RUC-NLPIR/FlashRAG/blob/main/examples/methods/](https://github.com/RUC-NLPIR/FlashRAG/blob/main/examples/methods/run_exp.py)`

```
run_exp.py
```

5 Model Card: `[https://huggingface.co/intfloat/e5-base-v2](https://huggingface.co/intfloat/e5-base-v2)`
6 Model Card: `[https://huggingface.co/BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)`
7 `[https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)`


22


**A.2** **Specific Settings for Different Methods**


In addition to the general settings mentioned above, each method often has its own configuration. We
introduce it as follows:


**AAR** [ 101 ]: This work focuses on optimizing the retriever. In our experiments, we use the pretrained retriever provided by the authors ( `AAR-Contriever-KILT` ). [8] FlashRAG also supports using
AAR-ANCE as the retriever.


**LLMLingua** [ 48, 49 ]: In this method, we use `LLaMA-2-7B` to compute perplexity and
LongLLMLingua as the compressor, with a compression rate set to 0 _._ 55 . Other parameters are
set to default values. Different from the original LongLLMLingua example, we only use the retrieved
text as input to the refiner rather tan the entire prompt. This is because we find that the LLaMA-3’s
prompt requires special tokens, and using the original setting caused these tokens to be omitted,
resulting in degraded performance.


**RECOMP** [ 52 ]: We use the abstractive model provided by RECOMP for our experiments. [9] For
the NQ, TQA, and HotpotQA datasets, we use the corresponding models. For the remaining
datasets, there are no trained checkpoints available. Therefore, we use the HotpotQA checkpoint
for 2WikiMultihopQA, and the NQ checkpoint for PopQA and WebQuestions. The maximum input
length for the refiner is set to 1024, and the maximum output length is set to 512.


**Selective-Context** [ 50 ]: We use GPT-2 to compute perplexity and set the compression rate to 0.5.
Similar to LongLLMLingua, we use the retrieved passages as input to the refiner.


**Ret-Robust** [ 104 ]: This method focuses on optimizing the generative model. It is trained with the SelfAsk prompt method. The authors provided the LoRA models trained on NQ and 2WikiMultihopQA. [10]
Consequently, we test using the `LLaMA-2-13B` model loaded with the corresponding LoRA
parameters. As there is no trained model for HotpotQA, we use the LoRA parameters trained
on 2WikiMultihopQA. For the remaining datasets, we use the LoRA parameters trained on NQ. We
set the maximum interaction rounds to five and the maximum output tokens to 100. For HotpotQA
and 2WikiMultihopQA, we disable the “single_hop” setting to allow the process to automatically
decompose complex queries into multiple iterations.


**SuRe** [ 98 ]: This method prompts the model to generate candidate answers and scores, and then ranks
them to select the best one. To ensure consistency, we use the prompts provided in the original paper,
which can be referenced alongside our code implementation.


**SKR** [ 37 ]: We implement the SKR-KNN method, which requires an encoder model and inferencetime training data. Specifically, it identifies the most similar queries from the training data based on
the input query, determining whether the input query needs retrieval. Our library includes the training
data provided by the authors. The corresponding encoder model is released by the authors. [11]


**Self-RAG** [ 11 ]: We use the `LLaMA-2-7B` checkpoint provided by Self-RAG and set the maximum
number of output tokens to 100 to ensure proper operation. [12] The temperature is set to 0, and top_p
is set to 1.


**B** **Collecting Details of Various Datasets**


To facilitate users in understanding the sources and composition of the datasets, we have outlined
the collection sources and pre-processing methods for each dataset provided by our library. For
datasets with multiple versions, we refer to existing studies and select the version most widely used
by researchers.


For datasets containing multiple subsets, we merge all subsets and indicate the subset each data point
belongs to in the corresponding metadata attribute. This allows users to conveniently load specific
subsets using the loading functions provided in our toolkit. Each dataset is stored in a separate folder,


8 `[https://huggingface.co/OpenMatch/AAR-Contriever-KILT](https://huggingface.co/OpenMatch/AAR-Contriever-KILT)`
9 `[https://huggingface.co/fangyuan](https://huggingface.co/fangyuan)`
10 `[https://huggingface.co/Ori/llama-2-13b-peft-nq-retrobust](https://huggingface.co/Ori/llama-2-13b-peft-nq-retrobust)`
11 `[https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased](https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased)`
12 `[https://huggingface.co/selfrag/selfrag_llama2_7b](https://huggingface.co/selfrag/selfrag_llama2_7b)`


23


with all splits named “train”, “dev”, or “test”, and each file is in JSONL format. Splits without
golden answers are excluded from our consideration. For special cases involving multiple files, such
as MMLU, we provide detailed explanations later. All datasets have been processed into a unified
format, with each data point containing the following fields:


    - `id` : A unique identifier for the data point, composed of the split it belongs to and its
corresponding position.


    - `question` : This field generally represents the input portion of each data point. For example,
for QA tasks, this is the question; for fact verification tasks, this is the claim to be judged.


    - `golden answers` : A list containing the correct answers. Even if there is only one correct
answer, it is still stored in a list to maintain a unified format.


    - `choices` : A list of options, which is only available in datasets with multiple-choice tasks.
For these datasets, their golden answers are the index corresponding to the correct options
wrapped in a list.


    - `metadata` : A dictionary containing additional information about the dataset ( _e.g._, subset,
annotation data, etc.). During the collecting process, useful information from the original
datasets is stored in the metadata for convenient data filtering by users in subsequent tasks.


The collecting details for each dataset are listed below.


**B.1** **QA Datasets**


**Natural Questions (NQ)** [ 58 ], **TriviaQA (TQA)** [ 59 ], **WebQuestions (WebQ)** [ 64 ]: These are the
most commonly used QA datasets, each available in multiple versions. We utilize the DPR [ 42 ]
versions of these datasets. For NQ, the training set is partially split into a validation set based on
NQ-Open [ 117 ] (which includes only training and test sets). Compared to the original NQ dataset,
this version has fewer examples as it excludes instances without golden annotations. For TQA,
this version employs a re-split of the original training and test sets, similar to the approach used in
Self-RAG [11].


**PopQA** [ 60 ]: This dataset is collected from the HuggingFace repository `[https://huggingface.](https://huggingface.co/datasets/akariasai/PopQA)`
`[co/datasets/akariasai/PopQA](https://huggingface.co/datasets/akariasai/PopQA)` . We reformat the dataset, including re-encoding IDs, modifying
golden answer fields, and setting metadata.


**Squad** [ 61 ]: The dataset is sourced from the HuggingFace repository `[https://huggingface.co/](https://huggingface.co/datasets/lhoestq/squad)`
`[datasets/lhoestq/squad](https://huggingface.co/datasets/lhoestq/squad)`, based on Squad v1.1. We merge the annotated context and title fields
and remove redundant answer start positions.


**Fermi** [ 70 ]: This dataset is collected from the HuggingFace repository `[https://huggingface.](https://huggingface.co/datasets/jeggers/fermi)`
`[co/datasets/jeggers/fermi](https://huggingface.co/datasets/jeggers/fermi)` . We merge the Fermi-real and Fermi-synth versions and label the
type field in the metadata to indicate the source dataset.


**MS MARCO QA** [ 62 ]: This dataset is released by Microsoft, which can be directly downloaded
from the URL: `[https://microsoft.github.io/MSMARCO-Question-Answering](https://microsoft.github.io/MSMARCO-Question-Answering)` .


**NarrativeQA** [ 24 ]: This dataset is released by DeepMind, which can be downloaded from the
URL: `[https://github.com/deepmind/narrativeqa](https://github.com/deepmind/narrativeqa)` . The text field from the original data’s
`question` is used as the question, while the text field from the original data’s `answers` is designated
as the `golden_answers` . Other field information is preserved in the metadata.


**SIQA** [ 66 ]: This dataset can be downloaded from the URL: `[https://leaderboard.allenai.](https://leaderboard.allenai.org/socialiqa/submissions/get-started)`
`[org/socialiqa/submissions/get-started](https://leaderboard.allenai.org/socialiqa/submissions/get-started)` . The `answerA`, `answerB`, and `answerC` fields from
the original data are combined into a list and used as the `golden_answers` . Other field information
is preserved in the metadata.


**PIQA** [ 69 ]: This dataset is crafted for probing the physical commonsense capabilities of NLP models.
We obtain it from `[https://github.com/ybisk/ybisk.github.io/tree/master/piqa](https://github.com/ybisk/ybisk.github.io/tree/master/piqa)` . The
original data comprises four fileds: `goal`, `sol1`, `sol2`, and `label` . The `label` indicates the correct
solution, with “0” corresponding to `sol1` and “1” to `sol2` . We only retain the correct solution in our
dataset, omitting the incorrect option and the `label` field.


24


**BoolQ** [ 68 ]: BoolQ is a dataset specifically designed for “yes/no” question answering,
encompasses 15,942 examples. We obtain it from its official repository: `[https://github.com/](https://github.com/google-research-datasets/boolean-questions)`
`[google-research-datasets/boolean-questions](https://github.com/google-research-datasets/boolean-questions)` .


**AmbigQA** [ 65 ]: We obtain this dataset from its official repository: `[https://nlp.cs.](https://nlp.cs.washington.edu/ambigqa/)`
`[washington.edu/ambigqa/](https://nlp.cs.washington.edu/ambigqa/)` . The `annotations` are relabeled as `golden_answers`, and the
`viewed_doc_titles`, `used_queries`, `nq_answer`, and `nq_doc_title` are integrated into the
metadata.


**CommonsenseQA** [ 67 ]: This dataset is a multiple-choice question answering dataset, necessitates
a diverse array of commonsense knowledge to infer the correct answers. We obtain it from the
official URL: `[https://www.tau-nlp.sites.tau.ac.il/commonsenseqa](https://www.tau-nlp.sites.tau.ac.il/commonsenseqa)` . The correct response
from choices is preserved according to the `answerKey`, with the original choices, `answerKey`, and
`question_concept` are set in the metadata.


**WikiQA** [ 63 ]: WikiQA is an annotated resource of question-sentence pairs developed by Microsoft
to enhance open-domain question answering research, originally includes `question`, `answer`,
`document_title`, and `label` . We obtain it from official URL: `[https://aka.ms/WikiQA](https://aka.ms/WikiQA)` . In
our adaptation, `document_title` and `label` are retained within the metadata.


**B.2** **MultihopQA Datasets**


**HotpotQA** [ 71 ]: We collect the distractor version of the HotpotQA dataset from the official
HuggingFace repository `[https://huggingface.co/datasets/hotpot_qa](https://huggingface.co/datasets/hotpot_qa)` . We retain the
question type, level, and all annotated supporting facts. The original answer strings are converted
into single-element lists, with additional information integrated into the metadata.


**MuSiQue** [ 73 ]: We download the MuSiQue-Ans version of the dataset from its official repository
`[https://github.com/StonyBrookNLP/musique](https://github.com/StonyBrookNLP/musique)` . Since the test set lacks golden answers, we
excluded it from our collections.


**2WikiMultihopQA** [ 72 ]: We obtain the latest version of this dataset from its official repository. The
original dataset includes a file recording aliases for some answer entities; we match these aliases for
each entry and add them to the golden answers to ensure evaluation accuracy. Similar to HotpotQA,
we place supporting facts and other additional information into the metadata.


**Bamboogle** [ 74 ]: We collect and process this dataset from the official repository link `[https://](https://docs.google.com/spreadsheets/d/1jwcsA5kE4TObr9YHn9Gc-wQHYjTbLhDGx6tmIzMhl_U/edit#gid=0)`
```
docs.google.com/spreadsheets/d/1jwcsA5kE4TObr9YHn9Gc-wQHYjTbLhDGx6tmIzMhl_
```

`[U/edit#gid=0](https://docs.google.com/spreadsheets/d/1jwcsA5kE4TObr9YHn9Gc-wQHYjTbLhDGx6tmIzMhl_U/edit#gid=0)` .


**B.3** **Long-Form QA Datasets**


**ASQA** [ 76 ]: The dataset is collected from the HuggingFace repository `[https://huggingface.co/](https://huggingface.co/datasets/din0s/asqa)`
`[datasets/din0s/asqa](https://huggingface.co/datasets/din0s/asqa)` . We reprocess unnecessary structures and selected the long answer part
from the annotations as the final golden answer, with other annotated content and QA pairs placed
into the metadata.


**ELI5** [ 77 ]: The dataset is collected from the KILT benchmark [ 87 ]. Due to the lack of correct
answers in the test set, we only retain the training and development sets.


**B.4** **Multiple-Choice Datasets**


**TruthfulQA** [ 81 ]: This is a benchmark aimed at evaluating the veracity of language model-generated
answers. We obtain it from HuggingFace: `[https://huggingface.co/datasets/truthful_qa](https://huggingface.co/datasets/truthful_qa)` .


**ARC** [ 83 ]: The dataset is obtained from the Allen Institute for AI’s official website at `[https:](https://allenai.org/data/arc)`
`[//allenai.org/data/arc](https://allenai.org/data/arc)` . We merge the easy and challenging version of ARC dataset and mark
the type in the metadata of each item.


**HellaSwag** [ 82 ]: HellaSwag is obtained from the HuggingFace repository `[https://huggingface.](https://huggingface.co/datasets/Rowan/hellaswag)`
`[co/datasets/Rowan/hellaswag](https://huggingface.co/datasets/Rowan/hellaswag)` . The content from the original data’s `ctx_a` field is used as
the question, while the label field’s content is designated as the `golden_answers` . Other field
information is preserved in the metadata key.


25


**MMLU** [ 79, 80 ]: We obtain this dataset in HuggingFace repository `[https://huggingface.co/](https://huggingface.co/datasets/lighteval/mmlu)`
`[datasets/lighteval/mmlu](https://huggingface.co/datasets/lighteval/mmlu)` . In comparison to the standard MMLU dataset, this version includes
an auxiliary training set. Given the numerous subsets, each covering various categories of questions,
we have merged all subsets into a single dataset. The original MMLU dataset includes a validation
set with five samples intended for the 5-shot evaluation scenario. We have separated this set into a
distinct file and renamed the original validation set as “dev”.


**OpenBookQA** [ 84 ]: The dataset is released by the Allen Institute for AI, which can be
downloaded from the URL: `[https://huggingface.co/datasets/allenai/openbookqa](https://huggingface.co/datasets/allenai/openbookqa)` . The
`question_stem` field from the original data is used as the question, while the text field from the
choices in the original data is designated as the `golden_answers` .


**B.5** **Other Datasets**


**Fever** [ 91 ], **WOW** [ 92 ], **AIDA CoNll-YAGO** [ 86 ], **WNED** [ 88 ], **T-REx** [ 89 ], **Zero-shot RE** [ 90 ]:
These datasets are collected from the KILT benchmark [ 87 ]. We process the format of the original
data file to maintain consistency. Due to the lack of correct answers in the test set, we only retain the
training and development sets. For Fever, We put “SUPPORT” or “REFUTES” into the list as golden

answers.


**WikiAsp** [ 23 ]: We collect this dataset from the official URL: `[https://github.com/neulab/](https://github.com/neulab/wikiasp)`
`[wikiasp](https://github.com/neulab/wikiasp)` . Since the original task is summarization, we adapt it following the approach of FLARE [ 12 ]
to generate summaries based on the corresponding query and aspect, making it more suitable for the
RAG scenario. Specifically, we retrieve the most relevant passages from the Wikipedia corpus using
the answer from each data point and use the title of the retrieved passage as the query. Our retriever
is `E5-base-v2`, and the Wikipedia corpus is the DPR version of the 2018 December Wiki. Each data
point’s aspect is included in the metadata.


**B.6** **License**


According to the settings of the original authors for each dataset, the license for each dataset is
different, including CC BY-NC-SA 4.0, [13] CC BY-NC-SA 3.0, [14] CC BY 4.0, [15] Apache-2.0, [16] MIT, [17]
BSD. [18] All datasets allow for scientific use and redistribution.


**C** **Limitations**


Due to limitations in development time and manpower, our toolkit currently has some limitations,
which we plan to gradually improve in the future:


(1) Although we strive to encompass many representative RAG methods, due to time and cost
considerations, we have not included all existing RAG works. We have surveyed the RAG works
published before 2024 and plan to continue implementing some representative works. However, due
to the large quantity, this may require support from the open source community.


(2) Our toolkit lacks support for training RAG-related components. We consider training during
the initial design, but given the diversity of training methods and the presence of many repositories
specifically dedicated to the training of retrievers and generators, we do not include this part currently.
In the future, we plan to add some supplementary scripts to support researchers’ training needs.


13 `[https://creativecommons.org/licenses/by-nc-sa/4.0/](https://creativecommons.org/licenses/by-nc-sa/4.0/)`
14 `[https://creativecommons.org/licenses/by-nc-sa/3.0/](https://creativecommons.org/licenses/by-nc-sa/3.0/)`
15 `[https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)`
16 `[https://apache.org/licenses/LICENSE-2.0](https://apache.org/licenses/LICENSE-2.0)`
17 `[https://choosealicense.com/licenses/mit/](https://choosealicense.com/licenses/mit/)`
18 `[https://www.linfo.org/bsdlicense.html](https://www.linfo.org/bsdlicense.html)`


26


