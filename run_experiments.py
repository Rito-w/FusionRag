#!/usr/bin/env python3
"""
FusionRAGå®éªŒè¿è¡Œå™¨
ç”¨äºè¿è¡Œå®Œæ•´çš„è®ºæ–‡çº§åˆ«å®éªŒ
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path

sys.path.append('.')

from tests.universal_test import test_with_config


class ExperimentRunner:
    """å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, output_dir: str = "experiment_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # æ•°æ®é›†é…ç½®
        self.datasets = {
            'nfcorpus': 'configs/datasets/nfcorpus/high_performance.yaml',
            'trec-covid': 'configs/datasets/trec-covid/high_performance.yaml',
            'natural-questions': 'configs/datasets/natural-questions/high_performance.yaml'
        }
        
        self.baseline_configs = {
            'nfcorpus_baseline': 'configs/datasets/nfcorpus/baseline.yaml',
        }
    
    def run_single_experiment(self, config_path: str, experiment_name: str) -> dict:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        print(f"ğŸš€ è¿è¡Œå®éªŒ: {experiment_name}")
        print(f"ğŸ“‹ é…ç½®æ–‡ä»¶: {config_path}")
        
        start_time = time.time()
        
        try:
            # è¿è¡Œæµ‹è¯•
            results = test_with_config(config_path, auto_download=True)
            
            if results:
                # æ·»åŠ å®éªŒå…ƒä¿¡æ¯
                results['experiment_info'] = {
                    'name': experiment_name,
                    'config_path': config_path,
                    'duration': time.time() - start_time,
                    'timestamp': time.time()
                }
                
                # ä¿å­˜ç»“æœ
                result_file = self.output_dir / f"{experiment_name}_results.json"
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=2, ensure_ascii=False)
                
                print(f"âœ… å®éªŒå®Œæˆ: {experiment_name}")
                print(f"ğŸ“„ ç»“æœä¿å­˜: {result_file}")
                
                # æ‰“å°å…³é”®æŒ‡æ ‡
                if 'evaluation' in results:
                    metrics = results['evaluation'].get('metrics', {})
                    if 'ndcg@10' in metrics:
                        print(f"ğŸ“Š NDCG@10: {metrics['ndcg@10']:.4f}")
                    if 'recall@10' in metrics:
                        print(f"ğŸ“Š Recall@10: {metrics['recall@10']:.4f}")
                
                return results
            else:
                print(f"âŒ å®éªŒå¤±è´¥: {experiment_name}")
                return None
                
        except Exception as e:
            print(f"âŒ å®éªŒå¼‚å¸¸: {experiment_name} - {e}")
            return None
    
    def run_all_datasets(self) -> dict:
        """è¿è¡Œæ‰€æœ‰æ•°æ®é›†å®éªŒ"""
        print("ğŸ¯ å¼€å§‹è¿è¡Œæ‰€æœ‰æ•°æ®é›†å®éªŒ")
        print("=" * 60)
        
        all_results = {}
        
        for dataset_name, config_path in self.datasets.items():
            if Path(config_path).exists():
                results = self.run_single_experiment(config_path, dataset_name)
                if results:
                    all_results[dataset_name] = results
            else:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        # ä¿å­˜ç»¼åˆç»“æœ
        summary_file = self.output_dir / "all_datasets_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š ç»¼åˆç»“æœä¿å­˜: {summary_file}")
        
        return all_results
    
    def run_baseline_comparison(self) -> dict:
        """è¿è¡ŒåŸºçº¿å¯¹æ¯”å®éªŒ"""
        print("ğŸ“ˆ å¼€å§‹åŸºçº¿å¯¹æ¯”å®éªŒ")
        print("=" * 60)
        
        baseline_results = {}
        
        for exp_name, config_path in self.baseline_configs.items():
            if Path(config_path).exists():
                results = self.run_single_experiment(config_path, f"baseline_{exp_name}")
                if results:
                    baseline_results[exp_name] = results
        
        return baseline_results
    
    def generate_comparison_table(self, results: dict) -> str:
        """ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼"""
        
        table = "\nğŸ“Š å®éªŒç»“æœå¯¹æ¯”è¡¨\n"
        table += "=" * 80 + "\n"
        table += f"{'æ•°æ®é›†':<20} {'NDCG@10':<10} {'Recall@10':<12} {'MAP':<10} {'å“åº”æ—¶é—´(s)':<12}\n"
        table += "-" * 80 + "\n"
        
        for dataset_name, result in results.items():
            if 'evaluation' in result:
                metrics = result['evaluation'].get('metrics', {})
                perf = result.get('performance', {})
                
                ndcg = metrics.get('ndcg@10', 0.0)
                recall = metrics.get('recall@10', 0.0)
                map_score = metrics.get('map', 0.0)
                response_time = perf.get('avg_query_time', 0.0)
                
                table += f"{dataset_name:<20} {ndcg:<10.4f} {recall:<12.4f} {map_score:<10.4f} {response_time:<12.4f}\n"
        
        return table
    
    def run_complete_evaluation(self) -> None:
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("ğŸ“ FusionRAGå®Œæ•´å®éªŒè¯„ä¼°")
        print("=" * 80)
        
        start_time = time.time()
        
        # 1. è¿è¡Œæ‰€æœ‰æ•°æ®é›†
        all_results = self.run_all_datasets()
        
        # 2. è¿è¡ŒåŸºçº¿å¯¹æ¯”
        baseline_results = self.run_baseline_comparison()
        
        # 3. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        if all_results:
            comparison_table = self.generate_comparison_table(all_results)
            print(comparison_table)
            
            # ä¿å­˜å¯¹æ¯”è¡¨æ ¼
            with open(self.output_dir / "comparison_table.txt", 'w', encoding='utf-8') as f:
                f.write(comparison_table)
        
        # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        total_time = time.time() - start_time
        
        final_report = {
            'experiment_summary': {
                'total_experiments': len(all_results) + len(baseline_results),
                'successful_experiments': len([r for r in all_results.values() if r is not None]),
                'total_duration': total_time,
                'datasets_tested': list(all_results.keys()),
                'timestamp': time.time()
            },
            'results': all_results,
            'baselines': baseline_results
        }
        
        with open(self.output_dir / "final_report.json", 'w', encoding='utf-8') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ‰ å®Œæ•´å®éªŒè¯„ä¼°å®Œæˆï¼")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"ğŸ“ ç»“æœç›®å½•: {self.output_dir}")
        print(f"ğŸ“Š æœ€ç»ˆæŠ¥å‘Š: {self.output_dir}/final_report.json")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="FusionRAGå®éªŒè¿è¡Œå™¨")
    parser.add_argument('--dataset', choices=['nfcorpus', 'trec-covid', 'natural-questions', 'all'], 
                       default='all', help='è¦è¿è¡Œçš„æ•°æ®é›†')
    parser.add_argument('--output', '-o', default='experiment_results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--baseline', action='store_true', help='è¿è¡ŒåŸºçº¿å¯¹æ¯”')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = ExperimentRunner(args.output)
    
    if args.dataset == 'all':
        # è¿è¡Œå®Œæ•´è¯„ä¼°
        runner.run_complete_evaluation()
    else:
        # è¿è¡Œå•ä¸ªæ•°æ®é›†
        config_path = runner.datasets.get(args.dataset)
        if config_path and Path(config_path).exists():
            runner.run_single_experiment(config_path, args.dataset)
        else:
            print(f"âŒ æ•°æ®é›†é…ç½®ä¸å­˜åœ¨: {args.dataset}")


if __name__ == "__main__":
    main()