#!/usr/bin/env python3
"""
èåˆæ–¹æ³•åŸºçº¿å®éªŒ
å®ç°å¹¶æµ‹è¯•æ ‡å‡†çš„èåˆæ–¹æ³•ï¼šRRFå’Œçº¿æ€§åŠ æƒèåˆ
"""

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
from collections import defaultdict

from modules.retriever.bm25_retriever import BM25Retriever
from modules.retriever.efficient_vector_index import EfficientVectorIndex
from modules.evaluation.evaluator import IndexEvaluator
from modules.utils.interfaces import Query, Document, RetrievalResult, FusionResult


def load_dataset(dataset_name: str) -> Tuple[List[Document], List[Query], Dict[str, Dict[str, int]]]:
    """åŠ è½½æ•°æ®é›†"""
    print(f"åŠ è½½æ•°æ®é›†: {dataset_name}")
    
    # åŠ è½½æ–‡æ¡£
    documents = []
    corpus_path = f"data/processed/{dataset_name}_corpus.jsonl"
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            doc = Document(
                doc_id=data['doc_id'],
                title=data.get('title', ''),
                text=data.get('text', '')
            )
            documents.append(doc)
    
    # åŠ è½½æŸ¥è¯¢
    queries = []
    queries_path = f"data/processed/{dataset_name}_queries.jsonl"
    with open(queries_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            query = Query(
                query_id=data['query_id'],
                text=data['text']
            )
            queries.append(query)
    
    # åŠ è½½ç›¸å…³æ€§åˆ¤æ–­
    relevance_judgments = {}
    qrels_path = f"data/processed/{dataset_name}_qrels.tsv"
    with open(qrels_path, 'r', encoding='utf-8') as f:
        next(f)  # è·³è¿‡è¡¨å¤´
        for line in f:
            query_id, doc_id, relevance = line.strip().split('\t')
            if query_id not in relevance_judgments:
                relevance_judgments[query_id] = {}
            relevance_judgments[query_id][doc_id] = int(relevance)
    
    print(f"æ•°æ®é›†åŠ è½½å®Œæˆ: æ–‡æ¡£æ•°é‡={len(documents)}, æŸ¥è¯¢æ•°é‡={len(queries)}")
    return documents, queries, relevance_judgments


class SimpleFusionMethods:
    """ç®€å•èåˆæ–¹æ³•å®ç°"""
    
    @staticmethod
    def reciprocal_rank_fusion(results_list: List[List[RetrievalResult]], k: int = 60) -> List[FusionResult]:
        """å€’æ•°æ’åèåˆ (RRF)"""
        if not results_list:
            return []
        
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£
        all_docs = {}
        doc_ranks = defaultdict(dict)
        
        # ä¸ºæ¯ä¸ªæ£€ç´¢å™¨çš„ç»“æœè®¡ç®—æ’å
        for retriever_idx, results in enumerate(results_list):
            retriever_name = results[0].retriever_name if results else f"retriever_{retriever_idx}"
            
            for rank, result in enumerate(results):
                doc_id = result.doc_id
                all_docs[doc_id] = result.document
                doc_ranks[doc_id][retriever_name] = rank + 1  # æ’åä»1å¼€å§‹
        
        # è®¡ç®—RRFåˆ†æ•°
        fusion_results = []
        for doc_id, ranks in doc_ranks.items():
            rrf_score = sum(1.0 / (k + rank) for rank in ranks.values())
            
            # æ”¶é›†å„æ£€ç´¢å™¨çš„åŸå§‹åˆ†æ•°
            individual_scores = {}
            for results in results_list:
                for result in results:
                    if result.doc_id == doc_id:
                        individual_scores[result.retriever_name] = result.score
                        break
            
            fusion_result = FusionResult(
                doc_id=doc_id,
                final_score=rrf_score,
                document=all_docs[doc_id],
                individual_scores=individual_scores
            )
            fusion_results.append(fusion_result)
        
        # æŒ‰åˆ†æ•°æ’åº
        fusion_results.sort(key=lambda x: x.final_score, reverse=True)
        return fusion_results
    
    @staticmethod
    def linear_weighted_fusion(results_list: List[List[RetrievalResult]], weights: List[float] = None) -> List[FusionResult]:
        """çº¿æ€§åŠ æƒèåˆ"""
        if not results_list:
            return []
        
        # é»˜è®¤ç­‰æƒé‡
        if weights is None:
            weights = [1.0 / len(results_list)] * len(results_list)
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights)
        weights = [w / total_weight for w in weights]
        
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£å’Œåˆ†æ•°
        all_docs = {}
        doc_scores = defaultdict(dict)
        
        for results, weight in zip(results_list, weights):
            if not results:
                continue
                
            retriever_name = results[0].retriever_name
            
            # å½’ä¸€åŒ–åˆ†æ•°åˆ°[0,1]
            scores = [r.score for r in results]
            max_score = max(scores) if scores else 1.0
            min_score = min(scores) if scores else 0.0
            score_range = max_score - min_score if max_score > min_score else 1.0
            
            for result in results:
                doc_id = result.doc_id
                all_docs[doc_id] = result.document
                
                # å½’ä¸€åŒ–åˆ†æ•°
                normalized_score = (result.score - min_score) / score_range
                doc_scores[doc_id][retriever_name] = {
                    'normalized': normalized_score,
                    'original': result.score,
                    'weight': weight
                }
        
        # è®¡ç®—åŠ æƒåˆ†æ•°
        fusion_results = []
        for doc_id, scores_dict in doc_scores.items():
            weighted_score = sum(
                info['normalized'] * info['weight'] 
                for info in scores_dict.values()
            )
            
            # æ”¶é›†å„æ£€ç´¢å™¨çš„åŸå§‹åˆ†æ•°
            individual_scores = {
                retriever: info['original'] 
                for retriever, info in scores_dict.items()
            }
            
            fusion_result = FusionResult(
                doc_id=doc_id,
                final_score=weighted_score,
                document=all_docs[doc_id],
                individual_scores=individual_scores
            )
            fusion_results.append(fusion_result)
        
        # æŒ‰åˆ†æ•°æ’åº
        fusion_results.sort(key=lambda x: x.final_score, reverse=True)
        return fusion_results


def run_fusion_experiment(dataset_name: str, config: Dict[str, Any], 
                         sample_size: Optional[int] = None, top_k: int = 10) -> Dict[str, Dict[str, float]]:
    """è¿è¡Œèåˆæ–¹æ³•å®éªŒ"""
    print(f"\n=== è¿è¡Œ {dataset_name} èåˆå®éªŒ ===")
    
    # åŠ è½½æ•°æ®é›†
    documents, queries, relevance_judgments = load_dataset(dataset_name)
    
    # é‡‡æ ·æŸ¥è¯¢
    if sample_size and sample_size < len(queries):
        import random
        random.seed(42)
        queries = random.sample(queries, sample_size)
        print(f"éšæœºæŠ½æ · {sample_size} ä¸ªæŸ¥è¯¢")
    
    # åˆ›å»ºæ£€ç´¢å™¨
    print("åˆ›å»ºæ£€ç´¢å™¨...")
    bm25_retriever = BM25Retriever(config.get('bm25', {}))
    vector_retriever = EfficientVectorIndex("EfficientVector", config.get('efficient_vector', {}))
    
    # åŠ è½½ç´¢å¼•
    cache_dir = Path("checkpoints/retriever_cache")
    
    # åŠ è½½BM25ç´¢å¼•
    bm25_cache_path = cache_dir / f"bm25_{dataset_name}_index.pkl"
    if bm25_cache_path.exists():
        bm25_retriever.load_index(str(bm25_cache_path))
        print(f"âœ… BM25ç´¢å¼•ä»ç¼“å­˜åŠ è½½")
    else:
        print("ğŸ”„ æ„å»ºBM25ç´¢å¼•...")
        bm25_retriever.build_index(documents)
    
    # åŠ è½½å‘é‡ç´¢å¼•
    vector_cache_path = cache_dir / f"efficientvector_{dataset_name}_index.pkl"
    if vector_cache_path.exists():
        vector_retriever.load_index(str(vector_cache_path))
        print(f"âœ… å‘é‡ç´¢å¼•ä»ç¼“å­˜åŠ è½½")
    else:
        print("ğŸ”„ æ„å»ºå‘é‡ç´¢å¼•...")
        vector_retriever.build_index(documents)
    
    # æ‰§è¡Œæ£€ç´¢
    print("æ‰§è¡Œæ£€ç´¢...")
    all_results = {}
    
    # å•ä¸€æ£€ç´¢å™¨ç»“æœ
    bm25_results = []
    vector_results = []
    
    for i, query in enumerate(queries):
        if (i + 1) % 20 == 0:
            print(f"æ£€ç´¢è¿›åº¦: {i + 1}/{len(queries)}")
        
        # BM25æ£€ç´¢
        bm25_query_results = bm25_retriever.retrieve(query, top_k * 2)  # è·å–æ›´å¤šç»“æœç”¨äºèåˆ
        bm25_results.append(bm25_query_results)
        
        # å‘é‡æ£€ç´¢
        vector_query_results = vector_retriever.retrieve(query, top_k * 2)
        vector_results.append(vector_query_results)
    
    # èåˆæ–¹æ³•
    fusion_methods = SimpleFusionMethods()
    
    print("æ‰§è¡Œèåˆ...")
    fusion_results = {}
    
    # RRFèåˆ
    rrf_results = []
    for i in range(len(queries)):
        query_results = [bm25_results[i], vector_results[i]]
        fused = fusion_methods.reciprocal_rank_fusion(query_results)
        rrf_results.append(fused[:top_k])  # åªå–top-k
    
    # çº¿æ€§åŠ æƒèåˆ (ç­‰æƒé‡)
    linear_equal_results = []
    for i in range(len(queries)):
        query_results = [bm25_results[i], vector_results[i]]
        fused = fusion_methods.linear_weighted_fusion(query_results, [0.5, 0.5])
        linear_equal_results.append(fused[:top_k])
    
    # çº¿æ€§åŠ æƒèåˆ (ä¼˜åŒ–æƒé‡ï¼ŒåŸºäºä¹‹å‰çš„ç»“æœç»™å‘é‡æ£€ç´¢æ›´é«˜æƒé‡)
    linear_optimized_results = []
    for i in range(len(queries)):
        query_results = [bm25_results[i], vector_results[i]]
        fused = fusion_methods.linear_weighted_fusion(query_results, [0.3, 0.7])
        linear_optimized_results.append(fused[:top_k])
    
    # è¯„ä¼°ç»“æœ
    print("è¯„ä¼°èåˆç»“æœ...")
    evaluator = IndexEvaluator(config.get('evaluator', {}))
    
    # è½¬æ¢èåˆç»“æœä¸ºæ£€ç´¢ç»“æœæ ¼å¼è¿›è¡Œè¯„ä¼°
    def fusion_to_retrieval_results(fusion_results_list: List[List[FusionResult]], method_name: str) -> List[List[RetrievalResult]]:
        converted = []
        for query_results in fusion_results_list:
            query_converted = []
            for fusion_result in query_results:
                retrieval_result = RetrievalResult(
                    doc_id=fusion_result.doc_id,
                    score=fusion_result.final_score,
                    document=fusion_result.document,
                    retriever_name=method_name
                )
                query_converted.append(retrieval_result)
            converted.append(query_converted)
        return converted
    
    # ç®€åŒ–è¯„ä¼°ï¼šç›´æ¥è®¡ç®—åŸºæœ¬æŒ‡æ ‡
    final_results = {}
    
    # è¯„ä¼°RRF
    rrf_metrics = evaluate_fusion_results(rrf_results, queries, relevance_judgments, top_k)
    final_results['RRF'] = rrf_metrics
    print(f"RRF: MRR={rrf_metrics.get('mrr', 0):.3f}, NDCG={rrf_metrics.get('ndcg', 0):.3f}")
    
    # è¯„ä¼°çº¿æ€§èåˆ(ç­‰æƒé‡)
    linear_equal_metrics = evaluate_fusion_results(linear_equal_results, queries, relevance_judgments, top_k)
    final_results['LinearEqual'] = linear_equal_metrics
    print(f"LinearEqual: MRR={linear_equal_metrics.get('mrr', 0):.3f}, NDCG={linear_equal_metrics.get('ndcg', 0):.3f}")
    
    # è¯„ä¼°çº¿æ€§èåˆ(ä¼˜åŒ–æƒé‡)
    linear_opt_metrics = evaluate_fusion_results(linear_optimized_results, queries, relevance_judgments, top_k)
    final_results['LinearOptimized'] = linear_opt_metrics
    print(f"LinearOptimized: MRR={linear_opt_metrics.get('mrr', 0):.3f}, NDCG={linear_opt_metrics.get('ndcg', 0):.3f}")
    
    return final_results


def evaluate_fusion_results(fusion_results_list: List[List[FusionResult]], 
                           queries: List[Query], 
                           relevance_judgments: Dict[str, Dict[str, int]], 
                           top_k: int = 10) -> Dict[str, float]:
    """è¯„ä¼°èåˆç»“æœ"""
    import numpy as np
    
    all_mrr = []
    all_ndcg = []
    all_precision = []
    all_recall = []
    
    for query, fusion_results in zip(queries, fusion_results_list):
        query_id = query.query_id
        
        if query_id not in relevance_judgments:
            continue
        
        relevant_docs = set(relevance_judgments[query_id].keys())
        retrieved_docs = [r.doc_id for r in fusion_results[:top_k]]
        
        # è®¡ç®—æŒ‡æ ‡
        if retrieved_docs:
            # Precision@k
            relevant_retrieved = len(set(retrieved_docs) & relevant_docs)
            precision = relevant_retrieved / len(retrieved_docs)
            all_precision.append(precision)
            
            # Recall@k
            recall = relevant_retrieved / len(relevant_docs) if relevant_docs else 0
            all_recall.append(recall)
            
            # MRR
            mrr = 0
            for i, doc_id in enumerate(retrieved_docs):
                if doc_id in relevant_docs:
                    mrr = 1.0 / (i + 1)
                    break
            all_mrr.append(mrr)
            
            # NDCG@k (ç®€åŒ–ç‰ˆæœ¬)
            dcg = 0
            idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant_docs), top_k)))
            
            for i, doc_id in enumerate(retrieved_docs):
                if doc_id in relevant_docs:
                    dcg += 1.0 / np.log2(i + 2)
            
            ndcg = dcg / idcg if idcg > 0 else 0
            all_ndcg.append(ndcg)
    
    return {
        'precision': np.mean(all_precision) if all_precision else 0,
        'recall': np.mean(all_recall) if all_recall else 0,
        'mrr': np.mean(all_mrr) if all_mrr else 0,
        'ndcg': np.mean(all_ndcg) if all_ndcg else 0
    }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¿è¡Œèåˆæ–¹æ³•åŸºçº¿å®éªŒ")
    parser.add_argument("--config", type=str, default="configs/paper_experiments.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--datasets", type=str, nargs="+", default=["fiqa", "quora", "scidocs"], help="æ•°æ®é›†åˆ—è¡¨")
    parser.add_argument("--sample", type=int, default=100, help="æŸ¥è¯¢æ ·æœ¬å¤§å°")
    parser.add_argument("--top_k", type=int, default=10, help="æ£€ç´¢æ–‡æ¡£æ•°é‡")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        config = json.load(f)
    
    print("ğŸš€ å¼€å§‹è¿è¡Œèåˆæ–¹æ³•åŸºçº¿å®éªŒ")
    print("=" * 50)
    
    all_results = {}
    
    for dataset in args.datasets:
        try:
            results = run_fusion_experiment(dataset, config, args.sample, args.top_k)
            all_results[dataset] = results
        except Exception as e:
            print(f"æ•°æ®é›† {dataset} å®éªŒå¤±è´¥: {e}")
            continue
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"reports/fusion_baseline_results_{timestamp}.json"
    with open(output_file, 'w') as f:
        json.dump(all_results, f, indent=2)
    
    print(f"\nèåˆåŸºçº¿å®éªŒå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # æ˜¾ç¤ºæ€»ç»“
    print("\nå®éªŒç»“æœæ€»ç»“:")
    for dataset, methods in all_results.items():
        print(f"\n{dataset}:")
        for method, metrics in methods.items():
            mrr = metrics.get('mrr', 0)
            ndcg = metrics.get('ndcg', 0)
            print(f"  {method}: MRR={mrr:.3f}, NDCG={ndcg:.3f}")


if __name__ == "__main__":
    main()