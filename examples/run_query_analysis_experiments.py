"""
æŸ¥è¯¢ç±»å‹åˆ†æè„šæœ¬
ä¸“é—¨ç”¨äºåˆ†æä¸åŒæŸ¥è¯¢ç±»å‹ä¸‹å„æ£€ç´¢æ–¹æ³•çš„æ€§èƒ½
"""

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
from collections import defaultdict

from modules.adaptive_hybrid_index import AdaptiveHybridIndex, create_adaptive_hybrid_index
from modules.retriever.bm25_retriever import BM25Retriever
from modules.retriever.efficient_vector_index import EfficientVectorIndex
from modules.retriever.semantic_bm25 import SemanticBM25
from modules.evaluation.evaluator import IndexEvaluator
from modules.utils.interfaces import Query, Document


def load_dataset(dataset_name: str) -> Tuple[List[Document], List[Query], Dict[str, Dict[str, int]]]:
    """åŠ è½½æ•°æ®é›†"""
    print(f"åŠ è½½æ•°æ®é›†: {dataset_name}")
    
    # åŠ è½½æ–‡æ¡£
    documents = []
    corpus_path = f"data/processed/{dataset_name}_corpus.jsonl"
    print(f"åŠ è½½æ–‡æ¡£: {corpus_path}")
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            doc = Document(
                doc_id=data['doc_id'],
                title=data.get('title', ''),
                text=data.get('text', '')
            )
            documents.append(doc)
    
    # åŠ è½½æŸ¥è¯¢
    queries = []
    queries_path = f"data/processed/{dataset_name}_queries.jsonl"
    print(f"åŠ è½½æŸ¥è¯¢: {queries_path}")
    with open(queries_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            query = Query(
                query_id=data['query_id'],
                text=data['text']
            )
            queries.append(query)
    
    # åŠ è½½ç›¸å…³æ€§åˆ¤æ–­
    relevance_judgments = {}
    qrels_path = f"data/processed/{dataset_name}_qrels.tsv"
    print(f"åŠ è½½ç›¸å…³æ€§åˆ¤æ–­: {qrels_path}")
    with open(qrels_path, 'r', encoding='utf-8') as f:
        next(f)  # è·³è¿‡è¡¨å¤´
        for line in f:
            query_id, doc_id, relevance = line.strip().split('\t')
            if query_id not in relevance_judgments:
                relevance_judgments[query_id] = {}
            relevance_judgments[query_id][doc_id] = int(relevance)
    
    print(f"æ•°æ®é›†åŠ è½½å®Œæˆ: æ–‡æ¡£æ•°é‡={len(documents)}, æŸ¥è¯¢æ•°é‡={len(queries)}, ç›¸å…³æ€§åˆ¤æ–­æ•°é‡={len(relevance_judgments)}")
    return documents, queries, relevance_judgments


def create_retrievers(config: Dict[str, Any] = None) -> Dict[str, Any]:
    """åˆ›å»ºæ£€ç´¢å™¨"""
    config = config or {}
    
    retrievers = {
        "BM25": BM25Retriever(config.get('bm25', {})),
        "EfficientVector": EfficientVectorIndex(config.get('efficient_vector', {})),
        "SemanticBM25": SemanticBM25(config.get('semantic_bm25', {})),
    }
    
    # åˆ›å»ºè‡ªé€‚åº”æ··åˆç´¢å¼•
    adaptive_config = {
        'retrievers': {
            'bm25': config.get('bm25', {}),
            'efficient_vector': config.get('efficient_vector', {}),
            'semantic_bm25': config.get('semantic_bm25', {})
        },
        'query_analyzer': config.get('query_analyzer', {}),
        'adaptive_router': config.get('adaptive_router', {}),
        'adaptive_fusion': config.get('adaptive_fusion', {})
    }
    
    retrievers["AdaptiveHybrid"] = create_adaptive_hybrid_index(adaptive_config)
    
    return retrievers


def run_query_type_analysis(dataset_name: str, config: Dict[str, Any] = None,
                          top_k: int = 10, sample_size: Optional[int] = None) -> Dict[str, Dict[str, Dict[str, float]]]:
    """è¿è¡ŒæŸ¥è¯¢ç±»å‹åˆ†æ"""
    print(f"\n=== è¿è¡Œ {dataset_name} æŸ¥è¯¢ç±»å‹åˆ†æ ===")
    
    # åŠ è½½æ•°æ®é›†
    documents, queries, relevance_judgments = load_dataset(dataset_name)
    
    # å¦‚æœæŒ‡å®šäº†æ ·æœ¬å¤§å°ï¼ŒéšæœºæŠ½æ ·
    if sample_size and sample_size < len(queries):
        import random
        random.seed(42)
        queries = random.sample(queries, sample_size)
        print(f"éšæœºæŠ½æ · {sample_size} ä¸ªæŸ¥è¯¢")
    
    # åˆ›å»ºæ£€ç´¢å™¨
    print("åˆ›å»ºæ£€ç´¢å™¨...")
    retrievers = create_retrievers(config)
    
    # æ„å»ºç´¢å¼•
    print("æ„å»ºç´¢å¼•...")
    for name, retriever in retrievers.items():
        print(f"æ„å»º {name} ç´¢å¼•...")
        start_time = time.time()
        retriever.build_index(documents)
        end_time = time.time()
        print(f"{name} ç´¢å¼•æ„å»ºå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
    
    # åˆ†ææŸ¥è¯¢ç±»å‹
    print("åˆ†ææŸ¥è¯¢ç±»å‹...")
    adaptive_retriever = retrievers.get("AdaptiveHybrid")
    if not adaptive_retriever:
        print("é”™è¯¯: æœªæ‰¾åˆ°AdaptiveHybridæ£€ç´¢å™¨ï¼Œæ— æ³•è¿›è¡ŒæŸ¥è¯¢ç±»å‹åˆ†æ")
        return {}
    
    query_analyzer = adaptive_retriever.query_analyzer
    query_types = {}
    
    for query in queries:
        features = query_analyzer.analyze_query(query)
        query_types[query.query_id] = features.query_type.value
    
    # ç»Ÿè®¡æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ
    type_distribution = defaultdict(int)
    for query_type in query_types.values():
        type_distribution[query_type] += 1
    
    print("æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ:")
    for query_type, count in type_distribution.items():
        print(f"  {query_type}: {count} ({count/len(queries)*100:.1f}%)")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator_config = config.get('evaluator', {})
    evaluator = IndexEvaluator({**evaluator_config, 'query_types': query_types})
    
    # æŒ‰æŸ¥è¯¢ç±»å‹è¯„ä¼°
    print("æŒ‰æŸ¥è¯¢ç±»å‹è¯„ä¼°...")
    results = evaluator.evaluate_by_query_type(retrievers, queries, relevance_judgments, top_k)
    
    # ç”ŸæˆæŠ¥å‘Š
    report_dir = f"reports/{dataset_name}/query_types"
    Path(report_dir).mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"{report_dir}/query_types_{timestamp}.json"
    
    # ä¿å­˜ç»“æœ
    with open(report_file, 'w') as f:
        json.dump({
            "dataset": dataset_name,
            "timestamp": datetime.now().isoformat(),
            "query_type_distribution": {k: v for k, v in type_distribution.items()},
            "results": {k: {r: v for r, v in v.items()} for k, v in results.items()}
        }, f, indent=2)
    
    print(f"æŸ¥è¯¢ç±»å‹åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    # æ˜¾ç¤ºç®€è¦ç»“æœ
    print("\næŸ¥è¯¢ç±»å‹åˆ†æç»“æœæ‘˜è¦:")
    for query_type, retrievers_results in results.items():
        print(f"\næŸ¥è¯¢ç±»å‹: {query_type} (æ•°é‡: {type_distribution[query_type]})")
        for retriever_name, metrics in retrievers_results.items():
            precision = metrics.get('precision', 0.0)
            recall = metrics.get('recall', 0.0)
            mrr = metrics.get('mrr', 0.0)
            print(f"  {retriever_name}: P={precision:.3f}, R={recall:.3f}, MRR={mrr:.3f}")
    
    return results


def run_multiple_datasets(config_path: str = "configs/lightweight_config.json", 
                        datasets: List[str] = None,
                        top_k: int = 10,
                        sample_size: Optional[int] = None) -> None:
    """åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿è¡ŒæŸ¥è¯¢ç±»å‹åˆ†æ"""
    # åŠ è½½é…ç½®
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
    except FileNotFoundError:
        print(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config = {
            "bm25": {"k1": 1.2, "b": 0.75},
            "efficient_vector": {
                "model_name": "sentence-transformers/all-MiniLM-L6-v2",
                "index_type": "hnsw",
                "batch_size": 8
            },
            "semantic_bm25": {
                "semantic_model_name": "sentence-transformers/all-MiniLM-L6-v2",
                "semantic_weight": 0.3,
                "batch_size": 8
            },
            "query_analyzer": {
                "semantic_model_name": "sentence-transformers/all-MiniLM-L6-v2"
            },
            "adaptive_router": {
                "routing_strategy": "hybrid"
            },
            "adaptive_fusion": {
                "default_method": "weighted_sum"
            },
            "evaluator": {
                "metrics": ["precision", "recall", "mrr", "ndcg", "latency"],
                "report_dir": "reports"
            }
        }
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®é›†ï¼Œä½¿ç”¨é…ç½®ä¸­çš„æ•°æ®é›†æˆ–é»˜è®¤æ•°æ®é›†
    if datasets is None:
        datasets = config.get('datasets', ['nfcorpus'])
    
    # è¿è¡ŒæŸ¥è¯¢ç±»å‹åˆ†æ
    for dataset in datasets:
        try:
            run_query_type_analysis(dataset, config, top_k, sample_size)
        except Exception as e:
            print(f"æ•°æ®é›† {dataset} æŸ¥è¯¢ç±»å‹åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print("\næ‰€æœ‰æŸ¥è¯¢ç±»å‹åˆ†æå®Œæˆ!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è¿è¡ŒæŸ¥è¯¢ç±»å‹åˆ†æ")
    parser.add_argument("--config", type=str, default="configs/lightweight_config.json", 
                       help="å®éªŒé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--datasets", type=str, nargs="+", 
                       help="è¦è¯„ä¼°çš„æ•°æ®é›†")
    parser.add_argument("--top_k", type=int, default=10, 
                       help="æ£€ç´¢çš„æ–‡æ¡£æ•°é‡")
    parser.add_argument("--sample", type=int, 
                       help="æŸ¥è¯¢æ ·æœ¬å¤§å°ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•")
    
    args = parser.parse_args()
    
    print("ğŸ” å¼€å§‹è¿è¡ŒæŸ¥è¯¢ç±»å‹åˆ†æ")
    print("=" * 50)
    
    run_multiple_datasets(
        config_path=args.config,
        datasets=args.datasets, 
        top_k=args.top_k, 
        sample_size=args.sample
    )