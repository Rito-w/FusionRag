#!/usr/bin/env python3
"""
è‡ªé€‚åº”èåˆæ–¹æ³•å®Œæ•´å®éªŒ
é›†æˆæŸ¥è¯¢åˆ†æå™¨ã€è‡ªé€‚åº”è·¯ç”±å™¨å’Œèåˆå¼•æ“çš„ç«¯åˆ°ç«¯ç³»ç»Ÿ
"""

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime

from modules.retriever.bm25_retriever import BM25Retriever
from modules.retriever.efficient_vector_index import EfficientVectorIndex
from modules.analysis.simple_query_analyzer import SimpleQueryAnalyzer, create_simple_query_analyzer
from modules.adaptive.simple_adaptive_router import SimpleAdaptiveRouter, create_simple_adaptive_router
from modules.adaptive.simple_adaptive_fusion import SimpleAdaptiveFusion, create_simple_adaptive_fusion
from modules.evaluation.evaluator import IndexEvaluator
from modules.utils.interfaces import Query, Document, RetrievalResult, FusionResult


def load_dataset(dataset_name: str) -> Tuple[List[Document], List[Query], Dict[str, Dict[str, int]]]:
    """åŠ è½½æ•°æ®é›†"""
    print(f"åŠ è½½æ•°æ®é›†: {dataset_name}")
    
    # åŠ è½½æ–‡æ¡£
    documents = []
    corpus_path = f"data/processed/{dataset_name}_corpus.jsonl"
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            doc = Document(
                doc_id=data['doc_id'],
                title=data.get('title', ''),
                text=data.get('text', '')
            )
            documents.append(doc)
    
    # åŠ è½½æŸ¥è¯¢
    queries = []
    queries_path = f"data/processed/{dataset_name}_queries.jsonl"
    with open(queries_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            query = Query(
                query_id=data['query_id'],
                text=data['text']
            )
            queries.append(query)
    
    # åŠ è½½ç›¸å…³æ€§åˆ¤æ–­
    relevance_judgments = {}
    qrels_path = f"data/processed/{dataset_name}_qrels.tsv"
    with open(qrels_path, 'r', encoding='utf-8') as f:
        next(f)  # è·³è¿‡è¡¨å¤´
        for line in f:
            query_id, doc_id, relevance = line.strip().split('\t')
            if query_id not in relevance_judgments:
                relevance_judgments[query_id] = {}
            relevance_judgments[query_id][doc_id] = int(relevance)
    
    print(f"æ•°æ®é›†åŠ è½½å®Œæˆ: æ–‡æ¡£æ•°é‡={len(documents)}, æŸ¥è¯¢æ•°é‡={len(queries)}")
    return documents, queries, relevance_judgments


class AdaptiveFusionPipeline:
    """è‡ªé€‚åº”èåˆå®Œæ•´æµæ°´çº¿"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # åˆ›å»ºç»„ä»¶
        self.query_analyzer = create_simple_query_analyzer(config.get('query_analyzer', {}))
        self.adaptive_router = create_simple_adaptive_router(config.get('adaptive_router', {}))
        self.adaptive_fusion = create_simple_adaptive_fusion(config.get('adaptive_fusion', {}))
        
        # æ£€ç´¢å™¨
        self.retrievers = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.pipeline_stats = {
            'total_queries': 0,
            'avg_processing_time': 0,
            'routing_decisions': [],
            'fusion_results': []
        }
    
    def setup_retrievers(self, documents: List[Document], dataset_name: str):
        """è®¾ç½®æ£€ç´¢å™¨"""
        print("è®¾ç½®æ£€ç´¢å™¨...")
        
        # BM25æ£€ç´¢å™¨
        bm25_retriever = BM25Retriever(self.config.get('bm25', {}))
        cache_dir = Path("checkpoints/retriever_cache")
        bm25_cache_path = cache_dir / f"bm25_{dataset_name}_index.pkl"
        
        if bm25_cache_path.exists():
            bm25_retriever.load_index(str(bm25_cache_path))
            print("âœ… BM25ç´¢å¼•ä»ç¼“å­˜åŠ è½½")
        else:
            print("ğŸ”„ æ„å»ºBM25ç´¢å¼•...")
            bm25_retriever.build_index(documents)
        
        self.retrievers['BM25'] = bm25_retriever
        
        # å‘é‡æ£€ç´¢å™¨
        vector_retriever = EfficientVectorIndex("EfficientVector", self.config.get('efficient_vector', {}))
        vector_cache_path = cache_dir / f"efficientvector_{dataset_name}_index.pkl"
        
        if vector_cache_path.exists():
            vector_retriever.load_index(str(vector_cache_path))
            print("âœ… å‘é‡ç´¢å¼•ä»ç¼“å­˜åŠ è½½")
        else:
            print("ğŸ”„ æ„å»ºå‘é‡ç´¢å¼•...")
            vector_retriever.build_index(documents)
        
        self.retrievers['EfficientVector'] = vector_retriever
        
        print(f"æ£€ç´¢å™¨è®¾ç½®å®Œæˆ: {list(self.retrievers.keys())}")
    
    def process_query(self, query: Query, top_k: int = 10) -> Tuple[List[FusionResult], Dict[str, Any]]:
        """å¤„ç†å•ä¸ªæŸ¥è¯¢"""
        start_time = time.time()
        
        # 1. æŸ¥è¯¢åˆ†æ
        query_features = self.query_analyzer.analyze_query(query)
        
        # 2. è‡ªé€‚åº”è·¯ç”±
        routing_decision = self.adaptive_router.route(query_features)
        
        # 3. æ‰§è¡Œæ£€ç´¢
        retrieval_results = {}
        for retriever_name in routing_decision.selected_retrievers:
            if retriever_name in self.retrievers:
                results = self.retrievers[retriever_name].retrieve(query, top_k * 2)  # è·å–æ›´å¤šç»“æœç”¨äºèåˆ
                retrieval_results[retriever_name] = results
        
        # 4. è‡ªé€‚åº”èåˆ
        fusion_results = self.adaptive_fusion.fuse(query, retrieval_results, routing_decision, top_k)
        
        # 5. è®°å½•ç»Ÿè®¡ä¿¡æ¯
        processing_time = (time.time() - start_time) * 1000  # æ¯«ç§’
        
        query_stats = {
            'query_id': query.query_id,
            'query_text': query.text,
            'query_features': query_features.to_dict(),
            'routing_decision': routing_decision.to_dict(),
            'processing_time': processing_time,
            'num_results': len(fusion_results)
        }
        
        return fusion_results, query_stats
    
    def run_experiment(self, queries: List[Query], relevance_judgments: Dict[str, Dict[str, int]], 
                      sample_size: Optional[int] = None, top_k: int = 10) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        
        # é‡‡æ ·æŸ¥è¯¢
        if sample_size and sample_size < len(queries):
            import random
            random.seed(42)
            queries = random.sample(queries, sample_size)
            print(f"éšæœºæŠ½æ · {sample_size} ä¸ªæŸ¥è¯¢")
        
        print(f"å¼€å§‹å¤„ç† {len(queries)} ä¸ªæŸ¥è¯¢...")
        
        all_fusion_results = []
        all_query_stats = []
        
        for i, query in enumerate(queries):
            if (i + 1) % 20 == 0:
                print(f"å¤„ç†è¿›åº¦: {i + 1}/{len(queries)}")
            
            try:
                fusion_results, query_stats = self.process_query(query, top_k)
                all_fusion_results.append(fusion_results)
                all_query_stats.append(query_stats)
                
            except Exception as e:
                print(f"å¤„ç†æŸ¥è¯¢ {query.query_id} å¤±è´¥: {e}")
                continue
        
        # è¯„ä¼°ç»“æœ
        print("è¯„ä¼°è‡ªé€‚åº”èåˆç»“æœ...")
        metrics = self._evaluate_results(all_fusion_results, queries[:len(all_fusion_results)], relevance_judgments, top_k)
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        stats_report = self._generate_stats_report(all_query_stats)
        
        return {
            'metrics': metrics,
            'statistics': stats_report,
            'query_details': all_query_stats
        }
    
    def _evaluate_results(self, fusion_results_list: List[List[FusionResult]], 
                         queries: List[Query], 
                         relevance_judgments: Dict[str, Dict[str, int]], 
                         top_k: int = 10) -> Dict[str, float]:
        """è¯„ä¼°èåˆç»“æœ"""
        import numpy as np
        
        all_mrr = []
        all_ndcg = []
        all_precision = []
        all_recall = []
        
        for query, fusion_results in zip(queries, fusion_results_list):
            query_id = query.query_id
            
            if query_id not in relevance_judgments:
                continue
            
            relevant_docs = set(relevance_judgments[query_id].keys())
            retrieved_docs = [r.doc_id for r in fusion_results[:top_k]]
            
            if retrieved_docs:
                # Precision@k
                relevant_retrieved = len(set(retrieved_docs) & relevant_docs)
                precision = relevant_retrieved / len(retrieved_docs)
                all_precision.append(precision)
                
                # Recall@k
                recall = relevant_retrieved / len(relevant_docs) if relevant_docs else 0
                all_recall.append(recall)
                
                # MRR
                mrr = 0
                for i, doc_id in enumerate(retrieved_docs):
                    if doc_id in relevant_docs:
                        mrr = 1.0 / (i + 1)
                        break
                all_mrr.append(mrr)
                
                # NDCG@k (ç®€åŒ–ç‰ˆæœ¬)
                dcg = 0
                idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant_docs), top_k)))
                
                for i, doc_id in enumerate(retrieved_docs):
                    if doc_id in relevant_docs:
                        dcg += 1.0 / np.log2(i + 2)
                
                ndcg = dcg / idcg if idcg > 0 else 0
                all_ndcg.append(ndcg)
        
        return {
            'precision': np.mean(all_precision) if all_precision else 0,
            'recall': np.mean(all_recall) if all_recall else 0,
            'mrr': np.mean(all_mrr) if all_mrr else 0,
            'ndcg': np.mean(all_ndcg) if all_ndcg else 0
        }
    
    def _generate_stats_report(self, query_stats: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        if not query_stats:
            return {}
        
        # å¤„ç†æ—¶é—´ç»Ÿè®¡
        processing_times = [stat['processing_time'] for stat in query_stats]
        avg_processing_time = sum(processing_times) / len(processing_times)
        
        # æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ
        query_types = [stat['query_features']['query_type'] for stat in query_stats]
        type_distribution = {}
        for qtype in query_types:
            type_distribution[qtype] = type_distribution.get(qtype, 0) + 1
        
        # èåˆæ–¹æ³•ä½¿ç”¨ç»Ÿè®¡
        fusion_methods = [stat['routing_decision']['fusion_method'] for stat in query_stats]
        method_distribution = {}
        for method in fusion_methods:
            method_distribution[method] = method_distribution.get(method, 0) + 1
        
        # æ£€ç´¢å™¨é€‰æ‹©ç»Ÿè®¡
        retriever_usage = {}
        for stat in query_stats:
            for retriever in stat['routing_decision']['selected_retrievers']:
                retriever_usage[retriever] = retriever_usage.get(retriever, 0) + 1
        
        return {
            'total_queries': len(query_stats),
            'avg_processing_time_ms': avg_processing_time,
            'query_type_distribution': type_distribution,
            'fusion_method_distribution': method_distribution,
            'retriever_usage': retriever_usage,
            'router_stats': self.adaptive_router.get_statistics(),
            'fusion_stats': self.adaptive_fusion.get_statistics()
        }


def run_adaptive_fusion_experiment(dataset_name: str, config: Dict[str, Any], 
                                 sample_size: Optional[int] = None, top_k: int = 10) -> Dict[str, Any]:
    """è¿è¡Œè‡ªé€‚åº”èåˆå®éªŒ"""
    print(f"\n=== è¿è¡Œ {dataset_name} è‡ªé€‚åº”èåˆå®éªŒ ===")
    
    # åŠ è½½æ•°æ®é›†
    documents, queries, relevance_judgments = load_dataset(dataset_name)
    
    # åˆ›å»ºæµæ°´çº¿
    pipeline = AdaptiveFusionPipeline(config)
    
    # è®¾ç½®æ£€ç´¢å™¨
    pipeline.setup_retrievers(documents, dataset_name)
    
    # è¿è¡Œå®éªŒ
    results = pipeline.run_experiment(queries, relevance_judgments, sample_size, top_k)
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¿è¡Œè‡ªé€‚åº”èåˆå®éªŒ")
    parser.add_argument("--config", type=str, default="configs/paper_experiments.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--datasets", type=str, nargs="+", default=["fiqa", "quora", "scidocs"], help="æ•°æ®é›†åˆ—è¡¨")
    parser.add_argument("--sample", type=int, default=50, help="æŸ¥è¯¢æ ·æœ¬å¤§å°")
    parser.add_argument("--top_k", type=int, default=10, help="æ£€ç´¢æ–‡æ¡£æ•°é‡")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        config = json.load(f)
    
    print("ğŸš€ å¼€å§‹è¿è¡Œè‡ªé€‚åº”èåˆå®éªŒ")
    print("=" * 50)
    
    all_results = {}
    
    for dataset in args.datasets:
        try:
            results = run_adaptive_fusion_experiment(dataset, config, args.sample, args.top_k)
            all_results[dataset] = results
            
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            metrics = results['metrics']
            stats = results['statistics']
            
            print(f"\n{dataset} è‡ªé€‚åº”èåˆç»“æœ:")
            print(f"  MRR: {metrics.get('mrr', 0):.3f}")
            print(f"  NDCG: {metrics.get('ndcg', 0):.3f}")
            print(f"  Precision: {metrics.get('precision', 0):.3f}")
            print(f"  Recall: {metrics.get('recall', 0):.3f}")
            print(f"  å¹³å‡å¤„ç†æ—¶é—´: {stats.get('avg_processing_time_ms', 0):.1f}ms")
            print(f"  æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ: {stats.get('query_type_distribution', {})}")
            print(f"  èåˆæ–¹æ³•åˆ†å¸ƒ: {stats.get('fusion_method_distribution', {})}")
            
        except Exception as e:
            print(f"æ•°æ®é›† {dataset} å®éªŒå¤±è´¥: {e}")
            continue
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"reports/adaptive_fusion_results_{timestamp}.json"
    with open(output_file, 'w') as f:
        json.dump(all_results, f, indent=2)
    
    print(f"\nè‡ªé€‚åº”èåˆå®éªŒå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    main()